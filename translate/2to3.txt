--- .\i18n.py	(original)
+++ .\i18n.py	(refactored)
@@ -22,6 +22,6 @@
 """
 
 import gettext
-import __builtin__
-if not '_' in __builtin__.__dict__:
-    gettext.install("translate-toolkit", unicode=1)
+import builtins
+if not '_' in builtins.__dict__:
+    gettext.install("translate-toolkit", str=1)
--- .\convert\accesskey.py	(original)
+++ .\convert\accesskey.py	(refactored)
@@ -21,7 +21,7 @@
 
 from translate.storage.placeables.general import XMLEntityPlaceable
 
-DEFAULT_ACCESSKEY_MARKER = u"&"
+DEFAULT_ACCESSKEY_MARKER = "&"
 
 
 class UnitMixer(object):
@@ -116,12 +116,12 @@
     :type accesskey_marker: Char
     :param accesskey_marker: The character that is used to prefix an access key
     """
-    assert isinstance(string, unicode)
-    assert isinstance(accesskey_marker, unicode)
+    assert isinstance(string, str)
+    assert isinstance(accesskey_marker, str)
     assert len(accesskey_marker) == 1
-    if string == u"":
-        return u"", u""
-    accesskey = u""
+    if string == "":
+        return "", ""
+    accesskey = ""
     label = string
     marker_pos = 0
     while marker_pos >= 0:
@@ -153,8 +153,8 @@
     :rtype: unicode or None
     :return: label+accesskey string or None if uncombineable
     """
-    assert isinstance(label, unicode)
-    assert isinstance(accesskey, unicode)
+    assert isinstance(label, str)
+    assert isinstance(accesskey, str)
     if len(accesskey) == 0:
         return None
     searchpos = 0
--- .\convert\convert.py	(original)
+++ .\convert\convert.py	(refactored)
@@ -23,9 +23,9 @@
 
 import os.path
 try:
-    from cStringIO import StringIO
+    from io import StringIO
 except ImportError:
-    from StringIO import StringIO
+    from io import StringIO
 
 from translate.misc import optrecurse
 # don't import optparse ourselves, get the version from optrecurse
@@ -124,7 +124,7 @@
         """Filters output options, processing relevant switches in options."""
         if self.usepots and options.pot:
             outputoptions = {}
-            for (inputformat, templateformat), (outputformat, convertor) in self.outputoptions.iteritems():
+            for (inputformat, templateformat), (outputformat, convertor) in self.outputoptions.items():
                 inputformat = self.potifyformat(inputformat)
                 templateformat = self.potifyformat(templateformat)
                 outputformat = self.potifyformat(outputformat)
@@ -165,7 +165,7 @@
         options.outputoptions = self.filteroutputoptions(options)
         try:
             self.verifyoptions(options)
-        except Exception, e:
+        except Exception as e:
             self.error(str(e))
         self.recursiveprocess(options)
 
@@ -266,7 +266,7 @@
 
     def isarchive(self, fileoption, filepurpose='input'):
         """Returns whether the file option is an archive file."""
-        if not isinstance(fileoption, (str, unicode)):
+        if not isinstance(fileoption, str):
             return False
         mustexist = (filepurpose != 'output')
         if mustexist and not os.path.isfile(fileoption):
@@ -477,10 +477,10 @@
 
     from translate.storage import statsdb
 
-    units = filter(lambda unit: unit.istranslatable(), store.units)
-    translated = filter(lambda unit: unit.istranslated(), units)
-    wordcounts = dict(map(lambda unit: (unit, statsdb.wordsinunit(unit)), units))
-    sourcewords = lambda elementlist: sum(map(lambda unit: wordcounts[unit][0], elementlist))
+    units = [unit for unit in store.units if unit.istranslatable()]
+    translated = [unit for unit in units if unit.istranslated()]
+    wordcounts = dict([(unit, statsdb.wordsinunit(unit)) for unit in units])
+    sourcewords = lambda elementlist: sum([wordcounts[unit][0] for unit in elementlist])
     tranlated_count = sourcewords(translated)
     total_count = sourcewords(units)
     percent = tranlated_count * 100 / total_count
--- .\convert\csv2po.py	(original)
+++ .\convert\csv2po.py	(refactored)
@@ -56,7 +56,7 @@
 
 
 def simplify(string):
-    return filter(type(string).isalnum, string)
+    return list(filter(type(string).isalnum, string))
 
 
 class csv2po:
--- .\convert\factory.py	(original)
+++ .\convert\factory.py	(refactored)
@@ -47,7 +47,7 @@
         return 'Unable to find extension for file: %s' % (self.file)
 
     def __unicode__(self):
-        return unicode(str(self))
+        return str(str(self))
 
 
 class UnsupportedConversionError(Exception):
@@ -64,7 +64,7 @@
         return msg
 
     def __unicode__(self):
-        return unicode(str(self))
+        return str(str(self))
 
 
 def get_extension(filename):
--- .\convert\odf2xliff.py	(original)
+++ .\convert\odf2xliff.py	(refactored)
@@ -37,17 +37,17 @@
     """
 
     def translate_toolkit_implementation(store):
-        import cStringIO
+        import io
         import zipfile
 
         from translate.storage.xml_extract import extract
         from translate.storage import odf_shared
 
         contents = odf_io.open_odf(inputfile)
-        for data in contents.values():
+        for data in list(contents.values()):
             parse_state = extract.ParseState(odf_shared.no_translate_content_elements,
                                              odf_shared.inline_elements)
-            extract.build_store(cStringIO.StringIO(data), store, parse_state)
+            extract.build_store(io.StringIO(data), store, parse_state)
 
     def itools_implementation(store):
         from itools.handlers import get_handler
@@ -74,7 +74,7 @@
         try:
             store.setfilename(store.getfilenode('NoName'), inputfile.name)
         except:
-            print "couldn't set origin filename"
+            print("couldn't set origin filename")
         yield store
         store.save()
 
--- .\convert\oo2po.py	(original)
+++ .\convert\oo2po.py	(refactored)
@@ -27,7 +27,7 @@
 
 import sys
 import logging
-from urllib import urlencode
+from urllib.parse import urlencode
 
 from translate.storage import po
 from translate.storage import oo
--- .\convert\oo2xliff.py	(original)
+++ .\convert\oo2xliff.py	(refactored)
@@ -27,7 +27,7 @@
 
 import sys
 import logging
-from urllib import urlencode
+from urllib.parse import urlencode
 
 from translate.storage import xliff
 from translate.storage import oo
--- .\convert\po2dtd.py	(original)
+++ .\convert\po2dtd.py	(refactored)
@@ -173,7 +173,7 @@
     # identify them seems to be on their file path in the tree (based on code
     # in compare-locales).
     android_dtd = False
-    header_comment = u""
+    header_comment = ""
     input_header = inputstore.header()
     if input_header:
         header_comment = input_header.getnotes("developer")
--- .\convert\po2oo.py	(original)
+++ .\convert\po2oo.py	(refactored)
@@ -60,7 +60,7 @@
     def makeindex(self):
         """makes an index of the oo keys that are used in the source file"""
         self.index = {}
-        for ookey, theoo in self.o.ookeys.iteritems():
+        for ookey, theoo in self.o.ookeys.items():
             sourcekey = oo.makekey(ookey, self.long_keys)
             self.index[sourcekey] = theoo
 
@@ -94,7 +94,7 @@
                                key, len(self.index))
                 try:
                     sourceunitlines = str(unit)
-                    if isinstance(sourceunitlines, unicode):
+                    if isinstance(sourceunitlines, str):
                         sourceunitlines = sourceunitlines.encode("utf-8")
                     logger.warning(sourceunitlines)
                 except:
@@ -124,7 +124,7 @@
         # If there is no translation, we don't want to add a line
         if len(unquotedstr) == 0:
             return
-        if isinstance(unquotedstr, unicode):
+        if isinstance(unquotedstr, str):
             unquotedstr = unquotedstr.encode("UTF-8")
         # finally set the new definition in the oo, but not if its empty
         if len(unquotedstr) > 0:
@@ -161,7 +161,7 @@
         filterresult = self.filterunit(unit)
         if filterresult:
             if filterresult != autocorrect:
-                for filtername, filtermessage in filterresult.iteritems():
+                for filtername, filtermessage in filterresult.items():
                     location = unit.getlocations()[0].encode('utf-8')
                     if filtername in self.options.error:
                         logger.error("Error at %s::%s: %s",
--- .\convert\po2php.py	(original)
+++ .\convert\po2php.py	(refactored)
@@ -55,7 +55,7 @@
         return outputlines
 
     def convertline(self, line):
-        line = unicode(line, 'utf-8')
+        line = str(line, 'utf-8')
         returnline = ""
         # handle multiline msgid if we're in one
         if self.inmultilinemsgid:
@@ -136,7 +136,7 @@
                 # continue
                 if endpos == -1 or line[endpos-1] == '\\':
                     self.inmultilinemsgid = True
-        if isinstance(returnline, unicode):
+        if isinstance(returnline, str):
             returnline = returnline.encode('utf-8')
         return returnline
 
--- .\convert\po2prop.py	(original)
+++ .\convert\po2prop.py	(refactored)
@@ -29,7 +29,7 @@
 from translate.storage import po
 from translate.storage import properties
 
-eol = u"\n"
+eol = "\n"
 
 
 class reprop:
@@ -57,7 +57,7 @@
         for line in content.splitlines(True):
             outputstr = self.convertline(line)
             outputlines.append(outputstr)
-        return u"".join(outputlines).encode(self.encoding)
+        return "".join(outputlines).encode(self.encoding)
 
     def _explode_gaia_plurals(self):
         """Explode the gaia plurals."""
@@ -76,7 +76,7 @@
                 if category == 'zero':
                     # [zero] cases are translated as separate units
                     continue
-                new_unit = self.inputstore.addsourceunit(u"fish") # not used
+                new_unit = self.inputstore.addsourceunit("fish") # not used
                 new_location = '%s[%s]' % (location, category)
                 new_unit.addlocation(new_location)
                 new_unit.target = text
@@ -86,7 +86,7 @@
             del self.inputstore.locationindex[location]
 
     def convertline(self, line):
-        returnline = u""
+        returnline = ""
         # handle multiline msgid if we're in one
         if self.inmultilinemsgid:
             msgid = quote.rstripeol(line).strip()
@@ -117,14 +117,14 @@
             if key in self.inputstore.locationindex:
                 unit = self.inputstore.locationindex[key]
                 if not unit.istranslated() and bool(unit.source) and self.remove_untranslated:
-                    returnline = u""
+                    returnline = ""
                 else:
                     if unit.isfuzzy() and not self.includefuzzy or len(unit.target) == 0:
                         value = unit.source
                     else:
                         value = unit.target
                     self.inecho = False
-                    assert isinstance(value, unicode)
+                    assert isinstance(value, str)
                     returnline = "%(key)s%(del)s%(value)s%(term)s%(eol)s" % \
                          {"key": "%s%s%s" % (self.personality.key_wrap_char,
                                              key,
@@ -139,7 +139,7 @@
             else:
                 self.inecho = True
                 returnline = line + eol
-        assert isinstance(returnline, unicode)
+        assert isinstance(returnline, str)
         return returnline
 
 
@@ -194,9 +194,9 @@
                                          description=__doc__)
     parser.add_option("", "--personality", dest="personality",
             default=properties.default_dialect, type="choice",
-            choices=properties.dialects.keys(),
+            choices=list(properties.dialects.keys()),
             help="override the input file format: %s (for .properties files, default: %s)" %
-                 (", ".join(properties.dialects.iterkeys()),
+                 (", ".join(iter(properties.dialects.keys())),
                   properties.default_dialect),
             metavar="TYPE")
     parser.add_option("", "--encoding", dest="encoding", default=None,
--- .\convert\po2rc.py	(original)
+++ .\convert\po2rc.py	(refactored)
@@ -61,7 +61,7 @@
 
     def convertblock(self, block):
         newblock = block
-        if isinstance(newblock, unicode):
+        if isinstance(newblock, str):
             newblock = newblock.encode('utf-8')
         if newblock.startswith("LANGUAGE"):
             return "LANGUAGE %s, %s" % (self.lang, self.sublang)
@@ -72,7 +72,7 @@
                     newmatch = unit.match.group().replace(unit.match.groupdict()['value'],
                                                           self.inputdict[location])
                     newblock = newblock.replace(unit.match.group(), newmatch)
-        if isinstance(newblock, unicode):
+        if isinstance(newblock, str):
             newblock = newblock.encode(self.charset)
         return newblock
 
--- .\convert\po2symb.py	(original)
+++ .\convert\po2symb.py	(refactored)
@@ -30,7 +30,7 @@
 
 
 def escape(text):
-    for key, val in po_escape_map.iteritems():
+    for key, val in po_escape_map.items():
         text = text.replace(key, val)
     return '"%s"' % text
 
@@ -57,7 +57,7 @@
                 key = match.groupdict()['id']
                 if key in body_replacements:
                     value = body_replacements[key].target or body_replacements[key].source
-                    ps.current_line = match.expand(u'\g<start>\g<id>\g<space>%s\n' % escape(value))
+                    ps.current_line = match.expand('\g<start>\g<id>\g<space>%s\n' % escape(value))
             ps.read_line()
     except StopIteration:
         pass
--- .\convert\po2web2py.py	(original)
+++ .\convert\po2web2py.py	(refactored)
@@ -35,7 +35,7 @@
         return
 
     def convertstore(self, inputstore, includefuzzy):
-        from StringIO import StringIO
+        from io import StringIO
         str_obj = StringIO()
 
         mydict = dict()
--- .\convert\pot2po.py	(original)
+++ .\convert\pot2po.py	(refactored)
@@ -203,7 +203,7 @@
 
     if template_store is not None and isinstance(template_store, poheader.poheader):
         templateheadervalues = template_store.parseheader()
-        for key, value in templateheadervalues.iteritems():
+        for key, value in templateheadervalues.items():
             if key == "Project-Id-Version":
                 project_id_version = value
             elif key == "Last-Translator":
@@ -225,7 +225,7 @@
                 kwargs[key] = value
 
     inputheadervalues = input_store.parseheader()
-    for key, value in inputheadervalues.iteritems():
+    for key, value in inputheadervalues.items():
         if key in ("Project-Id-Version", "Last-Translator", "Language-Team", \
                    "PO-Revision-Date", "Content-Type", \
                    "Content-Transfer-Encoding", "Plural-Forms"):
--- .\convert\prop2mozfunny.py	(original)
+++ .\convert\prop2mozfunny.py	(refactored)
@@ -44,7 +44,7 @@
             pendingblanks.append("\n")
         else:
             definition = "#define %s %s\n" % (unit.name, unit.value.replace("\n", "\\n"))
-            if isinstance(definition, unicode):
+            if isinstance(definition, str):
                 definition = definition.encode("UTF-8")
             for blank in pendingblanks:
                 yield blank
@@ -65,7 +65,7 @@
             yield ""
         else:
             definition = "%s=%s\n" % (unit.name, unit.value)
-            if isinstance(definition, unicode):
+            if isinstance(definition, str):
                 definition = definition.encode("UTF-8")
             yield definition
 
--- .\convert\prop2po.py	(original)
+++ .\convert\prop2po.py	(refactored)
@@ -117,7 +117,7 @@
             # handle the header case specially...
             if not appendedheader:
                 if origprop.isblank():
-                    targetheader.addnote(u"".join(waitingcomments).rstrip(),
+                    targetheader.addnote("".join(waitingcomments).rstrip(),
                                          "developer", position="prepend")
                     waitingcomments = []
                     origpo = None
@@ -136,7 +136,7 @@
             if origpo is not None:
                 if translatedpo is not None and not blankmsgstr:
                     origpo.target = translatedpo.source
-                origpo.addnote(u"".join(waitingcomments).rstrip(),
+                origpo.addnote("".join(waitingcomments).rstrip(),
                                "developer", position="prepend")
                 waitingcomments = []
                 thetargetfile.addunit(origpo)
@@ -152,12 +152,12 @@
         """Fold the multiple plural units of a gaia file into a gettext plural."""
         new_store = type(postore)()
         plurals = {}
-        current_plural = u""
+        current_plural = ""
         for unit in postore.units:
             if not unit.istranslatable():
                 #TODO: reconsider: we could lose header comments here
                 continue
-            if u"plural(n)" in unit.source:
+            if "plural(n)" in unit.source:
                 # start of a set of plural units
                 location = unit.getlocations()[0]
                 current_plural = location
@@ -176,7 +176,7 @@
                     new_unit = _collapse(new_store, plurals[current_plural])
                     new_unit.addlocation(current_plural)
                     del plurals[current_plural]
-                    current_plural = u""
+                    current_plural = ""
 
                 new_store.addunit(unit)
 
@@ -185,12 +185,12 @@
             new_unit = _collapse(new_store, plurals[current_plural])
             new_unit.addlocation(current_plural)
             del plurals[current_plural]
-            current_plural = u""
+            current_plural = ""
 
         # if everything went well, there should be nothing left in plurals
         if len(plurals) != 0:
             logger.warning("Not all plural units converted correctly:" +
-                           "\n".join(plurals.keys()))
+                           "\n".join(list(plurals.keys())))
         return new_store
 
     def convertunit(self, propunit, commenttype):
@@ -204,13 +204,13 @@
             for comment in propunit.comments:
                 if "DONT_TRANSLATE" in comment:
                     return "discard"
-            pounit.addnote(u"".join(propunit.getnotes()).rstrip(), commenttype)
+            pounit.addnote("".join(propunit.getnotes()).rstrip(), commenttype)
         # TODO: handle multiline msgid
         if propunit.isblank():
             return None
         pounit.addlocation(propunit.name)
         pounit.source = propunit.source
-        pounit.target = u""
+        pounit.target = ""
         return pounit
 
 
@@ -267,9 +267,9 @@
     parser.add_option("", "--personality", dest="personality",
             default=properties.default_dialect,
             type="choice",
-            choices=properties.dialects.keys(),
+            choices=list(properties.dialects.keys()),
             help="override the input file format: %s (for .properties files, default: %s)" %
-                 (", ".join(properties.dialects.iterkeys()),
+                 (", ".join(iter(properties.dialects.keys())),
                   properties.default_dialect),
             metavar="TYPE")
     parser.add_option("", "--encoding", dest="encoding", default=None,
--- .\convert\test_accesskey.py	(original)
+++ .\convert\test_accesskey.py	(refactored)
@@ -26,56 +26,56 @@
 def test_get_label_and_accesskey():
     """test that we can extract the label and accesskey components from an
     accesskey+label string"""
-    assert accesskey.extract(u"") == (u"", u"")
-    assert accesskey.extract(u"File") == (u"File", u"")
-    assert accesskey.extract(u"&File") == (u"File", u"F")
-    assert accesskey.extract(u"~File", u"~") == (u"File", u"F")
-    assert accesskey.extract(u"_File", u"_") == (u"File", u"F")
+    assert accesskey.extract("") == ("", "")
+    assert accesskey.extract("File") == ("File", "")
+    assert accesskey.extract("&File") == ("File", "F")
+    assert accesskey.extract("~File", "~") == ("File", "F")
+    assert accesskey.extract("_File", "_") == ("File", "F")
 
 
 def test_ignore_entities():
     """test that we don't get confused with entities and a & access key
     marker"""
-    assert accesskey.extract(u"Set &browserName; as &Default") != (u"Set &browserName; as &Default", u"b")
-    assert accesskey.extract(u"Set &browserName; as &Default") == (u"Set &browserName; as Default", u"D")
+    assert accesskey.extract("Set &browserName; as &Default") != ("Set &browserName; as &Default", "b")
+    assert accesskey.extract("Set &browserName; as &Default") == ("Set &browserName; as Default", "D")
 
 
 def test_alternate_accesskey_marker():
     """check that we can identify the accesskey if the marker is different"""
-    assert accesskey.extract(u"~File", u"~") == (u"File", u"F")
-    assert accesskey.extract(u"&File", u"~") == (u"&File", u"")
+    assert accesskey.extract("~File", "~") == ("File", "F")
+    assert accesskey.extract("&File", "~") == ("&File", "")
 
 
 def test_unicode():
     """test that we can do the same with unicode strings"""
--- .\convert\test_convert.py	(original)
+++ .\convert\test_convert.py	(refactored)
@@ -44,7 +44,7 @@
         argv = list(argv)
         kwoptions = getattr(self, "defaultoptions", {}).copy()
         kwoptions.update(kwargs)
-        for key, value in kwoptions.iteritems():
+        for key, value in kwoptions.items():
             if value is True:
                 argv.append("--%s" % key)
             else:
@@ -107,7 +107,7 @@
             sys.stdout = stdout
         helpfile.close()
         help_string = self.read_testfile("help.txt")
-        print help_string
+        print(help_string)
         convertsummary = self.convertmodule.__doc__.split("\n")[0]
         # the convertsummary might be wrapped. this will probably unwrap it
         assert convertsummary in help_string.replace("\n", " ")
--- .\convert\test_csv2po.py	(original)
+++ .\convert\test_csv2po.py	(refactored)
@@ -32,7 +32,7 @@
 
     def singleelement(self, storage):
         """checks that the pofile contains a single non-header element, and returns it"""
-        print str(storage)
+        print(str(storage))
         assert headerless_len(storage.units) == 1
         return first_translatable(storage)
 
@@ -74,7 +74,7 @@
         unit = self.singleelement(pofile)
         assert unit.getlocations() == ['Random comment\nwith continuation']
         assert unit.source == "Original text"
-        print unit.target
+        print(unit.target)
         assert unit.target == "Langdradige teks\nwat lank aanhou"
 
     def test_tabs(self):
@@ -82,7 +82,7 @@
         minicsv = ',"First column\tSecond column","Twee kolomme gesky met \t"'
         pofile = self.csv2po(minicsv)
         unit = self.singleelement(pofile)
-        print unit.source
+        print(unit.source)
         assert unit.source == "First column\tSecond column"
         assert not pofile.findunit("First column\tSecond column").target == "Twee kolomme gesky met \\t"
 
@@ -90,18 +90,18 @@
         """Test the escaping of quotes (and slash)"""
         minicsv = r''',"Hello ""Everyone""","Good day ""All"""
 ,"Use \"".","Gebruik \""."'''
-        print minicsv
+        print(minicsv)
         csvfile = csvl10n.csvfile(wStringIO.StringIO(minicsv))
-        print str(csvfile)
+        print(str(csvfile))
         pofile = self.csv2po(minicsv)
         unit = first_translatable(pofile)
         assert unit.source == 'Hello "Everyone"'
         assert pofile.findunit('Hello "Everyone"').target == 'Good day "All"'
-        print str(pofile)
+        print(str(pofile))
         for unit in pofile.units:
-            print unit.source
-            print unit.target
-            print
+            print(unit.source)
+            print(unit.target)
+            print()
 #        assert pofile.findunit('Use \\".').target == 'Gebruik \\".'
 
     def test_empties(self):
--- .\convert\test_dtd2po.py	(original)
+++ .\convert\test_dtd2po.py	(refactored)
@@ -37,7 +37,7 @@
         """checks that the pofile contains a single non-header element, and returns it"""
         assert len(pofile.units) == 2
         assert pofile.units[0].isheader()
-        print pofile.units[1]
+        print(pofile.units[1])
         return pofile.units[1]
 
     def countelements(self, pofile):
@@ -83,7 +83,7 @@
         dtdsource = """<!ENTITY test.metoo '"Bananas" for sale'>\n"""
         pofile = self.dtd2po(dtdsource)
         pounit = self.singleelement(pofile)
-        print str(pounit)
+        print(str(pounit))
         assert pounit.source == '"Bananas" for sale'
 
     def test_emptyentity(self):
@@ -98,8 +98,8 @@
         """checks that two empty entitu definitions have correct context (bug 2190)."""
         dtdsource = '<!ENTITY community.exp.start "">\n<!ENTITY contribute.end "">\n'
         pofile = self.dtd2po(dtdsource)
-        assert pofile.units[-2].getcontext() == u"community.exp.start"
-        assert pofile.units[-1].getcontext() == u"contribute.end"
+        assert pofile.units[-2].getcontext() == "community.exp.start"
+        assert pofile.units[-1].getcontext() == "contribute.end"
 
     def test_emptyentity_translated(self):
         """checks that if we translate an empty entity it makes it into the PO, bug 101"""
@@ -107,7 +107,7 @@
         dtdsource = '<!ENTITY credit.translation "Translators Names">\n'
         pofile = self.dtd2po(dtdsource, dtdtemplate)
         unit = self.singleelement(pofile)
-        print unit
+        print(unit)
         assert "credit.translation" in str(unit)
         # We don't want this to simply be seen as a header:
         assert len(unit.getid()) != 0
@@ -124,7 +124,7 @@
 '''
         pofile = self.dtd2po(dtdsource)
         posource = str(pofile)
-        print posource
+        print(posource)
         assert posource.count('#.') == 5  # 1 Header extracted from, 3 comment lines, 1 autoinserted comment
 
     def test_localisation_note_merge(self):
@@ -134,7 +134,7 @@
         dtdsource = dtdtemplate % ("note1.label", "note1.label") + dtdtemplate % ("note2.label", "note2.label")
         pofile = self.dtd2po(dtdsource)
         posource = str(pofile.units[1]) + str(pofile.units[2])
-        print posource
+        print(posource)
         assert posource.count('#.') == 2
         assert posource.count('msgctxt') == 2
 
@@ -258,7 +258,7 @@
         dtdsource = '<!ENTITY mainWindow.titlemodifiermenuseparator " - with a newline\n    and more text">'
         pofile = self.dtd2po(dtdsource)
         unit = self.singleelement(pofile)
-        print repr(unit.source)
+        print(repr(unit.source))
         assert unit.source == " - with a newline \nand more text"
 
     def test_escaping_newline_tabs(self):
@@ -269,8 +269,8 @@
         thedtd.parse(dtdsource)
         thepo = po.pounit()
         converter.convertstrings(thedtd, thepo)
-        print thedtd
-        print thepo.source
+        print(thedtd)
+        print(thepo.source)
         # \n in a dtd should also appear as \n in the PO file
         assert thepo.source == r"A hard coded newline.\nAnd tab\t and a \r carriage return."
 
@@ -304,11 +304,11 @@
--- .\convert\test_html2po.py	(original)
+++ .\convert\test_html2po.py	(refactored)
@@ -31,16 +31,16 @@
         if actual > 0:
             if pofile.units[0].isheader():
                 actual = actual - 1
-        print pofile
+        print(pofile)
         assert actual == expected
 
     def compareunit(self, pofile, unitnumber, expected):
         """helper to validate a PO message"""
         if not pofile.units[0].isheader():
             unitnumber = unitnumber - 1
-        print 'unit source: ' + pofile.units[unitnumber].source.encode('utf-8') + '|'
-        print 'expected: ' + expected.encode('utf-8') + '|'
-        assert unicode(pofile.units[unitnumber].source) == unicode(expected)
+        print('unit source: ' + pofile.units[unitnumber].source.encode('utf-8') + '|')
+        print('expected: ' + expected.encode('utf-8') + '|')
+        assert str(pofile.units[unitnumber].source) == str(expected)
 
     def check_single(self, markup, itemtext):
         """checks that converting this markup produces a single element with value itemtext"""
@@ -323,8 +323,8 @@
         pofile = self.html2po(htmlsource)
 
         self.countunits(pofile, 4)
-        self.compareunit(pofile, 3, u'We aim to please \x96 will you aim too, please?')
-        self.compareunit(pofile, 4, u'South Africa\x92s language diversity can be challenging.')
+        self.compareunit(pofile, 3, 'We aim to please \x96 will you aim too, please?')
+        self.compareunit(pofile, 4, 'South Africa\x92s language diversity can be challenging.')
 
     def test_strip_html(self):
         """Ensure that unnecessary html is stripped from the resulting unit."""
@@ -362,8 +362,8 @@
 '''
         pofile = self.html2po(htmlsource)
         self.countunits(pofile, 3)
-        self.compareunit(pofile, 2, u'Projects')
-        self.compareunit(pofile, 3, u'Home Page')
+        self.compareunit(pofile, 2, 'Projects')
+        self.compareunit(pofile, 3, 'Home Page')
 
         # Translate and convert back:
         pofile.units[2].target = 'Projekte'
@@ -422,7 +422,7 @@
         pofile = self.html2po('<!-- comment outside block --><p><!-- a comment -->A paragraph<!-- with another comment -->.</p>', keepcomments=True)
         self.compareunit(pofile, 1, 'A paragraph.')
         notes = pofile.getunits()[-1].getnotes()
-        assert unicode(notes) == ' a comment \n with another comment '
+        assert str(notes) == ' a comment \n with another comment '
 
 
 class TestHTML2POCommand(test_convert.TestConvertCommand, TestHTML2PO):
--- .\convert\test_json2po.py	(original)
+++ .\convert\test_json2po.py	(refactored)
@@ -18,7 +18,7 @@
 
     def singleelement(self, storage):
         """checks that the pofile contains a single non-header element, and returns it"""
-        print str(storage)
+        print(str(storage))
         assert len(storage.units) == 1
         return storage.units[0]
 
@@ -71,7 +71,7 @@
 
         poresult = self.json2po(jsonsource)
         assert poresult.units[0].isheader()
-        print len(poresult.units)
+        print(len(poresult.units))
         assert len(poresult.units) == 11
 
 
--- .\convert\test_mozfunny2prop.py	(original)
+++ .\convert\test_mozfunny2prop.py	(refactored)
@@ -25,13 +25,13 @@
         """checks that the pofile contains a single non-header element, and returns it"""
         assert len(pofile.units) == 2
         assert pofile.units[0].isheader()
-        print pofile
+        print(pofile)
         return pofile.units[1]
 
     def countelements(self, pofile):
         """counts the number of non-header entries"""
         assert pofile.units[0].isheader()
-        print pofile
+        print(pofile)
         return len(pofile.units) - 1
 
     def test_simpleentry(self):
--- .\convert\test_mozlang2po.py	(original)
+++ .\convert\test_mozlang2po.py	(refactored)
@@ -32,13 +32,13 @@
         """checks that the pofile contains a single non-header element, and returns it"""
         assert len(pofile.units) == 2
         assert pofile.units[0].isheader()
-        print pofile
+        print(pofile)
         return pofile.units[1]
 
     def countelements(self, pofile):
         """counts the number of non-header entries"""
         assert pofile.units[0].isheader()
-        print pofile
+        print(pofile)
         return len(pofile.units) - 1
 
     def test_simpleentry(self):
--- .\convert\test_oo2po.py	(original)
+++ .\convert\test_oo2po.py	(refactored)
@@ -2,9 +2,9 @@
 # -*- coding: utf-8 -*-
 
 import os
-import urlparse
+import urllib.parse
 try:
-    from urlparse import parse_qs
+    from urllib.parse import parse_qs
 except ImportError:
     from cgi import parse_qs
 
@@ -59,7 +59,7 @@
         oooutputfile = wStringIO.StringIO()
         po2oo.convertoo(poinputfile, oooutputfile, ootemplatefile, targetlanguage="en-US")
         ooresult = oooutputfile.getvalue()
-        print "original oo:\n", oosource, "po version:\n", posource, "output oo:\n", ooresult
+        print("original oo:\n", oosource, "po version:\n", posource, "output oo:\n", ooresult)
         return ooresult.split('\t')[10]
 
     def check_roundtrip(self, filename, text):
@@ -80,7 +80,7 @@
         pofile = self.convert(oosource)
         pounit = self.singleelement(pofile)
         poelementsrc = str(pounit)
-        print poelementsrc
+        print(poelementsrc)
         assert "Newline \n Newline" in pounit.source
         assert "Tab \t Tab" in pounit.source
         assert "CR \r CR" in pounit.source
@@ -106,7 +106,7 @@
         pofile = self.convert(oosource)
         pounit = self.singleelement(pofile)
         poelementsrc = str(pounit)
-        print poelementsrc
+        print(poelementsrc)
         assert pounit.source == r"\<"
 
     def test_escapes_helpcontent2(self):
@@ -115,7 +115,7 @@
         pofile = self.convert(oosource)
         pounit = self.singleelement(pofile)
         poelementsrc = str(pounit)
-        print poelementsrc
+        print(poelementsrc)
         assert pounit.source == r'size *2 \langle x \rangle'
 
     def test_msgid_bug_error_address(self):
@@ -124,14 +124,14 @@
         pofile = self.convert(oosource)
         assert pofile.units[0].isheader()
         assert pofile.parseheader()["Report-Msgid-Bugs-To"]
-        bug_url = urlparse.urlparse(pofile.parseheader()["Report-Msgid-Bugs-To"])
-        print bug_url
+        bug_url = urllib.parse.urlparse(pofile.parseheader()["Report-Msgid-Bugs-To"])
+        print(bug_url)
         assert bug_url[:3] == ("http", "qa.openoffice.org", "/issues/enter_bug.cgi")
-        assert parse_qs(bug_url[4], True) == {u'comment': [u''],
-                                                       u'component': [u'l10n'],
-                                                       u'form_name': [u'enter_issue'],
-                                                       u'short_desc': [u'Localization issue in file: '],
-                                                       u'subcomponent': [u'ui'],
+        assert parse_qs(bug_url[4], True) == {'comment': [''],
+                                                       'component': ['l10n'],
+                                                       'form_name': ['enter_issue'],
+                                                       'short_desc': ['Localization issue in file: '],
+                                                       'subcomponent': ['ui'],
                                                       }
 
     def test_x_comment_inclusion(self):
@@ -239,4 +239,4 @@
         self.run_command("simple.oo", "simple.po", language="fr", multifile="onefile", error="traceback", duplicates="merge")
         pofile = self.target_filetype(self.open_testfile("simple.po"))
         assert len(pofile.units) == 2
--- .\convert\test_php2po.py	(original)
+++ .\convert\test_php2po.py	(refactored)
@@ -35,13 +35,13 @@
         """checks that the pofile contains a single non-header element, and returns it"""
         assert len(pofile.units) == 2
         assert pofile.units[0].isheader()
-        print pofile
+        print(pofile)
         return pofile.units[1]
 
     def countelements(self, pofile):
         """counts the number of non-header entries"""
         assert pofile.units[0].isheader()
-        print pofile
+        print(pofile)
         return len(pofile.units) - 1
 
     def test_simpleentry(self):
@@ -63,13 +63,13 @@
 
     def test_unicode(self):
         """checks that unicode entries convert properly"""
-        unistring = u'Norsk bokm\u00E5l'
+        unistring = 'Norsk bokm\u00E5l'
         phpsource = """$lang['nb'] = '%s';""" % unistring
         pofile = self.php2po(phpsource)
         pounit = self.singleelement(pofile)
-        print repr(pofile.units[0].target)
-        print repr(pounit.source)
-        assert pounit.source == u'Norsk bokm\u00E5l'
+        print(repr(pofile.units[0].target))
+        print(repr(pounit.source))
+        assert pounit.source == 'Norsk bokm\u00E5l'
 
     def test_multiline(self):
         """checks that multiline enties can be parsed"""
@@ -77,7 +77,7 @@
 of connections to this server. If so, use the Advanced IMAP Server Settings dialog to
 reduce the number of cached connections.';"""
         pofile = self.php2po(phpsource)
-        print repr(pofile.units[1].target)
+        print(repr(pofile.units[1].target))
         assert self.countelements(pofile) == 1
 
     def test_comments_before(self):
--- .\convert\test_po2csv.py	(original)
+++ .\convert\test_po2csv.py	(refactored)
@@ -102,20 +102,20 @@
 msgstr "Vind\\Opsies"
 '''
         csvfile = self.po2csv(minipo)
-        print minipo
-        print csvfile
+        print(minipo)
+        print(csvfile)
         assert csvfile.findunit(r'Find\Options').target == r'Vind\Opsies'
 
     def test_singlequotes(self):
         """Tests that single quotes are preserved correctly"""
         minipo = '''msgid "source 'source'"\nmsgstr "target 'target'"\n'''
         csvfile = self.po2csv(minipo)
-        print str(csvfile)
+        print(str(csvfile))
         assert csvfile.findunit("source 'source'").target == "target 'target'"
         # Make sure we don't mess with start quotes until writing
         minipo = '''msgid "'source'"\nmsgstr "'target'"\n'''
         csvfile = self.po2csv(minipo)
-        print str(csvfile)
+        print(str(csvfile))
         assert csvfile.findunit(r"'source'").target == r"'target'"
         # TODO check that we escape on writing not in the internal representation
 
--- .\convert\test_po2dtd.py	(original)
+++ .\convert\test_po2dtd.py	(refactored)
@@ -62,7 +62,7 @@
         dtdresult = dtdoutputfile.getvalue()
         print_string = "Original DTD:\n%s\n\nPO version:\n%s\n\n"
         print_string = print_string + "Output DTD:\n%s\n################"
-        print print_string % (dtdsource, posource, dtdresult)
+        print(print_string % (dtdsource, posource, dtdresult))
         return dtdresult
 
     def roundtripstring(self, entitystring):
@@ -163,7 +163,7 @@
 <!ENTITY searchIntegration.label       "Allow &searchIntegration.engineName; to search messages">'''
         dtdfile = self.merge2dtd(dtd_snippet, po_snippet)
         dtdsource = str(dtdfile)
-        print dtdsource
+        print(dtdsource)
--- .\convert\test_po2html.py	(original)
+++ .\convert\test_po2html.py	(refactored)
@@ -12,11 +12,11 @@
     def converthtml(self, posource, htmltemplate):
         """helper to exercise the command line function"""
         inputfile = wStringIO.StringIO(posource)
-        print inputfile.getvalue()
+        print(inputfile.getvalue())
         outputfile = wStringIO.StringIO()
         templatefile = wStringIO.StringIO(htmltemplate)
         assert po2html.converthtml(inputfile, outputfile, templatefile)
-        print outputfile.getvalue()
+        print(outputfile.getvalue())
         return outputfile.getvalue()
 
     def test_simple(self):
--- .\convert\test_po2ical.py	(original)
+++ .\convert\test_po2ical.py	(refactored)
@@ -44,19 +44,19 @@
         #templateprop = properties.propfile(templatefile)
         convertor = po2ical.reical(templatefile, inputpo)
         outputical = convertor.convertstore()
-        print outputical
+        print(outputical)
         return outputical
 
     def test_simple_summary(self):
         """test that we output correctly for Inno files."""
-        posource = ur'''#: [uid1@example.com]SUMMARY
+        posource = r'''#: [uid1@example.com]SUMMARY
 msgid "Value"
 msgstr "Waarde"
 '''
         icaltemplate = icalboiler % "Value"
         icalexpected = icalboiler % "Waarde"
         icalfile = self.merge2ical(icaltemplate, posource)
-        print icalexpected
+        print(icalexpected)
         assert icalfile == icalexpected
 
     # FIXME we should also test for DESCRIPTION, LOCATION and COMMENT
--- .\convert\test_po2ini.py	(original)
+++ .\convert\test_po2ini.py	(refactored)
@@ -29,7 +29,7 @@
         templatefile = wStringIO.StringIO(inisource)
         convertor = po2ini.reini(templatefile, inputpo, dialect=dialect)
         outputini = convertor.convertstore()
-        print outputini
+        print(outputini)
         return outputini
 
     def test_merging_simple(self):
@@ -38,7 +38,7 @@
         initemplate = '''[section]\nprop=value\n'''
         iniexpected = '''[section]\nprop=waarde\n'''
         inifile = self.merge2ini(initemplate, posource)
-        print inifile
+        print(inifile)
         assert inifile == iniexpected
 
     def test_space_preservation(self):
@@ -47,7 +47,7 @@
         initemplate = '''[section]\nprop  =  value\n'''
         iniexpected = '''[section]\nprop  =  waarde\n'''
         inifile = self.merge2ini(initemplate, posource)
-        print inifile
+        print(inifile)
         assert inifile == iniexpected
 
     def test_merging_blank_entries(self):
@@ -60,7 +60,7 @@
         initemplate = '[section]\naccesskey-accept=\n'
         iniexpected = '[section]\naccesskey-accept=\n'
         inifile = self.merge2ini(initemplate, posource)
-        print inifile
+        print(inifile)
         assert inifile == iniexpected
 
     def test_merging_fuzzy(self):
@@ -69,7 +69,7 @@
         initemplate = '''[section]\nprop=value\n'''
         iniexpected = '''[section]\nprop=value\n'''
         inifile = self.merge2ini(initemplate, posource)
-        print inifile
+        print(inifile)
         assert inifile == iniexpected
 
     def test_merging_propertyless_template(self):
@@ -78,7 +78,7 @@
         initemplate = "# A comment\n"
         iniexpected = initemplate
         inifile = self.merge2ini(initemplate, posource)
-        print inifile
+        print(inifile)
         assert inifile == iniexpected
 
     def test_empty_value(self):
@@ -91,19 +91,19 @@
         initemplate = '''[section]\nkey =\n'''
         iniexpected = '''[section]\nkey =translated\n'''
         inifile = self.merge2ini(initemplate, posource)
-        print inifile
+        print(inifile)
         assert inifile == iniexpected
 
     def test_dialects_inno(self):
         """test that we output correctly for Inno files."""
-        posource = ur'''#: [section]prop
+        posource = r'''#: [section]prop
 msgid "value\tvalue2\n"
--- .\convert\test_po2mozlang.py	(original)
+++ .\convert\test_po2mozlang.py	(refactored)
@@ -22,7 +22,7 @@
         posource = '''#: prop\nmsgid "Source"\nmsgstr "Target"\n'''
         propexpected = ''';Source\nTarget\n'''
         langfile = self.po2lang(posource)
-        print langfile
+        print(langfile)
         assert str(langfile) == propexpected
 
     def test_comment(self):
@@ -30,7 +30,7 @@
         posource = '''#. Comment\n#: prop\nmsgid "Source"\nmsgstr "Target"\n'''
         propexpected = '''# Comment\n;Source\nTarget\n'''
         langfile = self.po2lang(posource)
-        print langfile
+        print(langfile)
         assert str(langfile) == propexpected
 
     def test_fuzzy(self):
@@ -38,7 +38,7 @@
         posource = '''#. Comment\n#: prop\n#, fuzzy\nmsgid "Source"\nmsgstr "Target"\n'''
         propexpected = '''# Comment\n;Source\nSource\n'''
         langfile = self.po2lang(posource)
-        print langfile
+        print(langfile)
         assert str(langfile) == propexpected
 
     def test_ok_marker(self):
@@ -46,7 +46,7 @@
         posource = '''#: prop\nmsgid "Same"\nmsgstr "Same"\n'''
         propexpected = ''';Same\nSame {ok}\n'''
         langfile = self.po2lang(posource)
-        print langfile
+        print(langfile)
         assert str(langfile) == propexpected
 
 
--- .\convert\test_po2oo.py	(original)
+++ .\convert\test_po2oo.py	(refactored)
@@ -41,7 +41,7 @@
         oooutputfile = wStringIO.StringIO()
         po2oo.convertoo(poinputfile, oooutputfile, ootemplatefile, targetlanguage="en-US")
         ooresult = oooutputfile.getvalue()
-        print "original oo:\n", oosource, "po version:\n", posource, "output oo:\n", ooresult
+        print("original oo:\n", oosource, "po version:\n", posource, "output oo:\n", ooresult)
         assert ooresult.startswith(oointro) and ooresult.endswith(oooutro)
         return ooresult[len(oointro):-len(oooutro)]
 
@@ -101,7 +101,7 @@
         # once we've fixed that.
         """checks that (escaped) quotes in strings make it through a oo->po->oo roundtrip"""
         self.check_roundtrip(" ")
-        self.check_roundtrip(u"\u00a0")
+        self.check_roundtrip("\u00a0")
 
     def test_default_timestamp(self):
         """test to ensure that we revert to the default timestamp"""
--- .\convert\test_po2php.py	(original)
+++ .\convert\test_po2php.py	(refactored)
@@ -27,7 +27,7 @@
         #templatephp = php.phpfile(templatefile)
         convertor = po2php.rephp(templatefile, inputpo)
         outputphp = convertor.convertstore()
-        print outputphp
+        print(outputphp)
         return outputphp
 
     def test_merging_simple(self):
@@ -36,7 +36,7 @@
         phptemplate = '''$lang['name'] = 'value';\n'''
         phpexpected = '''$lang['name'] = 'waarde';\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile
+        print(phpfile)
         assert phpfile == [phpexpected]
 
     def test_space_preservation(self):
@@ -45,7 +45,7 @@
         phptemplate = '''$lang['name']  =  'value';\n'''
         phpexpected = '''$lang['name']  =  'waarde';\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile
+        print(phpfile)
         assert phpfile == [phpexpected]
 
     def test_merging_blank_entries(self):
@@ -58,7 +58,7 @@
         phptemplate = '''$lang['accesskey-accept'] = '';\n'''
         phpexpected = '''$lang['accesskey-accept'] = '';\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile
+        print(phpfile)
         assert phpfile == [phpexpected]
 
     def test_merging_fuzzy(self):
@@ -67,7 +67,7 @@
         phptemplate = '''$lang['name']  =  'value';\n'''
         phpexpected = '''$lang['name']  =  'value';\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile
+        print(phpfile)
         assert phpfile == [phpexpected]
 
     def test_locations_with_spaces(self):
@@ -76,7 +76,7 @@
         phptemplate = '''$lang[ 'name' ]  =  'value';\n'''
         phpexpected = '''$lang[ 'name' ]  =  'waarde';\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile
+        print(phpfile)
         assert phpfile == [phpexpected]
 
     def test_inline_comments(self):
@@ -85,7 +85,7 @@
         phptemplate = '''$lang[ 'name' ]  =  'value'; //inline comment\n'''
         phpexpected = '''$lang[ 'name' ]  =  'waarde'; //inline comment\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile
+        print(phpfile)
         assert phpfile == [phpexpected]
 
     def test_named_variables(self):
@@ -97,7 +97,7 @@
         phptemplate = '''$dictYear = 'Year';\n'''
         phpexpected = '''$dictYear = 'Jaar';\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile
+        print(phpfile)
         assert phpfile == [phpexpected]
 
     def test_multiline(self):
@@ -119,7 +119,7 @@
 <p>Once you do this you can not go back again.</p>
 <p>Are you sure you want to upgrade this server to this version?</p>';\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile[0]
+        print(phpfile[0])
         assert phpfile[0] == phptemplate
 
     def test_hash_comment(self):
@@ -131,7 +131,7 @@
         phptemplate = '''# inside alt= stuffies\n$variable = 'stringy';\n'''
         phpexpected = '''# inside alt= stuffies\n$variable = 'stringetjie';\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile
+        print(phpfile)
         assert "".join(phpfile) == phpexpected
 
     def test_arrays(self):
@@ -140,7 +140,7 @@
         phptemplate = '''$lang = array(\n    'name' => 'value',\n);\n'''
         phpexpected = '''$lang = array(\n    'name' => 'waarde',\n);\n'''
         phpfile = self.merge2php(phptemplate, posource)
-        print phpfile
+        print(phpfile)
         assert "".join(phpfile) == phpexpected
 
     @mark.xfail(reason="Need to review if we want this behaviour")
@@ -150,7 +150,7 @@
         proptemplate = "# A comment\n"
         propexpected = proptemplate
         propfile = self.merge2prop(proptemplate, posource)
-        print propfile
+        print(propfile)
         assert propfile == [propexpected]
 
 
--- .\convert\test_po2prop.py	(original)
+++ .\convert\test_po2prop.py	(refactored)
@@ -25,7 +25,7 @@
         #templateprop = properties.propfile(templatefile)
         convertor = po2prop.reprop(templatefile, inputpo, personality=personality, remove_untranslated=remove_untranslated)
         outputprop = convertor.convertstore()
-        print outputprop
+        print(outputprop)
         return outputprop
 
     def test_merging_simple(self):
@@ -34,7 +34,7 @@
         proptemplate = '''prop=value\n'''
         propexpected = '''prop=waarde\n'''
         propfile = self.merge2prop(proptemplate, posource)
-        print propfile
+        print(propfile)
         assert propfile == propexpected
 
     def test_merging_untranslated(self):
@@ -43,7 +43,7 @@
         proptemplate = '''prop=value\n'''
         propexpected = proptemplate
         propfile = self.merge2prop(proptemplate, posource)
-        print propfile
+        print(propfile)
         assert propfile == propexpected
 
     def test_hard_newlines_preserved(self):
@@ -52,7 +52,7 @@
         proptemplate = '''prop=\\nvalue\\n\\n\n'''
         propexpected = '''prop=\\nwaarde\\n\\n\n'''
         propfile = self.merge2prop(proptemplate, posource)
-        print propfile
+        print(propfile)
         assert propfile == propexpected
 
     def test_space_preservation(self):
@@ -61,7 +61,7 @@
         proptemplate = '''prop  =  value\n'''
         propexpected = '''prop  =  waarde\n'''
         propfile = self.merge2prop(proptemplate, posource)
-        print propfile
+        print(propfile)
         assert propfile == propexpected
 
     def test_merging_blank_entries(self):
@@ -74,7 +74,7 @@
         proptemplate = 'accesskey-accept=\n'
         propexpected = 'accesskey-accept=\n'
         propfile = self.merge2prop(proptemplate, posource)
-        print propfile
+        print(propfile)
         assert propfile == propexpected
 
     def test_merging_fuzzy(self):
@@ -83,7 +83,7 @@
         proptemplate = '''prop=value\n'''
         propexpected = '''prop=value\n'''
         propfile = self.merge2prop(proptemplate, posource)
-        print propfile
+        print(propfile)
         assert propfile == propexpected
 
     def test_merging_propertyless_template(self):
@@ -92,7 +92,7 @@
         proptemplate = "# A comment\n"
         propexpected = proptemplate
         propfile = self.merge2prop(proptemplate, posource)
-        print propfile
+        print(propfile)
         assert propfile == propexpected
 
     def test_delimiters(self):
@@ -101,9 +101,9 @@
         proptemplate = '''prop %s value\n'''
         propexpected = '''prop %s translated\n'''
         for delim in ['=', ':', '']:
-            print "testing '%s' as delimiter" % delim
+            print("testing '%s' as delimiter" % delim)
             propfile = self.merge2prop(proptemplate % delim, posource)
-            print propfile
+            print(propfile)
             assert propfile == propexpected % delim
 
     def test_empty_value(self):
@@ -116,28 +116,28 @@
         proptemplate = '''key\n'''
         propexpected = '''key = translated\n'''
         propfile = self.merge2prop(proptemplate, posource)
-        print propfile
+        print(propfile)
         assert propfile == propexpected
 
     def test_personalities(self):
         """test that we output correctly for Java and Mozilla style property files.  Mozilla uses Unicode, while Java uses escaped Unicode"""
--- .\convert\test_po2sub.py	(original)
+++ .\convert\test_po2sub.py	(refactored)
@@ -32,12 +32,12 @@
         templatefile = wStringIO.StringIO(subsource)
         convertor = po2sub.resub(templatefile, inputpo)
         outputsub = convertor.convertstore()
-        print outputsub
+        print(outputsub)
         return outputsub
 
     def test_subrip(self):
         """test SubRip or .srt files."""
-        posource = u'''#: 00:00:20.000-->00:00:24.400
+        posource = '''#: 00:00:20.000-->00:00:24.400
 msgid "Altocumulus clouds occur between six thousand"
 msgstr "Blah blah blah blah"
 
@@ -62,7 +62,7 @@
 Koei koei koei koei
 '''
         subfile = self.merge2sub(subtemplate, posource)
-        print subexpected
+        print(subexpected)
         assert subfile == subexpected
 
 
--- .\convert\test_po2tmx.py	(original)
+++ .\convert\test_po2tmx.py	(refactored)
@@ -42,8 +42,8 @@
 msgstr "Toepassings"
 """
         tmx = self.po2tmx(minipo)
-        print "The generated xml:"
-        print str(tmx)
+        print("The generated xml:")
+        print(str(tmx))
         assert tmx.translate("Applications") == "Toepassings"
         assert tmx.translate("bla") is None
         xmltext = str(tmx)
@@ -58,16 +58,16 @@
     def test_sourcelanguage(self):
         minipo = 'msgid "String"\nmsgstr "String"\n'
         tmx = self.po2tmx(minipo, sourcelanguage="xh")
-        print "The generated xml:"
-        print str(tmx)
+        print("The generated xml:")
+        print(str(tmx))
         header = tmx.document.find("header")
         assert header.get("srclang") == "xh"
 
     def test_targetlanguage(self):
         minipo = 'msgid "String"\nmsgstr "String"\n'
         tmx = self.po2tmx(minipo, targetlanguage="xh")
-        print "The generated xml:"
-        print str(tmx)
+        print("The generated xml:")
+        print(str(tmx))
         tuv = tmx.document.findall(".//%s" % tmx.namespaced("tuv"))[1]
         #tag[0] will be the source, we want the target tuv
         assert tuv.get("{%s}lang" % XML_NS) == "xh"
@@ -79,8 +79,8 @@
 msgstr "Eerste deel "
 "en ekstra"'''
         tmx = self.po2tmx(minipo)
-        print "The generated xml:"
-        print str(tmx)
+        print("The generated xml:")
+        print(str(tmx))
         assert tmx.translate('First part and extra') == 'Eerste deel en ekstra'
 
     def test_escapednewlines(self):
@@ -89,8 +89,8 @@
 msgstr "Eerste lyn\nTweede lyn"
 '''
         tmx = self.po2tmx(minipo)
-        print "The generated xml:"
-        print str(tmx)
+        print("The generated xml:")
+        print(str(tmx))
         assert tmx.translate("First line\nSecond line") == "Eerste lyn\nTweede lyn"
 
     def test_escapedtabs(self):
@@ -99,8 +99,8 @@
 msgstr "Eerste kolom\tTweede kolom"
 '''
         tmx = self.po2tmx(minipo)
-        print "The generated xml:"
-        print str(tmx)
+        print("The generated xml:")
+        print(str(tmx))
         assert tmx.translate("First column\tSecond column") == "Eerste kolom\tTweede kolom"
 
     def test_escapedquotes(self):
@@ -112,8 +112,8 @@
 msgstr "Gebruik \\\"."
 '''
         tmx = self.po2tmx(minipo)
-        print "The generated xml:"
-        print str(tmx)
+        print("The generated xml:")
+        print(str(tmx))
         assert tmx.translate('Hello "Everyone"') == 'Good day "All"'
         assert tmx.translate(r'Use \".') == r'Gebruik \".'
 
@@ -130,8 +130,8 @@
 msgstr "Drie"
 '''
         tmx = self.po2tmx(minipo)
-        print "The generated xml:"
-        print str(tmx)
+        print("The generated xml:")
+        print(str(tmx))
         assert "<tu" not in str(tmx)
         assert len(tmx.units) == 0
 
@@ -141,8 +141,8 @@
 msgstr "Bzier-kurwe"
 '''
         tmx = self.po2tmx(minipo)
-        print str(tmx)
-        assert tmx.translate(u"Bzier curve") == u"Bzier-kurwe"
+        print(str(tmx))
+        assert tmx.translate("Bzier curve") == "Bzier-kurwe"
 
     def test_nonecomments(self):
         """Tests that none comments are imported."""
@@ -151,8 +151,8 @@
 msgstr "Bzier-kurwe"
 '''
         tmx = self.po2tmx(minipo)
-        print str(tmx)
-        unit = tmx.findunits(u"Bzier curve")
+        print(str(tmx))
+        unit = tmx.findunits("Bzier curve")
         assert len(unit[0].getnotes()) == 0
 
     def test_otherscomments(self):
@@ -162,9 +162,9 @@
 msgstr "Bzier-kurwe"
 '''
         tmx = self.po2tmx(minipo, comment='others')
-        print str(tmx)
-        unit = tmx.findunits(u"Bzier curve")
-        assert unit[0].getnotes() == u"My comment rules"
+        print(str(tmx))
+        unit = tmx.findunits("Bzier curve")
+        assert unit[0].getnotes() == "My comment rules"
 
     def test_sourcecomments(self):
         """Tests that source comments are imported."""
@@ -173,9 +173,9 @@
 msgstr "Bzier-kurwe"
 '''
         tmx = self.po2tmx(minipo, comment='source')
-        print str(tmx)
-        unit = tmx.findunits(u"Bzier curve")
-        assert unit[0].getnotes() == u": ../PuzzleFourSided.h:45"
+        print(str(tmx))
+        unit = tmx.findunits("Bzier curve")
+        assert unit[0].getnotes() == ": ../PuzzleFourSided.h:45"
 
     def test_typecomments(self):
         """Tests that others comments are imported."""
@@ -184,9 +184,9 @@
 msgstr "Bzier-kurwe"
 '''
         tmx = self.po2tmx(minipo, comment='type')
-        print str(tmx)
-        unit = tmx.findunits(u"Bzier curve")
-        assert unit[0].getnotes() == u", csharp-format"
+        print(str(tmx))
+        unit = tmx.findunits("Bzier curve")
+        assert unit[0].getnotes() == ", csharp-format"
 
 class TestPO2TMXCommand(test_convert.TestConvertCommand, TestPO2TMX):
     """Tests running actual po2tmx commands on files"""
--- .\convert\test_po2ts.py	(original)
+++ .\convert\test_po2ts.py	(refactored)
@@ -27,7 +27,7 @@
 msgid "Term"
 msgstr "asdf"'''
         tsfile = self.po2ts(minipo)
-        print tsfile
+        print(tsfile)
         assert "<name>term.cpp</name>" in tsfile
         assert "<source>Term</source>" in tsfile
         assert "<translation>asdf</translation>" in tsfile
@@ -42,7 +42,7 @@
 msgstr "Target"
 '''
         tsfile = self.po2ts(posource)
-        print tsfile
+        print(tsfile)
         # The other section are a duplicate of test_simplentry
         # FIXME need to think about auto vs trans comments maybe in TS v1.1
         assert "<comment>Translator comment</comment>" in tsfile
@@ -54,7 +54,7 @@
 msgid "Source"
 msgstr "Target"'''
         tsfile = self.po2ts(posource)
-        print tsfile
+        print(tsfile)
         assert '''<translation type="unfinished">Target</translation>''' in tsfile
 
     def test_obsolete(self):
@@ -64,7 +64,7 @@
 msgid "Source"
 msgstr "Target"'''
         tsfile = self.po2ts(posource)
-        print tsfile
+        print(tsfile)
         assert '''<translation type="obsolete">Target</translation>''' in tsfile
 
     def test_duplicates(self):
@@ -78,7 +78,7 @@
 msgstr "b"
 '''
         tsfile = self.po2ts(posource)
-        print tsfile
+        print(tsfile)
         assert tsfile.find("English") != tsfile.rfind("English")
 
 
--- .\convert\test_po2txt.py	(original)
+++ .\convert\test_po2txt.py	(refactored)
@@ -11,14 +11,14 @@
     def po2txt(self, posource, txttemplate=None):
         """helper that converts po source to txt source without requiring files"""
         inputfile = wStringIO.StringIO(posource)
-        print inputfile.getvalue()
+        print(inputfile.getvalue())
         outputfile = wStringIO.StringIO()
         if txttemplate:
             templatefile = wStringIO.StringIO(txttemplate)
         else:
             templatefile = None
         assert po2txt.converttxt(inputfile, outputfile, templatefile)
-        print outputfile.getvalue()
+        print(outputfile.getvalue())
         return outputfile.getvalue()
 
     def test_basic(self):
--- .\convert\test_po2xliff.py	(original)
+++ .\convert\test_po2xliff.py	(refactored)
@@ -24,8 +24,8 @@
     def test_minimal(self):
         minipo = '''msgid "red"\nmsgstr "rooi"\n'''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        print str(xliff)
+        print("The generated xml:")
+        print(str(xliff))
         assert len(xliff.units) == 1
         assert xliff.translate("red") == "rooi"
         assert xliff.translate("bla") is None
@@ -51,8 +51,8 @@
 msgstr "Toepassings"
 """
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        print str(xliff)
+        print("The generated xml:")
+        print(str(xliff))
         assert xliff.translate("Applications") == "Toepassings"
         assert xliff.translate("bla") is None
         xmltext = str(xliff)
@@ -69,8 +69,8 @@
 msgstr "Eerste deel "
 "en ekstra"'''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        print str(xliff)
+        print("The generated xml:")
+        print(str(xliff))
         assert xliff.translate('First part and extra') == 'Eerste deel en ekstra'
 
     def test_escapednewlines(self):
@@ -79,9 +79,9 @@
 msgstr "Eerste lyn\nTweede lyn"
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert xliff.translate("First line\nSecond line") == "Eerste lyn\nTweede lyn"
         assert xliff.translate("First line\\nSecond line") is None
         assert xmltext.find("line\\nSecond") == -1
@@ -95,9 +95,9 @@
 msgstr "Eerste kolom\tTweede kolom"
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert xliff.translate("First column\tSecond column") == "Eerste kolom\tTweede kolom"
         assert xliff.translate("First column\\tSecond column") is None
         assert xmltext.find("column\\tSecond") == -1
@@ -114,9 +114,9 @@
 msgstr "Gebruik \\\"."
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert xliff.translate('Hello "Everyone"') == 'Good day "All"'
         assert xliff.translate(r'Use \".') == r'Gebruik \".'
         assert xmltext.find(r'\&quot;') > 0 or xmltext.find(r'\"') > 0
@@ -134,9 +134,9 @@
 msgstr "kunye"
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert xliff.translate("one") == "kunye"
         assert len(xliff.units) == 1
         node = xliff.units[0].xmlelement
@@ -155,9 +155,9 @@
 msgstr "kunye"
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert xliff.translate("one") == "kunye"
         assert len(xliff.units) == 1
         node = xliff.units[0].xmlelement
@@ -178,9 +178,9 @@
 msgstr "kunye"
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert xliff.translate("one") == "kunye"
         assert len(xliff.units) == 1
         node = xliff.units[0].xmlelement
@@ -201,9 +201,9 @@
 "Content-Type: text/plain; charset=UTF-8\n"
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert len(xliff.units) == 1
         unit = xliff.units[0]
         assert unit.source == unit.target == "Content-Type: text/plain; charset=UTF-8\n"
@@ -221,9 +221,9 @@
 msgstr "raro"
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert len(xliff.units) == 2
         assert xliff.units[0].isfuzzy()
         assert not xliff.units[1].isfuzzy()
@@ -235,9 +235,9 @@
 msgstr[1] "iinkomo"
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert len(xliff.units) == 1
         assert xliff.translate("cow") == "inkomo"
 
@@ -249,9 +249,9 @@
 msgstr[2] "iiinkomo"
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert len(xliff.units) == 1
         assert xliff.translate("cow") == "inkomo"
 
@@ -267,7 +267,7 @@
         minipo = r'''msgid "%s%s%s%s has made %s his or her buddy%s%s"
 msgstr "%s%s%s%s het %s sy/haar vriend/vriendin gemaak%s%s"'''
         xliff = self.po2xliff(minipo)
-        print xliff.units[0].source
+        print(xliff.units[0].source)
         assert xliff.units[0].source == "%s%s%s%s has made %s his or her buddy%s%s"
 
     def test_approved(self):
@@ -282,9 +282,9 @@
 msgstr ""
 '''
         xliff = self.po2xliff(minipo)
-        print "The generated xml:"
-        xmltext = str(xliff)
-        print xmltext
+        print("The generated xml:")
+        xmltext = str(xliff)
+        print(xmltext)
         assert len(xliff.units) == 3
         assert xliff.units[0].xmlelement.get("approved") != "yes"
         assert not xliff.units[0].isapproved()
--- .\convert\test_pot2po.py	(original)
+++ .\convert\test_pot2po.py	(refactored)
@@ -35,7 +35,7 @@
         """checks that the pofile contains a single non-header unit, and returns it"""
         assert len(pofile.units) == 2
         assert pofile.units[0].isheader()
-        print pofile.units[1]
+        print(pofile.units[1])
         return pofile.units[1]
 
     def test_convertpot_blank(self):
@@ -113,7 +113,7 @@
         posource = '''#: simple.label\n#: simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n'''
         poexpected = '''#: simple.label\n#: simple.accesskey\n#, fuzzy\nmsgid "Its &hard coding a newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n'''
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert str(self.singleunit(newpo)) == poexpected
 
     def test_merging_location_change(self):
@@ -122,7 +122,7 @@
         posource = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
         poexpected = '''#: new_simple.label%snew_simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert str(self.singleunit(newpo)) == poexpected
 
     def test_merging_location_and_whitespace_change(self):
@@ -133,7 +133,7 @@
         posource = '''#: doublespace.label%sdoublespace.accesskey\nmsgid "&We  have  spaces"\nmsgstr "&One  het  spasies"\n''' % po.lsep
         poexpected = '''#: singlespace.label%ssinglespace.accesskey\n#, fuzzy\nmsgid "&We have spaces"\nmsgstr "&One  het  spasies"\n''' % po.lsep
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert str(self.singleunit(newpo)) == poexpected
 
     def test_merging_location_ambiguous_with_disambiguous(self):
@@ -145,7 +145,7 @@
         poexpected1 = '''#: location.c:1\n#, fuzzy\nmsgid ""\n"_: location.c:1\\n"\n"Source"\nmsgstr "Target"\n'''
         poexpected2 = '''#: location.c:10\n#, fuzzy\nmsgid ""\n"_: location.c:10\\n"\n"Source"\nmsgstr "Target"\n'''
         newpo = self.convertpot(potsource, posource)
-        print "Expected:\n", poexpected1, "Actual:\n", newpo.units[1]
+        print("Expected:\n", poexpected1, "Actual:\n", newpo.units[1])
         assert str(newpo.units[1]) == poexpected1
         assert str(newpo.units[2]) == poexpected2
 
@@ -156,7 +156,7 @@
         posource = '''#: someline.c\nmsgid "&About"\nmsgstr "&Info"\n'''
         poexpected = '''#: someline.c\nmsgid "A&bout"\nmsgstr "&Info"\n'''
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert str(self.singleunit(newpo)) == poexpected
 
     @mark.xfail(reason="Not Implemented - review if this is even correct")
@@ -210,10 +210,10 @@
         poexpected = posource
         newpo = self.convertpot(potsource, posource)
         newpounit = self.singleunit(newpo)
-        print "expected"
-        print poexpected
-        print "got:"
-        print str(newpounit)
+        print("expected")
+        print(poexpected)
+        print("got:")
+        print(str(newpounit))
         assert str(newpounit) == poexpected
 
     def test_merging_msgidcomments(self):
@@ -276,7 +276,7 @@
         potsource = '''msgid "One"\nmsgid_plural "Two"\nmsgstr[0] ""\nmsgstr[1] ""\n'''
         posource = '''msgid "One"\nmsgid_plural "Two"\nmsgstr[0] "Een"\nmsgstr[1] "Twee"\nmsgstr[2] "Drie"\n'''
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         newpounit = self.singleunit(newpo)
         assert str(newpounit) == posource
 
@@ -287,7 +287,7 @@
         posource = '# Some comment\n#. Extracted comment\n#: obsoleteme:10\nmsgid "One"\nmsgstr "Een"\n'
         expected = '# Some comment\n#~ msgid "One"\n#~ msgstr "Een"\n'
         newpo = self.convertpot(potsource, posource)
-        print str(newpo)
+        print(str(newpo))
         newpounit = self.singleunit(newpo)
         assert str(newpounit) == expected
 
@@ -297,7 +297,7 @@
         potsource = 'msgid ""\nmsgstr ""\n'
         posource = '#: obsoleteme:10\nmsgid "One"\nmsgstr ""\n'
         newpo = self.convertpot(potsource, posource)
-        print str(newpo)
+        print(str(newpo))
         # We should only have the header
         assert len(newpo.units) == 1
 
@@ -327,7 +327,7 @@
         posource = '''#~ msgid "&About"\n#~ msgstr "&Omtrent"\n'''
         expected = '''#: resurect.c\nmsgid "&About"\nmsgstr "&Omtrent"\n'''
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert len(newpo.units) == 2
         assert newpo.units[0].isheader()
         newpounit = self.singleunit(newpo)
@@ -341,7 +341,7 @@
         expected1 = '''#: resurect1.c\nmsgid "About"\nmsgstr "Omtrent"\n'''
         expected2 = '''#: resurect2.c\n#, fuzzy\nmsgid ""\n"_: resurect2.c\\n"\n"About"\nmsgstr "Omtrent"\n'''
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert len(newpo.units) == 3
         assert newpo.units[0].isheader()
         assert str(newpo.units[1]) == expected1
@@ -393,8 +393,8 @@
 "X-Generator: Translate Toolkit 0.10rc2\n"
 '''
         newpo = self.convertpot(potsource, posource)
-        print 'Output Header:\n%s' % newpo
-        print 'Expected Header:\n%s' % expected
+        print('Output Header:\n%s' % newpo)
+        print('Expected Header:\n%s' % expected)
         assert str(newpo) == expected
 
     def test_merging_comments(self):
@@ -403,7 +403,7 @@
         posource = '''#. Don't do it!\n#: file.py:2\nmsgid "One"\nmsgstr "Een"\n'''
         poexpected = '''#. Don't do it!\n#: file.py:1\nmsgid "One"\nmsgstr "Een"\n'''
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         newpounit = self.singleunit(newpo)
         assert str(newpounit) == poexpected
 
@@ -414,7 +414,7 @@
         poexpected = '''#: file.c:1\n#, c-format\nmsgid "%d pipes"\nmsgstr "%d pype"\n'''
         newpo = self.convertpot(potsource, posource)
         newpounit = self.singleunit(newpo)
-        print newpounit
+        print(newpounit)
         assert str(newpounit) == poexpected
 
         potsource = '''#: file.c:1\n#, c-format\nmsgid "%d computers"\nmsgstr ""\n'''
@@ -462,7 +462,7 @@
 msgstr "teks"
 """
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert poexpected in str(newpo)
 
     def test_msgctxt_multiline(self):
@@ -636,12 +636,12 @@
 msgstr "trans"
 """
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert len(newpo.units) == 2
         assert newpo.units[0].isheader()
         unit = newpo.units[1]
-        assert unit.source == u""
-        assert unit.getid() == u"bla\04"
+        assert unit.source == ""
+        assert unit.getid() == "bla\04"
         assert unit.target == "trans"
         assert not unit.isfuzzy()
 
@@ -674,12 +674,12 @@
 msgstr "trans"
 '''
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert len(newpo.units) == 2
         assert newpo.units[0].isheader()
         unit = newpo.units[1]
-        assert unit.source == u""
-        assert unit.getid() == u"bla\04"
+        assert unit.source == ""
+        assert unit.getid() == "bla\04"
         assert unit.target == "trans"
         assert not unit.isfuzzy()
 
@@ -703,7 +703,7 @@
 #~ msgstr "Ou eenheid3"
 """
         newpo = self.convertpot(potsource, posource)
-        print newpo
+        print(newpo)
         assert len(newpo.units) == 5
         assert newpo.units[1].getcontext() == 'newContext'
         # Search in unit string, because obsolete units can't return a context
@@ -774,8 +774,8 @@
 msgstr ""
 '''
         newpo = self.convertpot(potsource, posource)
-        print 'Output:\n%s' % newpo
-        print 'Expected:\n%s' % expected
+        print('Output:\n%s' % newpo)
+        print('Expected:\n%s' % expected)
         assert str(newpo) == expected
 
 
--- .\convert\test_prop2mozfunny.py	(original)
+++ .\convert\test_prop2mozfunny.py	(refactored)
@@ -13,7 +13,7 @@
         outputfile = wStringIO.StringIO()
         result = prop2mozfunny.po2inc(inputfile, outputfile, templatefile)
         outputinc = outputfile.getvalue()
-        print outputinc
+        print(outputinc)
         assert result
         return outputinc
 
@@ -23,7 +23,7 @@
         inctemplate = '''#define MOZ_LANG_TITLE Deutsch (DE)\n'''
         incexpected = inctemplate
         incfile = self.merge2inc(inctemplate, posource)
-        print incfile
+        print(incfile)
         assert incfile == incexpected
 
     def test_uncomment_contributors(self):
@@ -36,7 +36,7 @@
         inctemplate = '''# #define MOZ_LANGPACK_CONTRIBUTORS <em:contributor>Joe Solon</em:contributor>\n'''
         incexpected = '''#define MOZ_LANGPACK_CONTRIBUTORS <em:contributor>Mr Fury</em:contributor>\n'''
         incfile = self.merge2inc(inctemplate, posource)
-        print incfile
+        print(incfile)
         assert incfile == incexpected
 
     def test_multiline_comment_newlines(self):
@@ -48,5 +48,5 @@
 '''
         incexpected = inctemplate
         incfile = self.merge2inc(inctemplate, None)
-        print incfile
+        print(incfile)
         assert incfile == incexpected
--- .\convert\test_prop2po.py	(original)
+++ .\convert\test_prop2po.py	(refactored)
@@ -37,13 +37,13 @@
         """checks that the pofile contains a single non-header element, and returns it"""
         assert len(pofile.units) == 2
         assert pofile.units[0].isheader()
-        print pofile
+        print(pofile)
         return pofile.units[1]
 
     def countelements(self, pofile):
         """counts the number of non-header entries"""
         assert pofile.units[0].isheader()
-        print pofile
+        print(pofile)
         return len(pofile.units) - 1
 
     def test_simpleentry(self):
@@ -104,9 +104,9 @@
         propsource = 'nb = %s\n' % unistring
         pofile = self.prop2po(propsource)
         pounit = self.singleelement(pofile)
-        print repr(pofile.units[0].target)
-        print repr(pounit.source)
-        assert pounit.source == u'Norsk bokm\u00E5l'
+        print(repr(pofile.units[0].target))
+        print(repr(pounit.source))
+        assert pounit.source == 'Norsk bokm\u00E5l'
 
     def test_multiline_escaping(self):
         """checks that multiline enties can be parsed"""
@@ -114,7 +114,7 @@
 of connections to this server. If so, use the Advanced IMAP Server Settings dialog to \
 reduce the number of cached connections."""
         pofile = self.prop2po(propsource)
-        print repr(pofile.units[1].target)
+        print(repr(pofile.units[1].target))
         assert self.countelements(pofile) == 1
 
     def test_comments(self):
@@ -135,7 +135,7 @@
 prefPanel-smime=
 '''
         pofile = self.prop2po(propsource)
-        print str(pofile)
+        print(str(pofile))
         #header comments:
         assert "#. # Comment\n#. # commenty 2" in str(pofile)
         pounit = self.singleelement(pofile)
@@ -245,12 +245,12 @@
         outputpo = convertor.convertstore(inputprop, personality="gaia")
         pounit = outputpo.units[-1]
         assert pounit.hasplural()
-        assert pounit.getlocations() == [u'message-multiedit-header']
-
-        print outputpo
+        assert pounit.getlocations() == ['message-multiedit-header']
+
+        print(outputpo)
         zero_unit = outputpo.units[-2]
         assert not zero_unit.hasplural()
-        assert zero_unit.source == u"Edit"
+        assert zero_unit.source == "Edit"
 
 class TestProp2POCommand(test_convert.TestConvertCommand, TestProp2PO):
     """Tests running actual prop2po commands on files"""
--- .\convert\test_ts2po.py	(original)
+++ .\convert\test_ts2po.py	(refactored)
@@ -12,8 +12,8 @@
         converter = ts2po.ts2po()
         tsfile = wStringIO.StringIO(tssource)
         outputpo = converter.convertfile(tsfile)
-        print "The generated po:"
-        print str(outputpo)
+        print("The generated po:")
+        print(str(outputpo))
         return outputpo
 
     def test_blank(self):
@@ -50,7 +50,7 @@
         pofile = self.ts2po(tssource)
         assert len(pofile.units) == 2
         assert pofile.units[1].source == "&About"
--- .\convert\test_txt2po.py	(original)
+++ .\convert\test_txt2po.py	(refactored)
@@ -18,7 +18,7 @@
 
     def singleelement(self, storage):
         """checks that the pofile contains a single non-header element, and returns it"""
-        print str(storage)
+        print(str(storage))
         assert len(storage.units) == 1
         return storage.units[0]
 
@@ -73,7 +73,7 @@
 
     def singleelement(self, storage):
         """checks that the pofile contains a single non-header element, and returns it"""
-        print str(storage)
+        print(str(storage))
         assert len(storage.units) == 1
         return storage.units[0]
 
--- .\convert\test_xliff2po.py	(original)
+++ .\convert\test_xliff2po.py	(refactored)
@@ -27,9 +27,9 @@
         inputfile = wStringIO.StringIO(xliffsource)
         convertor = xliff2po.xliff2po()
         outputpo = convertor.convertstore(inputfile)
-        print "The generated po:"
-        print type(outputpo)
-        print str(outputpo)
+        print("The generated po:")
+        print(type(outputpo))
+        print(str(outputpo))
         return outputpo
 
     def test_minimal(self):
@@ -63,7 +63,7 @@
     <target>utshani</target>
   </trans-unit>''') % (headertext, headertext)
 
-        print minixlf
+        print(minixlf)
         pofile = self.xliff2po(minixlf)
         assert pofile.translate("gras") == "utshani"
         assert pofile.translate("bla") is None
@@ -208,7 +208,7 @@
         </trans-unit>
 </group>'''
         pofile = self.xliff2po(minixlf)
-        print str(pofile)
+        print(str(pofile))
         potext = str(pofile)
         assert headerless_len(pofile.units) == 1
         assert potext.index('msgid_plural "cows"')
@@ -312,4 +312,4 @@
         self.run_command("simple.xlf", "simple.po", error="traceback", duplicates="merge")
         pofile = self.target_filetype(self.open_testfile("simple.po"))
         assert len(pofile.units) == 2
-        assert pofile.units[1].target == u"matlhapolosa"
+        assert pofile.units[1].target == "matlhapolosa"
--- .\convert\ts2po.py	(original)
+++ .\convert\ts2po.py	(refactored)
@@ -57,7 +57,7 @@
         tsfile = ts.QtTsParser(inputfile)
         thetargetfile = po.pofile()
 
-        for contextname, messages in tsfile.iteritems():
+        for contextname, messages in tsfile.items():
             messagenum = 0
             for message in messages:
                 messagenum += 1
--- .\convert\web2py2po.py	(original)
+++ .\convert\web2py2po.py	(refactored)
@@ -47,14 +47,14 @@
         targetheader = self.mypofile.header()
         targetheader.addnote("extracted from web2py", "developer")
 
-        for source_str in mydict.keys():
+        for source_str in list(mydict.keys()):
             target_str = mydict[source_str]
             if target_str == source_str:
                 # a convention with new (untranslated) web2py files
-                target_str = u''
-            elif target_str.startswith(u'*** '):
+                target_str = ''
+            elif target_str.startswith('*** '):
                 # an older convention
-                target_str = u''
+                target_str = ''
             pounit = self.convertunit(source_str, target_str)
             self.mypofile.addunit(pounit)
 
--- .\convert\xliff2odf.py	(original)
+++ .\convert\xliff2odf.py	(refactored)
@@ -25,7 +25,7 @@
 for examples and usage instructions.
 """
 
-import cStringIO
+import io
 import zipfile
 
 import lxml.etree as etree
@@ -38,14 +38,14 @@
 
 
 def first_child(unit_node):
-    return unit_node.children.values()[0]
+    return list(unit_node.children.values())[0]
 
 
 def translate_odf(template, input_file):
 
     def load_dom_trees(template):
         odf_data = odf_io.open_odf(template)
-        return dict((filename, etree.parse(cStringIO.StringIO(data))) for filename, data in odf_data.iteritems())
+        return dict((filename, etree.parse(io.StringIO(data))) for filename, data in odf_data.items())
 
     def load_unit_tree(input_file, dom_trees):
         store = factory.getobject(input_file)
@@ -67,7 +67,7 @@
 
     def translate_dom_trees(unit_trees, dom_trees):
         make_parse_state = lambda: extract.ParseState(odf_shared.no_translate_content_elements, odf_shared.inline_elements)
-        for filename, dom_tree in dom_trees.iteritems():
+        for filename, dom_tree in dom_trees.items():
             file_unit_tree = unit_trees[filename]
             generate.apply_translations(dom_tree.getroot(), file_unit_tree, generate.replace_dom_text(make_parse_state))
         return dom_trees
@@ -85,7 +85,7 @@
 def write_odf(xlf_data, template, output_file, dom_trees):
 
     def write_content_to_odf(output_zip, dom_trees):
-        for filename, dom_tree in dom_trees.iteritems():
+        for filename, dom_tree in dom_trees.items():
             output_zip.writestr(filename, etree.tostring(dom_tree, encoding='UTF-8', xml_declaration=True))
 
     # Since the convertoptionsparser will give us an open file, we risk that
@@ -102,14 +102,14 @@
     # file once.
 #    output_zip = odf_io.copy_odf(template_zip, output_zip, dom_trees.keys() + ['META-INF/manifest.xml'])
 #    output_zip = odf_io.add_file(output_zip, template_zip.read('META-INF/manifest.xml'), 'translation.xlf', xlf_data)
-    output_zip = odf_io.copy_odf(template_zip, output_zip, dom_trees.keys())
+    output_zip = odf_io.copy_odf(template_zip, output_zip, list(dom_trees.keys()))
     write_content_to_odf(output_zip, dom_trees)
 
 
 def convertxliff(input_file, output_file, template):
     """reads in stdin using fromfileclass, converts using convertorclass, writes to stdout"""
     xlf_data = input_file.read()
-    dom_trees = translate_odf(template, cStringIO.StringIO(xlf_data))
+    dom_trees = translate_odf(template, io.StringIO(xlf_data))
     write_odf(xlf_data, template, output_file, dom_trees)
     output_file.close()
     return True
--- .\convert\xliff2oo.py	(original)
+++ .\convert\xliff2oo.py	(refactored)
@@ -59,7 +59,7 @@
     def makeindex(self):
         """makes an index of the oo keys that are used in the source file"""
         self.index = {}
-        for ookey, theoo in self.o.ookeys.iteritems():
+        for ookey, theoo in self.o.ookeys.items():
             sourcekey = oo.makekey(ookey, self.long_keys)
             self.index[sourcekey] = theoo
 
@@ -93,7 +93,7 @@
                                key, len(self.index))
                 try:
                     sourceunitlines = str(unit)
-                    if isinstance(sourceunitlines, unicode):
+                    if isinstance(sourceunitlines, str):
                         sourceunitlines = sourceunitlines.encode("utf-8")
                     logger.warning(sourceunitlines)
                 except:
@@ -123,7 +123,7 @@
         # If there is no translation, we don't want to add a line
         if len(unquotedstr.strip()) == 0:
             return
-        if isinstance(unquotedstr, unicode):
+        if isinstance(unquotedstr, str):
             unquotedstr = unquotedstr.encode("UTF-8")
         # finally set the new definition in the oo, but not if its empty
         if len(unquotedstr) > 0:
@@ -162,7 +162,7 @@
         filterresult = self.filterunit(unit)
         if filterresult:
             if filterresult != autocorrect:
-                for filtername, filtermessage in filterresult.iteritems():
+                for filtername, filtermessage in filterresult.items():
                     location = unit.getlocations()[0]
                     if filtername in self.options.error:
                         logger.error("Error at %s::%s: %s",
--- .\filters\autocorrect.py	(original)
+++ .\filters\autocorrect.py	(refactored)
@@ -25,8 +25,8 @@
 from translate.misc.typecheck import accepts, returns, IsOneOf
 
 
-@accepts(unicode, unicode)
-@returns(IsOneOf(unicode, type(None)))
+@accepts(str, str)
+@returns(IsOneOf(str, type(None)))
 def correct(source, target):
     """Runs a set of easy and automatic corrections
 
@@ -38,10 +38,10 @@
     old_target = target
     if target == "":
         return None
-    if u"" in source and "..." in target:
-        target = target.replace("...", u"")
-    elif "..." in source and u"" in target:
-        target = target.replace(u"", "...")
+    if "" in source and "..." in target:
+        target = target.replace("...", "")
+    elif "..." in source and "" in target:
+        target = target.replace("", "...")
     if decoration.spacestart(source) != decoration.spacestart(target) or decoration.spaceend(source) != decoration.spaceend(target):
         target = decoration.spacestart(source) + target.strip() + decoration.spaceend(source)
     punctuation = (".", ":", ". ", ": ", "?")
--- .\filters\checks.py	(original)
+++ .\filters\checks.py	(refactored)
@@ -43,6 +43,7 @@
 from translate.lang import data
 
 from translate.misc import lru
+import collections
 
 logger = logging.getLogger(__name__)
 
@@ -143,15 +144,15 @@
         if not isinstance(messages, list):
             messages = [messages]
 
-        assert isinstance(messages[0], unicode)  # Assumption: all of same type
+        assert isinstance(messages[0], str)  # Assumption: all of same type
 
         self.messages = messages
 
     def __unicode__(self):
-        return unicode(u", ".join(self.messages))
+        return str(", ".join(self.messages))
 
     def __str__(self):
-        return str(u", ".join(self.messages))
+        return str(", ".join(self.messages))
 
 
 class SeriousFilterFailure(FilterFailure):
@@ -313,7 +314,7 @@
         for functionname in dir(UnitChecker):
             function = getattr(self, functionname)
 
-            if callable(function):
+            if isinstance(function, collections.Callable):
                 self.helperfunctions[functionname] = function
 
         self.defaultfilters = self.getfilters(excludefilters, limitfilters)
@@ -345,7 +346,7 @@
                 continue
 
             filterfunction = getattr(self, functionname, None)
-            if not callable(filterfunction):
+            if not isinstance(filterfunction, collections.Callable):
                 continue
 
             filters[functionname] = filterfunction
@@ -427,9 +428,9 @@
         self.results_cache = {}
         failures = {}
         ignores = self.config.lang.ignoretests[:]
-        functionnames = self.defaultfilters.keys()
-        priorityfunctionnames = self.preconditions.keys()
-        otherfunctionnames = filter(lambda functionname: functionname not in self.preconditions, functionnames)
+        functionnames = list(self.defaultfilters.keys())
+        priorityfunctionnames = list(self.preconditions.keys())
+        otherfunctionnames = [functionname for functionname in functionnames if functionname not in self.preconditions]
 
         for functionname in priorityfunctionnames + otherfunctionnames:
             if functionname in ignores:
@@ -446,10 +447,10 @@
 
             try:
                 filterresult = self.run_test(filterfunction, unit)
-            except FilterFailure, e:
+            except FilterFailure as e:
                 filterresult = False
-                filtermessage = unicode(e)
-            except Exception, e:
+                filtermessage = str(e)
+            except Exception as e:
                 if self.errorhandler is None:
                     raise ValueError("error in filter %s: %r, %r, %s" % \
                             (functionname, unit.source, unit.target, e))
@@ -473,7 +474,7 @@
         self.results_cache = {}
 
         if not categorised:
-            for name, info in failures.iteritems():
+            for name, info in failures.items():
                 failures[name] = info['message']
         return failures
 
@@ -506,9 +507,9 @@
 
             for pluralform in unit.target.strings:
                 try:
-                    if not test(self.str1, unicode(pluralform)):
+                    if not test(self.str1, str(pluralform)):
                         filterresult = False
-                except FilterFailure, e:
+                except FilterFailure as e:
                     filterresult = False
                     filtermessages.extend(e.messages)
 
@@ -524,8 +525,8 @@
         """Do some optimisation by caching some data of the unit for the
         benefit of :meth:`~TranslationChecker.run_test`.
         """
-        self.str1 = data.normalized_unicode(unit.source) or u""
-        self.str2 = data.normalized_unicode(unit.target) or u""
+        self.str1 = data.normalized_unicode(unit.source) or ""
+        self.str2 = data.normalized_unicode(unit.target) or ""
         self.hasplural = unit.hasplural()
         self.locations = unit.getlocations()
 
@@ -650,7 +651,7 @@
         # we could also check for things like str1.isnumeric(), but the test
         # above (str1.upper() == str1) makes this unnecessary
         if str1.lower() == str2.lower():
-            raise FilterFailure(u"Consider translating")
+            raise FilterFailure("Consider translating")
 
         return True
 
@@ -662,7 +663,7 @@
         len2 = len(str2.strip())
 
         if len1 > 0 and len(str2) != 0 and len2 == 0:
-            raise FilterFailure(u"Translation is empty")
+            raise FilterFailure("Translation is empty")
         else:
             return True
 
@@ -676,7 +677,7 @@
         len2 = len(str2.strip())
 
         if (len1 > 0) and (0 < len2 < (len1 * 0.1)) or ((len1 > 1) and (len2 == 1)):
-            raise FilterFailure(u"The translation is much shorter than the original")
+            raise FilterFailure("The translation is much shorter than the original")
         else:
             return True
 
@@ -690,7 +691,7 @@
         len2 = len(str2.strip())
 
         if (len1 > 0) and (0 < len1 < (len2 * 0.1)) or ((len1 == 1) and (len2 > 1)):
-            raise FilterFailure(u"The translation is much longer than the original")
+            raise FilterFailure("The translation is much longer than the original")
         else:
             return True
 
@@ -698,11 +699,11 @@
     @critical
     def escapes(self, str1, str2):
         """Checks whether escaping is consistent between the two strings."""
-        if not helpers.countsmatch(str1, str2, (u"\\", u"\\\\")):
-            escapes1 = u", ".join([u"'%s'" % word for word in str1.split() if u"\\" in word])
-            escapes2 = u", ".join([u"'%s'" % word for word in str2.split() if u"\\" in word])
-
-            raise SeriousFilterFailure(u"Escapes in original (%s) don't match "
+        if not helpers.countsmatch(str1, str2, ("\\", "\\\\")):
+            escapes1 = ", ".join(["'%s'" % word for word in str1.split() if "\\" in word])
+            escapes2 = ", ".join(["'%s'" % word for word in str2.split() if "\\" in word])
+
+            raise SeriousFilterFailure("Escapes in original (%s) don't match "
                                        "escapes in translation (%s)" %
                                        (escapes1, escapes2))
         else:
@@ -712,14 +713,14 @@
     @critical
     def newlines(self, str1, str2):
         """Checks whether newlines are consistent between the two strings."""
-        if not helpers.countsmatch(str1, str2, (u"\n", u"\r")):
-            raise FilterFailure(u"Different line endings")
-
-        if str1.endswith(u"\n") and not str2.endswith(u"\n"):
-            raise FilterFailure(u"Newlines different at end")
-
-        if str1.startswith(u"\n") and not str2.startswith(u"\n"):
-            raise FilterFailure(u"Newlines different at beginning")
+        if not helpers.countsmatch(str1, str2, ("\n", "\r")):
+            raise FilterFailure("Different line endings")
+
+        if str1.endswith("\n") and not str2.endswith("\n"):
+            raise FilterFailure("Newlines different at end")
+
+        if str1.startswith("\n") and not str2.startswith("\n"):
+            raise FilterFailure("Newlines different at beginning")
 
         return True
 
@@ -728,7 +729,7 @@
     def tabs(self, str1, str2):
         """Checks whether tabs are consistent between the two strings."""
         if not helpers.countmatch(str1, str2, "\t"):
-            raise SeriousFilterFailure(u"Different tabs")
+            raise SeriousFilterFailure("Different tabs")
         else:
             return True
 
@@ -741,10 +742,10 @@
 
         str2 = self.filterwordswithpunctuation(self.filteraccelerators(self.filtervariables(str2)))
 
-        if helpers.countsmatch(str1, str2, (u"'", u"''", u"\\'")):
-            return True
-        else:
-            raise FilterFailure(u"Different quotation marks")
+        if helpers.countsmatch(str1, str2, ("'", "''", "\\'")):
+            return True
+        else:
+            raise FilterFailure("Different quotation marks")
 
 
     @cosmetic
@@ -759,11 +760,11 @@
         str2 = self.filteraccelerators(self.filtervariables(str2))
         str2 = self.filterxml(str2)
 
--- .\filters\decoration.py	(original)
+++ .\filters\decoration.py	(refactored)
@@ -28,7 +28,7 @@
 
 def spacestart(str1):
     """returns all the whitespace from the start of the string"""
-    newstring = u""
+    newstring = ""
     for c in str1:
         if c.isspace():
             newstring += c
@@ -39,7 +39,7 @@
 
 def spaceend(str1):
     """returns all the whitespace from the end of the string"""
-    newstring = u""
+    newstring = ""
     for n in range(len(str1)):
         c = str1[-1-n]
         if c.isspace():
@@ -51,7 +51,7 @@
 
 def puncstart(str1, punctuation):
     """returns all the punctuation from the start of the string"""
-    newstring = u""
+    newstring = ""
     for c in str1:
         if c in punctuation or c.isspace():
             newstring += c
@@ -64,14 +64,14 @@
     """returns all the punctuation from the end of the string"""
     # An implementation with regular expressions was slightly slower.
 
-    newstring = u""
+    newstring = ""
     for n in range(len(str1)):
         c = str1[-1-n]
         if c in punctuation or c.isspace():
             newstring = c + newstring
         else:
             break
-    return newstring.replace(u"\u00a0", u" ")
+    return newstring.replace("\u00a0", " ")
 
 
 def ispurepunctuation(str1):
@@ -93,8 +93,8 @@
     :rtype: Boolean
     :return: True if the supplied character is an acceptable accelerator
     """
-    assert isinstance(accelerator, unicode)
-    assert isinstance(acceptlist, unicode) or acceptlist is None
+    assert isinstance(accelerator, str)
+    assert isinstance(acceptlist, str) or acceptlist is None
     if len(accelerator) == 0:
         return False
     if acceptlist is not None:
@@ -106,7 +106,7 @@
         # Old code path - ensures that we don't get a large number of
         # regressions
         accelerator = accelerator.replace("_", "")
-        if accelerator in u"-?":
+        if accelerator in "-?":
             return True
         if not accelerator.isalnum():
             return False
@@ -225,10 +225,10 @@
 def getnumbers(str1):
     """returns any numbers that are in the string"""
     # TODO: handle locale-based periods e.g. 2,5 for Afrikaans
-    assert isinstance(str1, unicode)
+    assert isinstance(str1, str)
     numbers = []
     innumber = False
-    degreesign = u'\xb0'
+    degreesign = '\xb0'
     lastnumber = ""
     carryperiod = ""
     for chr1 in str1:
@@ -270,7 +270,7 @@
 def getfunctions(str1):
     """returns the functions() that are in a string, while ignoring the
     trailing punctuation in the given parameter"""
-    if u"()" in str1:
+    if "()" in str1:
         return _function_re.findall(str1)
     else:
         return []
--- .\filters\helpers.py	(original)
+++ .\filters\helpers.py	(refactored)
@@ -21,6 +21,7 @@
 """a set of helper functions for filters..."""
 
 import operator
+from functools import reduce
 
 
 def countmatch(str1, str2, countstr):
@@ -45,7 +46,7 @@
 
 def filtercount(str1, func):
     """returns the number of characters in str1 that pass func"""
-    return len(filter(func, str1))
+    return len(list(filter(func, str1)))
 
 
 def filtertestmethod(testmethod, strfilter):
--- .\filters\pofilter.py	(original)
+++ .\filters\pofilter.py	(refactored)
@@ -97,7 +97,7 @@
     def getfilterdocs(self):
         """Lists the docs for filters available on checker."""
         filterdict = self.checker.getfilters()
-        filterdocs = ["%s\t%s" % (name, filterfunc.__doc__) for (name, filterfunc) in filterdict.iteritems()]
+        filterdocs = ["%s\t%s" % (name, filterfunc.__doc__) for (name, filterfunc) in filterdict.items()]
         filterdocs.sort()
 
         return "\n".join(filterdocs)
@@ -147,7 +147,7 @@
 
             if filter_result:
                 if filter_result != autocorrect:
-                    for filter_name in filter_result.iterkeys():
+                    for filter_name in filter_result.keys():
                         filter_message = filter_result[filter_name]['message']
 
                         if self.options.addnotes:
@@ -203,7 +203,7 @@
         options.outputoptions = self.outputoptions
 
         if options.listfilters:
-            print options.checkfilter.getfilterdocs()
+            print(options.checkfilter.getfilterdocs())
         else:
             self.recursiveprocess(options)
 
--- .\filters\prefilters.py	(original)
+++ .\filters\prefilters.py	(refactored)
@@ -35,7 +35,7 @@
     
       "_: comment\n"
     """
-    assert isinstance(str1, unicode)
+    assert isinstance(str1, str)
     iskdecomment = False
     lines = str1.split("\n")
     removelines = []
@@ -151,7 +151,7 @@
 wordswithpunctuation = ["'n", "'t",  # Afrikaans
                        ]
 # map all the words to their non-punctified equivalent
-wordswithpunctuation = dict([(word, filter(str.isalnum, word)) for word in wordswithpunctuation])
+wordswithpunctuation = dict([(word, list(filter(str.isalnum, word))) for word in wordswithpunctuation])
 
 word_with_apos_re = re.compile("(?u)\w+'\w+")
 
@@ -160,14 +160,14 @@
     """Goes through a list of known words that have punctuation and removes the
     punctuation from them.
     """
-    if u"'" not in str1:
+    if "'" not in str1:
         return str1
     occurrences = []
-    for word, replacement in wordswithpunctuation.iteritems():
+    for word, replacement in wordswithpunctuation.items():
         occurrences.extend([(pos, word, replacement) for pos in quote.find_all(str1, word)])
     for match in word_with_apos_re.finditer(str1):
         word = match.group()
-        replacement = filter(unicode.isalnum, word)
+        replacement = list(filter(str.isalnum, word))
         occurrences.append((match.start(), word, replacement))
     occurrences.sort()
     if occurrences:
--- .\filters\spelling.py	(original)
+++ .\filters\spelling.py	(refactored)
@@ -38,8 +38,8 @@
             try:
                 checkers[lang] = checker.SpellChecker(lang)
                 # some versions only report an error when checking something
-                checkers[lang].check(u'bla')
-            except EnchantError, e:
+                checkers[lang].check('bla')
+            except EnchantError as e:
                 # sometimes this is raised instead of DictNotFoundError
                 logger.error(str(e))
                 checkers[lang] = None
@@ -50,7 +50,7 @@
         spellchecker = _get_checker(lang)
         if not spellchecker:
             return
-        spellchecker.set_text(unicode(text))
+        spellchecker.set_text(str(text))
         for err in spellchecker:
             yield err.word, err.wordpos, err.suggest()
 
@@ -58,7 +58,7 @@
         spellchecker = _get_checker(lang)
         if not spellchecker:
             return
-        spellchecker.set_text(unicode(text))
+        spellchecker.set_text(str(text))
         for err in spellchecker:
             yield err.word
 
--- .\filters\test_autocorrect.py	(original)
+++ .\filters\test_autocorrect.py	(refactored)
@@ -9,49 +9,49 @@
     def correct(self, msgid, msgstr, expected):
         """helper to run correct function from autocorrect module"""
         corrected = autocorrect.correct(msgid, msgstr)
-        print repr(msgid)
-        print repr(msgstr)
-        print msgid.encode('utf-8')
-        print msgstr.encode('utf-8')
-        print (corrected or u"").encode('utf-8')
+        print(repr(msgid))
+        print(repr(msgstr))
+        print(msgid.encode('utf-8'))
+        print(msgstr.encode('utf-8'))
+        print((corrected or "").encode('utf-8'))
         assert corrected == expected
 
     def test_empty_target(self):
         """test that we do nothing with an empty target"""
-        self.correct(u"String...", u"", None)
+        self.correct("String...", "", None)
 
     def test_correct_ellipsis(self):
         """test that we convert single  or ... to match source and target"""
-        self.correct(u"String...", u"Translated", u"Translated...")
-        self.correct(u"String", u"Translated...", u"Translated")
+        self.correct("String...", "Translated", "Translated...")
+        self.correct("String", "Translated...", "Translated")
 
     def test_correct_spacestart_spaceend(self):
         """test that we can correct leading and trailing space errors"""
-        self.correct(u"Simple string", u"Dimpled ring  ", u"Dimpled ring")
-        self.correct(u"Simple string", u"  Dimpled ring", u"Dimpled ring")
-        self.correct(u"  Simple string", u"Dimpled ring", u"  Dimpled ring")
-        self.correct(u"Simple string  ", u"Dimpled ring", u"Dimpled ring  ")
+        self.correct("Simple string", "Dimpled ring  ", "Dimpled ring")
+        self.correct("Simple string", "  Dimpled ring", "Dimpled ring")
+        self.correct("  Simple string", "Dimpled ring", "  Dimpled ring")
+        self.correct("Simple string  ", "Dimpled ring", "Dimpled ring  ")
 
     def test_correct_start_capitals(self):
         """test that we can correct the starting capital"""
-        self.correct(u"Simple string", u"dimpled ring", u"Dimpled ring")
-        self.correct(u"simple string", u"Dimpled ring", u"dimpled ring")
+        self.correct("Simple string", "dimpled ring", "Dimpled ring")
+        self.correct("simple string", "Dimpled ring", "dimpled ring")
 
     def test_correct_end_punc(self):
         """test that we can correct end punctuation"""
-        self.correct(u"Simple string:", u"Dimpled ring", u"Dimpled ring:")
+        self.correct("Simple string:", "Dimpled ring", "Dimpled ring:")
         #self.correct(u"Simple string: ", u"Dimpled ring", u"Dimpled ring: ")
-        self.correct(u"Simple string.", u"Dimpled ring", u"Dimpled ring.")
+        self.correct("Simple string.", "Dimpled ring", "Dimpled ring.")
         #self.correct(u"Simple string. ", u"Dimpled ring", u"Dimpled ring. ")
-        self.correct(u"Simple string?", u"Dimpled ring", u"Dimpled ring?")
+        self.correct("Simple string?", "Dimpled ring", "Dimpled ring?")
 
     def test_correct_combinations(self):
         """test that we can correct combinations of failures"""
-        self.correct(u"Simple string:", u"Dimpled ring ", u"Dimpled ring:")
-        self.correct(u"simple string ", u"Dimpled ring", u"dimpled ring ")
-        self.correct(u"Simple string...", u"dimpled ring..", u"Dimpled ring...")
-        self.correct(u"Simple string:", u"Dimpled ring ", u"Dimpled ring:")
+        self.correct("Simple string:", "Dimpled ring ", "Dimpled ring:")
+        self.correct("simple string ", "Dimpled ring", "dimpled ring ")
+        self.correct("Simple string...", "dimpled ring..", "Dimpled ring...")
+        self.correct("Simple string:", "Dimpled ring ", "Dimpled ring:")
 
     def test_nothing_to_do(self):
         """test that when nothing changes we return None"""
-        self.correct(u"Some text", u"A translation", None)
+        self.correct("Some text", "A translation", None)
--- .\filters\test_checks.py	(original)
+++ .\filters\test_checks.py	(refactored)
@@ -29,7 +29,7 @@
     str1, str2, no_message = strprep(str1, str2)
     try:
         filterresult = filterfunction(str1, str2)
-    except checks.FilterFailure, e:
+    except checks.FilterFailure as e:
         filterresult = False
 
     filterresult = filterresult and check_category(filterfunction)
@@ -42,13 +42,13 @@
     str1, str2, message = strprep(str1, str2, message)
     try:
         filterresult = filterfunction(str1, str2)
-    except checks.SeriousFilterFailure, e:
+    except checks.SeriousFilterFailure as e:
         filterresult = True
-    except checks.FilterFailure, e:
+    except checks.FilterFailure as e:
         if message:
             exc_message = e.messages[0]
             filterresult = exc_message != message
-            print exc_message.encode('utf-8')
+            print(exc_message.encode('utf-8'))
         else:
             filterresult = False
 
@@ -62,11 +62,11 @@
     str1, str2, message = strprep(str1, str2, message)
     try:
         filterresult = filterfunction(str1, str2)
-    except checks.SeriousFilterFailure, e:
+    except checks.SeriousFilterFailure as e:
         if message:
             exc_message = e.messages[0]
             filterresult = exc_message != message
-            print exc_message.encode('utf-8')
+            print(exc_message.encode('utf-8'))
         else:
             filterresult = False
 
@@ -123,7 +123,7 @@
     assert fails(stdchecker.accelerators, "File", "&Fayile")
     assert passes(stdchecker.accelerators, "Mail && News", "Pos en Nuus")
     assert fails(stdchecker.accelerators, "Mail &amp; News", "Pos en Nuus")
-    assert passes(stdchecker.accelerators, "&Allow", u'&\ufeb2\ufee3\ufe8e\ufea3')
+    assert passes(stdchecker.accelerators, "&Allow", '&\ufeb2\ufee3\ufe8e\ufea3')
     assert fails(stdchecker.accelerators, "Open &File", "Vula& Ifayile")
     kdechecker = checks.KdeChecker()
     assert passes(kdechecker.accelerators, "&File", "&Fayile")
@@ -242,10 +242,10 @@
--- .\filters\test_decoration.py	(original)
+++ .\filters\test_decoration.py	(refactored)
@@ -8,24 +8,24 @@
 def test_spacestart():
     """test operation of spacestart()"""
     assert decoration.spacestart("  Start") == "  "
-    assert decoration.spacestart(u"\u0020\u00a0Start") == u"\u0020\u00a0"
+    assert decoration.spacestart("\u0020\u00a0Start") == "\u0020\u00a0"
     # non-breaking space
-    assert decoration.spacestart(u"\u00a0\u202fStart") == u"\u00a0\u202f"
+    assert decoration.spacestart("\u00a0\u202fStart") == "\u00a0\u202f"
     # Some exotic spaces
-    assert decoration.spacestart(u"\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200aStart") == u"\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200a"
+    assert decoration.spacestart("\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200aStart") == "\u2000\u2001\u2002\u2003\u2004\u2005\u2006\u2007\u2008\u2009\u200a"
 
 
 def test_isvalidaccelerator():
     """test the isvalidaccelerator() function"""
     # Mostly this tests the old code path where acceptlist is None
-    assert decoration.isvalidaccelerator(u"") == False
-    assert decoration.isvalidaccelerator(u"a") == True
-    assert decoration.isvalidaccelerator(u"1") == True
--- .\filters\test_pofilter.py	(original)
+++ .\filters\test_pofilter.py	(refactored)
@@ -46,8 +46,8 @@
         """checks that an obviously wrong string fails"""
         self.unit.target = "REST"
         filter_result = self.filter(self.translationstore)
-        print filter_result
-        print filter_result.units
+        print(filter_result)
+        print(filter_result.units)
         assert 'startcaps' in first_translatable(filter_result).geterrors()
 
     def test_variables_across_lines(self):
@@ -163,7 +163,7 @@
     def test_notes(self):
         """tests the optional adding of notes"""
         # let's make sure we trigger the 'long' and/or 'doubleword' test
-        self.unit.target = u"asdf asdf asdf asdf asdf asdf asdf"
+        self.unit.target = "asdf asdf asdf asdf asdf asdf asdf"
         filter_result = self.filter(self.translationstore)
         assert headerless_len(filter_result.units) == 1
         assert first_translatable(filter_result).geterrors()
@@ -182,8 +182,8 @@
     def test_unicode(self):
         """tests that we can handle UTF-8 encoded characters when there is no
         known header specified encoding"""
-        self.unit.source = u'Bzier curve'
-        self.unit.target = u'Bzier-kurwe'
+        self.unit.source = 'Bzier curve'
+        self.unit.target = 'Bzier-kurwe'
         filter_result = self.filter(self.translationstore)
         assert headerless_len(filter_result.units) == 0
 
@@ -217,7 +217,7 @@
         pofile = self.parse_text(posource)
         filter_result = self.filter(pofile)
         if headerless_len(filter_result.units):
-            print first_translatable(filter_result)
+            print(first_translatable(filter_result))
         assert headerless_len(filter_result.units) == 0
 
 
--- .\filters\test_prefilters.py	(original)
+++ .\filters\test_prefilters.py	(refactored)
@@ -7,24 +7,24 @@
 
 
 def test_removekdecomments():
--- .\lang\af.py	(original)
+++ .\lang\af.py	(refactored)
@@ -32,11 +32,11 @@
 
 class af(common.Common):
     """This class represents Afrikaans."""
-    validdoublewords = [u"u"]
+    validdoublewords = ["u"]
 
-    punctuation = u"".join([common.Common.commonpunc, common.Common.quotes,
+    punctuation = "".join([common.Common.commonpunc, common.Common.quotes,
                             common.Common.miscpunc])
-    sentenceend = u".!?"
+    sentenceend = ".!?"
     sentencere = re.compile(r"""
         (?s)        # make . also match newlines
         .*?         # anything, but match non-greedy
@@ -47,14 +47,14 @@
         """ % sentenceend, re.VERBOSE
     )
 
--- .\lang\am.py	(original)
+++ .\lang\am.py	(refactored)
@@ -31,9 +31,9 @@
 class am(common.Common):
     """This class represents Amharic."""
 
--- .\lang\ar.py	(original)
+++ .\lang\ar.py	(refactored)
@@ -30,19 +30,19 @@
 
 def reverse_quotes(text):
     def convertquotation(match):
-        return u"%s" % match.group(1)
-    return re.sub(u'([^]+)', convertquotation, text)
+        return "%s" % match.group(1)
+    return re.sub('([^]+)', convertquotation, text)
 
 
 class ar(common.Common):
     """This class represents Arabic."""
 
--- .\lang\bn.py	(original)
+++ .\lang\bn.py	(refactored)
@@ -31,7 +31,7 @@
 class bn(common.Common):
     """This class represents Bengali."""
 
--- .\lang\code_or.py	(original)
+++ .\lang\code_or.py	(refactored)
@@ -31,7 +31,7 @@
 class code_or(common.Common):
     """This class represents Oriya."""
 
--- .\lang\common.py	(original)
+++ .\lang\common.py	(refactored)
@@ -117,50 +117,50 @@
     """This of languages that has different plural formula in Mozilla than the
     standard one in Gettext."""
 
-    listseperator = u", "
+    listseperator = ", "
     """This string is used to separate lists of textual elements. Most
     languages probably can stick with the default comma, but Arabic and some
     Asian languages might want to override this."""
 
-    specialchars = u""
+    specialchars = ""
     """Characters used by the language that might not be easy to input with
     common keyboard layouts"""
 
-    commonpunc = u".,;:!?-@#$%^*_()[]{}/\\'`\"<>"
+    commonpunc = ".,;:!?-@#$%^*_()[]{}/\\'`\"<>"
     """These punctuation marks are common in English and most languages that
     use latin script."""
 
--- .\lang\data.py	(original)
+++ .\lang\data.py	(refactored)
@@ -22,150 +22,150 @@
 
 
 languages = {
-'af': (u'Afrikaans', 2, '(n != 1)'),
-'ak': (u'Akan', 2, 'n > 1'),
-'am': (u'Amharic', 2, 'n > 1'),
-'an': (u'Aragonese', 2, '(n != 1)'),
-'ar': (u'Arabic', 6,
+'af': ('Afrikaans', 2, '(n != 1)'),
+'ak': ('Akan', 2, 'n > 1'),
+'am': ('Amharic', 2, 'n > 1'),
+'an': ('Aragonese', 2, '(n != 1)'),
+'ar': ('Arabic', 6,
        'n==0 ? 0 : n==1 ? 1 : n==2 ? 2 : n%100>=3 && n%100<=10 ? 3 : n%100>=11 ? 4 : 5'),
-'arn': (u'Mapudungun; Mapuche', 2, 'n > 1'),
-'ast': (u'Asturian; Bable; Leonese; Asturleonese', 2, '(n != 1)'),
-'az': (u'Azerbaijani', 2, '(n != 1)'),
-'be': (u'Belarusian', 3,
+'arn': ('Mapudungun; Mapuche', 2, 'n > 1'),
+'ast': ('Asturian; Bable; Leonese; Asturleonese', 2, '(n != 1)'),
+'az': ('Azerbaijani', 2, '(n != 1)'),
+'be': ('Belarusian', 3,
        'n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2'),
-'bg': (u'Bulgarian', 2, '(n != 1)'),
-'bn': (u'Bengali', 2, '(n != 1)'),
-'bn_IN': (u'Bengali (India)', 2, '(n != 1)'),
-'bo': (u'Tibetan', 1, '0'),
-'br': (u'Breton', 2, 'n > 1'),
-'bs': (u'Bosnian', 3,
+'bg': ('Bulgarian', 2, '(n != 1)'),
+'bn': ('Bengali', 2, '(n != 1)'),
+'bn_IN': ('Bengali (India)', 2, '(n != 1)'),
+'bo': ('Tibetan', 1, '0'),
+'br': ('Breton', 2, 'n > 1'),
+'bs': ('Bosnian', 3,
        'n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2'),
-'ca': (u'Catalan; Valencian', 2, '(n != 1)'),
-'ca@valencia': (u'Catalan; Valencian (Valencia)', 2, '(n != 1)'),
-'cs': (u'Czech', 3, '(n==1) ? 0 : (n>=2 && n<=4) ? 1 : 2'),
-'csb': (u'Kashubian', 3,
+'ca': ('Catalan; Valencian', 2, '(n != 1)'),
+'ca@valencia': ('Catalan; Valencian (Valencia)', 2, '(n != 1)'),
+'cs': ('Czech', 3, '(n==1) ? 0 : (n>=2 && n<=4) ? 1 : 2'),
+'csb': ('Kashubian', 3,
         'n==1 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2'),
-'cy': (u'Welsh', 2, '(n==2) ? 1 : 0'),
-'da': (u'Danish', 2, '(n != 1)'),
-'de': (u'German', 2, '(n != 1)'),
-'dz': (u'Dzongkha', 1, '0'),
-'el': (u'Greek, Modern (1453-)', 2, '(n != 1)'),
-'en': (u'English', 2, '(n != 1)'),
-'en_GB': (u'English (United Kingdom)', 2, '(n != 1)'),
-'en_ZA': (u'English (South Africa)', 2, '(n != 1)'),
-'eo': (u'Esperanto', 2, '(n != 1)'),
-'es': (u'Spanish; Castilian', 2, '(n != 1)'),
-'et': (u'Estonian', 2, '(n != 1)'),
-'eu': (u'Basque', 2, '(n != 1)'),
-'fa': (u'Persian', 1, '0'),
-'ff': (u'Fulah', 2, '(n != 1)'),
-'fi': (u'Finnish', 2, '(n != 1)'),
-'fil': (u'Filipino; Pilipino', 2, '(n > 1)'),
-'fo': (u'Faroese', 2, '(n != 1)'),
-'fr': (u'French', 2, '(n > 1)'),
-'fur': (u'Friulian', 2, '(n != 1)'),
-'fy': (u'Frisian', 2, '(n != 1)'),
-'ga': (u'Irish', 5, 'n==1 ? 0 : n==2 ? 1 : n<7 ? 2 : n<11 ? 3 : 4'),
-'gd': (u'Gaelic; Scottish Gaelic', 4, '(n==1 || n==11) ? 0 : (n==2 || n==12) ? 1 : (n > 2 && n < 20) ? 2 : 3'),
-'gl': (u'Galician', 2, '(n != 1)'),
-'gu': (u'Gujarati', 2, '(n != 1)'),
-'gun': (u'Gun', 2, '(n > 1)'),
-'ha': (u'Hausa', 2, '(n != 1)'),
-'he': (u'Hebrew', 2, '(n != 1)'),
-'hi': (u'Hindi', 2, '(n != 1)'),
-'hy': (u'Armenian', 1, '0'),
-'hr': (u'Croatian', 3, '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)'),
-'ht': (u'Haitian; Haitian Creole', 2, '(n != 1)'),
-'hu': (u'Hungarian', 2, '(n != 1)'),
-'ia': (u"Interlingua (International Auxiliary Language Association)", 2, '(n != 1)'),
-'id': (u'Indonesian', 1, '0'),
-'is': (u'Icelandic', 2, '(n != 1)'),
-'it': (u'Italian', 2, '(n != 1)'),
-'ja': (u'Japanese', 1, '0'),
-'jv': (u'Javanese', 2, '(n != 1)'),
-'ka': (u'Georgian', 1, '0'),
-'kk': (u'Kazakh', 1, '0'),
-'km': (u'Central Khmer', 1, '0'),
-'kn': (u'Kannada', 2, '(n != 1)'),
-'ko': (u'Korean', 1, '0'),
-'ku': (u'Kurdish', 2, '(n != 1)'),
-'kw': (u'Cornish', 4, '(n==1) ? 0 : (n==2) ? 1 : (n == 3) ? 2 : 3'),
-'ky': (u'Kirghiz; Kyrgyz', 1, '0'),
-'lb': (u'Luxembourgish; Letzeburgesch', 2, '(n != 1)'),
-'ln': (u'Lingala', 2, '(n > 1)'),
-'lo': (u'Lao', 1, '0'),
-'lt': (u'Lithuanian', 3, '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && (n%100<10 || n%100>=20) ? 1 : 2)'),
-'lv': (u'Latvian', 3, '(n%10==1 && n%100!=11 ? 0 : n != 0 ? 1 : 2)'),
-'mai': (u'Maithili', 2, '(n != 1)'),
-'mfe': (u'Morisyen', 2, '(n > 1)'),
-'mg': (u'Malagasy', 2, '(n > 1)'),
-'mi': (u'Maori', 2, '(n > 1)'),
-'mk': (u'Macedonian', 2, 'n==1 || n%10==1 ? 0 : 1'),
-'ml': (u'Malayalam', 2, '(n != 1)'),
-'mn': (u'Mongolian', 2, '(n != 1)'),
-'mr': (u'Marathi', 2, '(n != 1)'),
-'ms': (u'Malay', 1, '0'),
-'mt': (u'Maltese', 4,
+'cy': ('Welsh', 2, '(n==2) ? 1 : 0'),
+'da': ('Danish', 2, '(n != 1)'),
+'de': ('German', 2, '(n != 1)'),
+'dz': ('Dzongkha', 1, '0'),
+'el': ('Greek, Modern (1453-)', 2, '(n != 1)'),
+'en': ('English', 2, '(n != 1)'),
+'en_GB': ('English (United Kingdom)', 2, '(n != 1)'),
+'en_ZA': ('English (South Africa)', 2, '(n != 1)'),
+'eo': ('Esperanto', 2, '(n != 1)'),
+'es': ('Spanish; Castilian', 2, '(n != 1)'),
+'et': ('Estonian', 2, '(n != 1)'),
+'eu': ('Basque', 2, '(n != 1)'),
+'fa': ('Persian', 1, '0'),
+'ff': ('Fulah', 2, '(n != 1)'),
+'fi': ('Finnish', 2, '(n != 1)'),
+'fil': ('Filipino; Pilipino', 2, '(n > 1)'),
+'fo': ('Faroese', 2, '(n != 1)'),
+'fr': ('French', 2, '(n > 1)'),
+'fur': ('Friulian', 2, '(n != 1)'),
+'fy': ('Frisian', 2, '(n != 1)'),
+'ga': ('Irish', 5, 'n==1 ? 0 : n==2 ? 1 : n<7 ? 2 : n<11 ? 3 : 4'),
+'gd': ('Gaelic; Scottish Gaelic', 4, '(n==1 || n==11) ? 0 : (n==2 || n==12) ? 1 : (n > 2 && n < 20) ? 2 : 3'),
+'gl': ('Galician', 2, '(n != 1)'),
+'gu': ('Gujarati', 2, '(n != 1)'),
+'gun': ('Gun', 2, '(n > 1)'),
+'ha': ('Hausa', 2, '(n != 1)'),
+'he': ('Hebrew', 2, '(n != 1)'),
+'hi': ('Hindi', 2, '(n != 1)'),
+'hy': ('Armenian', 1, '0'),
+'hr': ('Croatian', 3, '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && n%10<=4 && (n%100<10 || n%100>=20) ? 1 : 2)'),
+'ht': ('Haitian; Haitian Creole', 2, '(n != 1)'),
+'hu': ('Hungarian', 2, '(n != 1)'),
+'ia': ("Interlingua (International Auxiliary Language Association)", 2, '(n != 1)'),
+'id': ('Indonesian', 1, '0'),
+'is': ('Icelandic', 2, '(n != 1)'),
+'it': ('Italian', 2, '(n != 1)'),
+'ja': ('Japanese', 1, '0'),
+'jv': ('Javanese', 2, '(n != 1)'),
+'ka': ('Georgian', 1, '0'),
+'kk': ('Kazakh', 1, '0'),
+'km': ('Central Khmer', 1, '0'),
+'kn': ('Kannada', 2, '(n != 1)'),
+'ko': ('Korean', 1, '0'),
+'ku': ('Kurdish', 2, '(n != 1)'),
+'kw': ('Cornish', 4, '(n==1) ? 0 : (n==2) ? 1 : (n == 3) ? 2 : 3'),
+'ky': ('Kirghiz; Kyrgyz', 1, '0'),
+'lb': ('Luxembourgish; Letzeburgesch', 2, '(n != 1)'),
+'ln': ('Lingala', 2, '(n > 1)'),
+'lo': ('Lao', 1, '0'),
+'lt': ('Lithuanian', 3, '(n%10==1 && n%100!=11 ? 0 : n%10>=2 && (n%100<10 || n%100>=20) ? 1 : 2)'),
+'lv': ('Latvian', 3, '(n%10==1 && n%100!=11 ? 0 : n != 0 ? 1 : 2)'),
+'mai': ('Maithili', 2, '(n != 1)'),
+'mfe': ('Morisyen', 2, '(n > 1)'),
+'mg': ('Malagasy', 2, '(n > 1)'),
+'mi': ('Maori', 2, '(n > 1)'),
+'mk': ('Macedonian', 2, 'n==1 || n%10==1 ? 0 : 1'),
+'ml': ('Malayalam', 2, '(n != 1)'),
+'mn': ('Mongolian', 2, '(n != 1)'),
+'mr': ('Marathi', 2, '(n != 1)'),
+'ms': ('Malay', 1, '0'),
+'mt': ('Maltese', 4,
        '(n==1 ? 0 : n==0 || ( n%100>1 && n%100<11) ? 1 : (n%100>10 && n%100<20 ) ? 2 : 3)'),
-'nah': (u'Nahuatl languages', 2, '(n != 1)'),
-'nap': (u'Neapolitan', 2, '(n != 1)'),
--- .\lang\el.py	(original)
+++ .\lang\el.py	(refactored)
@@ -32,9 +32,9 @@
     """This class represents Greek."""
 
     # Greek uses ; as question mark and the middot instead
-    sentenceend = u".!;"
+    sentenceend = ".!;"
 
-    sentencere = re.compile(ur"""
+    sentencere = re.compile(r"""
         (?s)        # make . also match newlines
         .*?         # anything, but match non-greedy
         [%s]        # the puntuation for sentence ending
@@ -44,19 +44,19 @@
     )
 
     puncdict = {
-        u"?": u";",
-        u";": u"",
+        "?": ";",
+        ";": "",
     }
 
     # Valid latin characters for use as accelerators
-    valid_latin_accel = u"abcdefghijklmnopqrstuvwxyz" + \
-                        u"ABCDEFGHIJKLMNOPQRSTUVWXYZ" + \
-                        u"1234567890"
+    valid_latin_accel = "abcdefghijklmnopqrstuvwxyz" + \
+                        "ABCDEFGHIJKLMNOPQRSTUVWXYZ" + \
+                        "1234567890"
 
     # Valid greek characters for use as accelerators (accented characters
--- .\lang\es.py	(original)
+++ .\lang\es.py	(refactored)
@@ -47,8 +47,8 @@
         if not first:
             return text
         if first[-1] == '?':
--- .\lang\fa.py	(original)
+++ .\lang\fa.py	(refactored)
@@ -34,33 +34,33 @@
         prefix = match.group(1)
         # Let's see that we didn't perhaps match an XML tag property like
         # <a href="something">
-        if prefix == u"=":
+        if prefix == "=":
             return match.group(0)
--- .\lang\factory.py	(original)
+++ .\lang\factory.py	(refactored)
@@ -44,7 +44,7 @@
                             internal_code)
         langclass = getattr(module, internal_code)
         return langclass(code)
-    except ImportError, e:
+    except ImportError as e:
         simplercode = data.simplercode(code)
         if simplercode:
             relatedlanguage = getlanguage(simplercode)
--- .\lang\fi.py	(original)
+++ .\lang\fi.py	(refactored)
@@ -27,6 +27,6 @@
 class fi(common.Common):
     """This class represents Finnish."""
 
-    validaccel = u"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ" + \
-                 u"1234567890" + \
--- .\lang\fr.py	(original)
+++ .\lang\fr.py	(refactored)
@@ -34,37 +34,37 @@
         prefix = match.group(1)
         # Let's see that we didn't perhaps match an XML tag property like
         # <a href="something">
-        if prefix == u"=":
+        if prefix == "=":
             return match.group(0)
--- .\lang\gd.py	(original)
+++ .\lang\gd.py	(refactored)
@@ -29,4 +29,4 @@
 class gd(common.Common):
     """This class represents Gaelic."""
 
--- .\lang\hy.py	(original)
+++ .\lang\hy.py	(refactored)
@@ -31,14 +31,14 @@
 class hy(common.Common):
     """This class represents Armenian."""
 
--- .\lang\identify.py	(original)
+++ .\lang\identify.py	(refactored)
@@ -99,7 +99,7 @@
         if not isinstance(instore, (TranslationStore, list, tuple)):
             return None
 
-        text = u' '.join(unit.source for unit in instore[:50] if unit.istranslatable() and unit.source)
+        text = ' '.join(unit.source for unit in instore[:50] if unit.istranslatable() and unit.source)
         if not text:
             return None
         return self.identify_lang(text)
@@ -116,7 +116,7 @@
         if not isinstance(instore, (TranslationStore, list, tuple)):
             return None
 
-        text = u' '.join(unit.target for unit in instore[:200] if unit.istranslatable() and unit.target)
+        text = ' '.join(unit.target for unit in instore[:200] if unit.istranslatable() and unit.target)
         if not text:
             return None
         return self.identify_lang(text)
@@ -128,4 +128,4 @@
     import locale
     encoding = locale.getpreferredencoding()
     text = file(argv[1]).read().decode(encoding)
-    print "Language detected:", identifier.identify_lang(text)
+    print("Language detected:", identifier.identify_lang(text))
--- .\lang\ja.py	(original)
+++ .\lang\ja.py	(refactored)
@@ -31,9 +31,9 @@
 class ja(common.Common):
     """This class represents Japanese."""
 
-    listseperator = u""
+    listseperator = ""
 
-    sentenceend = u"!?"
+    sentenceend = "!?"
 
     # Compared to common.py, we make the space after the sentence ending
     # optional and don't demand an uppercase letter to follow.
@@ -44,10 +44,10 @@
                             """ % sentenceend, re.VERBOSE)
 
     puncdict = {
-        u". ": u"",
-        u", ": u"",
-        u".\n": u"\n",
-        u",\n": u"\n",
+        ". ": "",
+        ", ": "",
+        ".\n": "\n",
+        ",\n": "\n",
     }
 
     ignoretests = ["startcaps", "simplecaps"]
--- .\lang\km.py	(original)
+++ .\lang\km.py	(refactored)
@@ -31,13 +31,13 @@
 class km(common.Common):
     """This class represents Khmer."""
 
--- .\lang\my.py	(original)
+++ .\lang\my.py	(refactored)
@@ -30,7 +30,7 @@
     """This class represents Burmese."""
 
     puncdict = {
--- .\lang\ne.py	(original)
+++ .\lang\ne.py	(refactored)
@@ -31,7 +31,7 @@
 class ne(common.Common):
     """This class represents Nepali."""
 
--- .\lang\ngram.py	(original)
+++ .\lang\ngram.py	(refactored)
@@ -39,7 +39,7 @@
 class _NGram:
 
     def __init__(self, arg=None):
-        if isinstance(arg, basestring):
+        if isinstance(arg, str):
             self.addText(arg)
             self.normalise()
         elif isinstance(arg, dict):
@@ -57,7 +57,7 @@
         for word in white_space_re.split(text):
             word = '_%s_' % word
             size = len(word)
-            for i in xrange(size - 1):
+            for i in range(size - 1):
                 for s in (1, 2, 3, 4):
                     end = i + s
                     if end >= size:
@@ -116,14 +116,14 @@
                 lines = f.read().decode('utf-8').splitlines()
                 try:
                     for i, line in enumerate(lines):
-                        ngram, _t, _f = line.partition(u'\t')
+                        ngram, _t, _f = line.partition('\t')
                         ngrams[ngram] = i
-                except AttributeError, e:
+                except AttributeError as e:
                     # Python2.4 doesn't have unicode.partition()
                     for i, line in enumerate(lines):
-                        ngram = line.split(u'\t')[0]
+                        ngram = line.split('\t')[0]
                         ngrams[ngram] = i
-            except UnicodeDecodeError, e:
+            except UnicodeDecodeError as e:
                 continue
 
             if ngrams:
@@ -136,7 +136,7 @@
         ngram = _NGram(text)
         r = 'guess'
 
-        min = sys.maxint
+        min = sys.maxsize
 
         for lang in self.ngrams:
             d = self.ngrams[lang].compare(ngram)
@@ -169,7 +169,7 @@
             self.ngrams[lang] = n
 
     def save(self, folder, ext='.lm'):
-        for lang in self.ngrams.keys():
+        for lang in list(self.ngrams.keys()):
             fname = path.join(folder, lang + ext)
             file = open(fname, 'w')
             for v, k in self.ngrams[lang].sorted_by_score():
@@ -186,4 +186,4 @@
     text = sys.stdin.readline()
     from translate.misc.file_discovery import get_abs_data_filename
     l = NGram(get_abs_data_filename('langmodels'))
-    print l.classify(text)
+    print(l.classify(text))
--- .\lang\nqo.py	(original)
+++ .\lang\nqo.py	(refactored)
@@ -30,20 +30,20 @@
 
 def reverse_quotes(text):
     def convertquotation(match):
-        return u"%s" % match.group(1)
-    return re.sub(u'([^]+)', convertquotation, text)
+        return "%s" % match.group(1)
+    return re.sub('([^]+)', convertquotation, text)
 
 
 class nqo(common.Common):
     """This class represents N'Ko."""
 
--- .\lang\pa.py	(original)
+++ .\lang\pa.py	(refactored)
@@ -31,7 +31,7 @@
 class pa(common.Common):
     """This class represents Punjabi."""
 
--- .\lang\poedit.py	(original)
+++ .\lang\poedit.py	(refactored)
@@ -196,7 +196,7 @@
 """ISO369 codes and names as used by Poedit.
 Mostly these are identical to ISO 639, but there are some differences."""
 
-lang_names = dict([(value, key) for (key, value) in lang_codes.items()])
+lang_names = dict([(value, key) for (key, value) in list(lang_codes.items())])
 """Reversed :data:`lang_codes`"""
 
 dialects = {
--- .\lang\son.py	(original)
+++ .\lang\son.py	(refactored)
@@ -29,4 +29,4 @@
 class son(common.Common):
     """This class represents Songhai."""
 
--- .\lang\st.py	(original)
+++ .\lang\st.py	(refactored)
@@ -27,4 +27,4 @@
 class st(common.Common):
     """This class represents Southern Sotho."""
 
-    validdoublewords = [u"o", u"le", u"ba"]
+    validdoublewords = ["o", "le", "ba"]
--- .\lang\sv.py	(original)
+++ .\lang\sv.py	(refactored)
@@ -29,6 +29,6 @@
 class sv(common.Common):
     """This class represents Swedish."""
 
-    validaccel = u"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ" + \
-                 u"1234567890" + \
--- .\lang\team.py	(original)
+++ .\lang\team.py	(refactored)
@@ -53,7 +53,7 @@
     "as": ("assam@mm.assam-glug.org", ),
     "ast": ("@softastur.org", "launchpad.net/~ubuntu-l10n-ast",
             "softast-xeneral@lists.sourceforge.net", "Softastur",),
--- .\lang\test_af.py	(original)
+++ .\lang\test_af.py	(refactored)
@@ -8,13 +8,13 @@
 def test_sentences():
     """Tests basic functionality of sentence segmentation."""
     language = factory.getlanguage('af')
-    sentences = language.sentences(u"Normal case. Nothing interesting.")
-    assert sentences == [u"Normal case.", "Nothing interesting."]
-    sentences = language.sentences(u"Wat? 'n Fout?")
-    assert sentences == [u"Wat?", "'n Fout?"]
-    sentences = language.sentences(u"Dit sal a.g.v. 'n fout gebeur.")
-    assert sentences == [u"Dit sal a.g.v. 'n fout gebeur."]
-    sentences = language.sentences(u"Weet nie hoe om ler '%s' te open nie.\nMiskien is dit 'n tipe beeld wat nog nie ondersteun word nie.\n\nKies liewer 'n ander prent.")
+    sentences = language.sentences("Normal case. Nothing interesting.")
+    assert sentences == ["Normal case.", "Nothing interesting."]
+    sentences = language.sentences("Wat? 'n Fout?")
+    assert sentences == ["Wat?", "'n Fout?"]
+    sentences = language.sentences("Dit sal a.g.v. 'n fout gebeur.")
+    assert sentences == ["Dit sal a.g.v. 'n fout gebeur."]
+    sentences = language.sentences("Weet nie hoe om ler '%s' te open nie.\nMiskien is dit 'n tipe beeld wat nog nie ondersteun word nie.\n\nKies liewer 'n ander prent.")
     assert len(sentences) == 3
 
 
@@ -35,8 +35,8 @@
 def test_transliterate_cyrillic():
 
     def trans(text):
-        print ("Orig: %s" % text).encode("utf-8")
+        print(("Orig: %s" % text).encode("utf-8"))
         trans = af.tranliterate_cyrillic(text)
-        print ("Trans: %s" % trans).encode("utf-8")
+        print(("Trans: %s" % trans).encode("utf-8"))
         return trans
-    assert trans(u"  ") == u"Boris Nikolajewitj Jeltsin"
+    assert trans("  ") == "Boris Nikolajewitj Jeltsin"
--- .\lang\test_am.py	(original)
+++ .\lang\test_am.py	(refactored)
@@ -7,20 +7,20 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('am')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
--- .\lang\test_ar.py	(original)
+++ .\lang\test_ar.py	(refactored)
@@ -7,25 +7,25 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('ar')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg.") == u"abc efg."
--- .\lang\test_common.py	(original)
+++ .\lang\test_common.py	(refactored)
@@ -9,30 +9,30 @@
 def test_characters():
     """Test the basic characters segmentation"""
     language = common.Common
-    assert language.characters(u"") == []
-    assert language.characters(u"Four") == [u"F", u"o", u"u", u"r"]
-    assert language.characters(u"A B") == [u"A", u" ", u"B"]
+    assert language.characters("") == []
+    assert language.characters("Four") == ["F", "o", "u", "r"]
+    assert language.characters("A B") == ["A", " ", "B"]
     # Spaces are compacted, source has 2 returned has only one
-    assert language.characters(u"A  B") == [u"A", u" ", u"B"]
+    assert language.characters("A  B") == ["A", " ", "B"]
 
 
 def test_words():
     """Tests basic functionality of word segmentation."""
     language = common.Common
-    words = language.words(u"")
+    words = language.words("")
     assert words == []
 
-    words = language.words(u"test sentence.")
-    assert words == [u"test", u"sentence"]
+    words = language.words("test sentence.")
+    assert words == ["test", "sentence"]
 
-    words = language.words(u"This is a weird test .")
-    assert words == [u"This", u"is", u"a", u"weird", u"test"]
+    words = language.words("This is a weird test .")
+    assert words == ["This", "is", "a", "weird", "test"]
 
-    words = language.words(u"Don't send e-mail!")
-    assert words == [u"Don't", u"send", u"e-mail"]
+    words = language.words("Don't send e-mail!")
+    assert words == ["Don't", "send", "e-mail"]
 
-    words = language.words(u"Dont send e-mail!")
-    assert words == [u"Dont", u"send", u"e-mail"]
+    words = language.words("Dont send e-mail!")
+    assert words == ["Dont", "send", "e-mail"]
 
 
 @mark.xfail("sys.version_info >= (2, 6)",
@@ -42,46 +42,46 @@
 def test_word_khmer():
     language = common.Common
     # Let's test Khmer with zero width space (\u200b)
--- .\lang\test_data.py	(original)
+++ .\lang\test_data.py	(refactored)
@@ -37,7 +37,7 @@
 
 def test_language_names():
     _ = data.tr_lang('en_US')
--- .\lang\test_el.py	(original)
+++ .\lang\test_el.py	(refactored)
@@ -7,22 +7,22 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('el')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg. hij.") == u"abc efg. hij."
-    assert language.punctranslate(u"abc efg;") == u"abc efg"
-    assert language.punctranslate(u"abc efg? hij!") == u"abc efg; hij!"
+    assert language.punctranslate("") == ""
+    assert language.punctranslate("abc efg") == "abc efg"
+    assert language.punctranslate("abc efg. hij.") == "abc efg. hij."
+    assert language.punctranslate("abc efg;") == "abc efg"
+    assert language.punctranslate("abc efg? hij!") == "abc efg; hij!"
 
 
 def test_sentences():
     """Tests basic functionality of sentence segmentation."""
     language = factory.getlanguage('el')
-    sentences = language.sentences(u"")
+    sentences = language.sentences("")
     assert sentences == []
 
--- .\lang\test_es.py	(original)
+++ .\lang\test_es.py	(refactored)
@@ -7,24 +7,24 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('es')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg.") == u"abc efg."
--- .\lang\test_fa.py	(original)
+++ .\lang\test_fa.py	(refactored)
@@ -7,30 +7,30 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('fa')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg.") == u"abc efg."
--- .\lang\test_fr.py	(original)
+++ .\lang\test_fr.py	(refactored)
@@ -7,41 +7,41 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('fr')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg.") == u"abc efg."
-    assert language.punctranslate(u"abc efg!") == u"abc efg\u00a0!"
-    assert language.punctranslate(u"abc efg? hij!") == u"abc efg\u00a0? hij\u00a0!"
-    assert language.punctranslate(u"Delete file: %s?") == u"Delete file\u00a0: %s\u00a0?"
--- .\lang\test_hy.py	(original)
+++ .\lang\test_hy.py	(refactored)
@@ -7,22 +7,22 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('hy')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
--- .\lang\test_identify.py	(original)
+++ .\lang\test_identify.py	(refactored)
@@ -147,13 +147,13 @@
 und in den Karnevalvereinen -
 """
 
-TEXT_LIST = [u"""
+TEXT_LIST = ["""
--- .\lang\test_km.py	(original)
+++ .\lang\test_km.py	(refactored)
@@ -7,23 +7,23 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('km')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
--- .\lang\test_ko.py	(original)
+++ .\lang\test_ko.py	(refactored)
@@ -8,21 +8,21 @@
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('ko')
     # Nothing should be translated
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg.") == u"abc efg."
-    assert language.punctranslate(u"abc efg. hij.") == u"abc efg. hij."
-    assert language.punctranslate(u"abc efg!") == u"abc efg!"
-    assert language.punctranslate(u"abc efg? hij!") == u"abc efg? hij!"
-    assert language.punctranslate(u"Delete file: %s?") == u"Delete file: %s?"
+    assert language.punctranslate("") == ""
+    assert language.punctranslate("abc efg") == "abc efg"
+    assert language.punctranslate("abc efg.") == "abc efg."
+    assert language.punctranslate("abc efg. hij.") == "abc efg. hij."
+    assert language.punctranslate("abc efg!") == "abc efg!"
+    assert language.punctranslate("abc efg? hij!") == "abc efg? hij!"
+    assert language.punctranslate("Delete file: %s?") == "Delete file: %s?"
 
 
 def test_sentences():
     """Tests basic functionality of sentence segmentation."""
     language = factory.getlanguage('ko')
-    sentences = language.sentences(u"")
+    sentences = language.sentences("")
     assert sentences == []
 
--- .\lang\test_ne.py	(original)
+++ .\lang\test_ne.py	(refactored)
@@ -7,23 +7,23 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('ne')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
--- .\lang\test_nqo.py	(original)
+++ .\lang\test_nqo.py	(refactored)
@@ -7,26 +7,26 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('nqo')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg.") == u"abc efg."
--- .\lang\test_or.py	(original)
+++ .\lang\test_or.py	(refactored)
@@ -7,11 +7,11 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('or')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"Document loaded") == u"Document loaded"
--- .\lang\test_team.py	(original)
+++ .\lang\test_team.py	(refactored)
@@ -8,18 +8,18 @@
     """test the regex, team snippet and language name snippets at a high
     level"""
     # standard regex guess
-    assert guess_language(u"ab@li.org") == "ab"
+    assert guess_language("ab@li.org") == "ab"
     # We never suggest 'en', it's always a mistake
-    assert guess_language(u"en@li.org") == None
+    assert guess_language("en@li.org") == None
     # We can't have a single char language code
-    assert guess_language(u"C@li.org") == None
+    assert guess_language("C@li.org") == None
     # Testing regex postfilter
-    assert guess_language(u"LL@li.org") == None
+    assert guess_language("LL@li.org") == None
 
     # snippet guess based on contact info
-    assert guess_language(u"assam@mm.assam-glug.org") == "as"
+    assert guess_language("assam@mm.assam-glug.org") == "as"
     # snippet guess based on a language name
-    assert guess_language(u"Hawaiian") == "haw"
+    assert guess_language("Hawaiian") == "haw"
 
     # We found nothing
-    assert guess_language(u"Bork bork") is None
+    assert guess_language("Bork bork") is None
--- .\lang\test_th.py	(original)
+++ .\lang\test_th.py	(refactored)
@@ -7,15 +7,15 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('th')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg.") == u"abc efg"
-    assert language.punctranslate(u"abc efg. hij") == u"abc efg hij"
+    assert language.punctranslate("") == ""
+    assert language.punctranslate("abc efg") == "abc efg"
+    assert language.punctranslate("abc efg.") == "abc efg"
+    assert language.punctranslate("abc efg. hij") == "abc efg hij"
 
 
 def test_sentences():
     """Tests basic functionality of sentence segmentation."""
     # We can forget to do this well without extra help.
     language = factory.getlanguage('th')
-    sentences = language.sentences(u"")
+    sentences = language.sentences("")
     assert sentences == []
--- .\lang\test_tr.py	(original)
+++ .\lang\test_tr.py	(refactored)
@@ -6,7 +6,7 @@
 def test_sentences():
     """Tests basic functionality of sentence segmentation."""
     language = factory.getlanguage('tr')
-    sentences = language.sentences(u"Normal case. Nothing interesting.")
-    assert sentences == [u"Normal case.", u"Nothing interesting."]
--- .\lang\test_uk.py	(original)
+++ .\lang\test_uk.py	(refactored)
@@ -7,7 +7,7 @@
 def test_sentences():
     """Tests basic functionality of sentence segmentation."""
     language = factory.getlanguage('uk')
-    sentences = language.sentences(u"")
+    sentences = language.sentences("")
     assert sentences == []
-    sentences = language.sentences(u". ")
-    assert sentences == [u". "]
+    sentences = language.sentences(". ")
+    assert sentences == [". "]
--- .\lang\test_vi.py	(original)
+++ .\lang\test_vi.py	(refactored)
@@ -7,26 +7,26 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('vi')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg.") == u"abc efg."
-    assert language.punctranslate(u"abc efg!") == u"abc efg !"
-    assert language.punctranslate(u"abc efg? hij!") == u"abc efg? hij !"
-    assert language.punctranslate(u"Delete file: %s?") == u"Delete file : %s?"
--- .\lang\test_zh.py	(original)
+++ .\lang\test_zh.py	(refactored)
@@ -7,20 +7,20 @@
 def test_punctranslate():
     """Tests that we can translate punctuation."""
     language = factory.getlanguage('zh')
-    assert language.punctranslate(u"") == u""
-    assert language.punctranslate(u"abc efg") == u"abc efg"
-    assert language.punctranslate(u"abc efg.") == u"abc efg"
-    assert language.punctranslate(u"(abc efg).") == u"(abc efg)"
-    assert language.punctranslate(u"(abc efg). hijk") == u"(abc efg)hijk"
-    assert language.punctranslate(u".") == u""
-    assert language.punctranslate(u"abc efg...") == u"abc efg..."
+    assert language.punctranslate("") == ""
+    assert language.punctranslate("abc efg") == "abc efg"
+    assert language.punctranslate("abc efg.") == "abc efg"
+    assert language.punctranslate("(abc efg).") == "(abc efg)"
+    assert language.punctranslate("(abc efg). hijk") == "(abc efg)hijk"
+    assert language.punctranslate(".") == ""
+    assert language.punctranslate("abc efg...") == "abc efg..."
 
 
 def test_sentences():
     """Tests basic functionality of sentence segmentation."""
     language = factory.getlanguage('zh')
-    sentences = language.sentences(u"")
+    sentences = language.sentences("")
     assert sentences == []
 
-    sentences = language.sentences(u"@QFo]\n")
-    assert sentences == [u"@Q", u"Fo]"]
+    sentences = language.sentences("@QFo]\n")
+    assert sentences == ["@Q", "Fo]"]
--- .\lang\th.py	(original)
+++ .\lang\th.py	(refactored)
@@ -30,7 +30,7 @@
     """This class represents Thai."""
 
     puncdict = {
-        u". ": u" ",
+        ". ": " ",
         #u"; ": u" ", # Test interaction with XML entities
     }
 
--- .\lang\tr.py	(original)
+++ .\lang\tr.py	(refactored)
@@ -26,4 +26,4 @@
 class tr(common.Common):
     """This class represents Turkish."""
 
--- .\lang\ug.py	(original)
+++ .\lang\ug.py	(refactored)
@@ -29,12 +29,12 @@
 class ug(common.Common):
     """This class represents Uyghur."""
 
--- .\lang\ur.py	(original)
+++ .\lang\ur.py	(refactored)
@@ -29,13 +29,13 @@
 class ur(common.Common):
     """This class represents Urdu."""
 
--- .\lang\vi.py	(original)
+++ .\lang\vi.py	(refactored)
@@ -33,8 +33,8 @@
     # Vietnamese uses similar rules for spacing two-part punctuation marks as
     # French, but does not use a space before '?'.
     puncdict = {}
-    for c in u":;!#":
-        puncdict[c] = u" %s" % c
+    for c in ":;!#":
+        puncdict[c] = " %s" % c
 
     def punctranslate(cls, text):
         """Implement some extra features for quotation marks.
--- .\lang\wo.py	(original)
+++ .\lang\wo.py	(refactored)
@@ -28,50 +28,50 @@
     """This class represents Wolof."""
 
     validdoublewords = [
-        u"am",
-        u"baana",
--- .\lang\zh.py	(original)
+++ .\lang\zh.py	(refactored)
@@ -31,9 +31,9 @@
 class zh(common.Common):
     """This class represents Chinese."""
 
-    listseperator = u""
+    listseperator = ""
 
-    sentenceend = u""
+    sentenceend = ""
 
     # Compared to common.py, we make the space after the sentence ending
     # optional and don't demand an uppercase letter to follow.
@@ -50,17 +50,17 @@
     # fullwidth comma (""). If comma is used as seperation of list items
     # like "apple, orange, grape, .....", "" is used.
     puncdict = {
-        u". ": u"",
-        u"; ": u"",
-        u": ": u"",
-        u"! ": u"",
-        u"? ": u"",
-        u".\n": u"\n",
-        u";\n": u"\n",
-        u":\n": u"\n",
-        u"!\n": u"\n",
-        u"?\n": u"",
-        u"% ": u"%",
+        ". ": "",
+        "; ": "",
+        ": ": "",
+        "! ": "",
+        "? ": "",
+        ".\n": "\n",
+        ";\n": "\n",
+        ":\n": "\n",
+        "!\n": "\n",
+        "?\n": "",
+        "% ": "%",
     }
 
     def length_difference(cls, length):
--- .\lang\zh_cn.py	(original)
+++ .\lang\zh_cn.py	(refactored)
@@ -26,5 +26,5 @@
 from translate.lang.zh import zh
 
 class zh_cn(zh):
--- .\lang\zh_hk.py	(original)
+++ .\lang\zh_hk.py	(refactored)
@@ -26,5 +26,5 @@
 from translate.lang.zh import zh
 
 class zh_hk(zh):
--- .\lang\zh_tw.py	(original)
+++ .\lang\zh_tw.py	(refactored)
@@ -26,5 +26,5 @@
 from translate.lang.zh import zh
 
 class zh_tw(zh):
--- .\misc\autoencode.py	(original)
+++ .\misc\autoencode.py	(refactored)
@@ -22,30 +22,30 @@
 and uses this when converting to a string."""
 
 
-class autoencode(unicode):
+class autoencode(str):
 
-    def __new__(newtype, string=u"", encoding=None, errors=None):
-        if isinstance(string, unicode):
+    def __new__(newtype, string="", encoding=None, errors=None):
+        if isinstance(string, str):
             if errors is None:
-                newstring = unicode.__new__(newtype, string)
+                newstring = str.__new__(newtype, string)
             else:
-                newstring = unicode.__new__(newtype, string, errors=errors)
+                newstring = str.__new__(newtype, string, errors=errors)
             if encoding is None and isinstance(string, autoencode):
                 newstring.encoding = string.encoding
             else:
                 newstring.encoding = encoding
         else:
             if errors is None and encoding is None:
-                newstring = unicode.__new__(newtype, string)
+                newstring = str.__new__(newtype, string)
             elif errors is None:
                 try:
-                    newstring = unicode.__new__(newtype, string, encoding)
-                except LookupError, e:
+                    newstring = str.__new__(newtype, string, encoding)
+                except LookupError as e:
                     raise ValueError(str(e))
             elif encoding is None:
-                newstring = unicode.__new__(newtype, string, errors)
+                newstring = str.__new__(newtype, string, errors)
             else:
-                newstring = unicode.__new__(newtype, string, encoding, errors)
+                newstring = str.__new__(newtype, string, encoding, errors)
             newstring.encoding = encoding
         return newstring
 
--- .\misc\contextlib.py	(original)
+++ .\misc\contextlib.py	(refactored)
@@ -46,14 +46,14 @@
 
     def __enter__(self):
         try:
-            return self.gen.next()
+            return next(self.gen)
         except StopIteration:
             raise RuntimeError("generator didn't yield")
 
     def __exit__(self, type, value, tb):
         if type is None:
             try:
-                self.gen.next()
+                next(self.gen)
             except StopIteration:
                 return
             else:
@@ -65,12 +65,12 @@
                 value = type()
             try:
                 try:
-                    self.gen.next()
+                    next(self.gen)
                 except StopIteration:
                     import traceback
                     traceback.print_exception(type, value, tb)
                     raise value
-            except StopIteration, exc:
+            except StopIteration as exc:
                 # Suppress the exception *unless* it's the same exception that
                 # was passed to throw().  This prevents a StopIteration
                 # raised inside the "with" statement from being suppressed
@@ -168,7 +168,7 @@
         # Don't rely on sys.exc_info() still containing
         # the right information. Another exception may
         # have been raised and caught by an exit method
-        raise exc[0], exc[1], exc[2]
+        raise exc[0](exc[1]).with_traceback(exc[2])
 
 
 class closing(object):
--- .\misc\dictutils.py	(original)
+++ .\misc\dictutils.py	(refactored)
@@ -37,19 +37,19 @@
             self.update(fromdict)
 
     def __getitem__(self, key):
-        if type(key) != str and type(key) != unicode:
+        if type(key) != str and type(key) != str:
             raise TypeError("cidict can only have str or unicode as key (got %r)" %
                             type(key))
-        for akey in self.iterkeys():
+        for akey in self.keys():
             if akey.lower() == key.lower():
                 return dict.__getitem__(self, akey)
         raise IndexError
 
     def __setitem__(self, key, value):
-        if type(key) != str and type(key) != unicode:
+        if type(key) != str and type(key) != str:
             raise TypeError("cidict can only have str or unicode as key (got %r)" %
                             type(key))
-        for akey in self.iterkeys():
+        for akey in self.keys():
             if akey.lower() == key.lower():
                 return dict.__setitem__(self, akey, value)
         return dict.__setitem__(self, key, value)
@@ -57,23 +57,23 @@
     def update(self, updatedict):
         """D.update(E) -> None.
            Update D from E: for k in E.keys(): D[k] = E[k]"""
-        for key, value in updatedict.iteritems():
+        for key, value in updatedict.items():
             self[key] = value
 
     def __delitem__(self, key):
-        if type(key) != str and type(key) != unicode:
+        if type(key) != str and type(key) != str:
             raise TypeError("cidict can only have str or unicode as key (got %r)" %
                             type(key))
-        for akey in self.iterkeys():
+        for akey in self.keys():
             if akey.lower() == key.lower():
                 return dict.__delitem__(self, akey)
         raise IndexError
 
     def __contains__(self, key):
-        if type(key) != str and type(key) != unicode:
+        if type(key) != str and type(key) != str:
             raise TypeError("cidict can only have str or unicode as key (got %r)" %
                             type(key))
-        for akey in self.iterkeys():
+        for akey in self.keys():
             if akey.lower() == key.lower():
                 return 1
         return 0
@@ -101,9 +101,9 @@
                             len(args))
         else:
             initarg = args[0]
-            apply(super(ordereddict, self).__init__, args)
+            super(ordereddict, self).__init__(*args)
             if hasattr(initarg, "keys"):
-                self.order = initarg.keys()
+                self.order = list(initarg.keys())
             else:
                 # danger: could have duplicate keys...
                 self.order = []
@@ -123,7 +123,7 @@
     def update(self, updatedict):
         """D.update(E) -> None.
         Update D from E: for k in E.keys(): D[k] = E[k]"""
-        for key, value in updatedict.iteritems():
+        for key, value in updatedict.items():
             self[key] = value
 
     def __delitem__(self, key):
--- .\misc\diff_match_patch.py	(original)
+++ .\misc\diff_match_patch.py	(refactored)
@@ -262,11 +262,11 @@
         lineStart = lineEnd + 1
 
         if line in lineHash:
-          chars.append(unichr(lineHash[line]))
+          chars.append(chr(lineHash[line]))
         else:
           lineArray.append(line)
           lineHash[line] = len(lineArray) - 1
-          chars.append(unichr(len(lineArray) - 1))
+          chars.append(chr(len(lineArray) - 1))
       return "".join(chars)
 
     chars1 = diff_linesToCharsMunge(text1)
@@ -280,7 +280,7 @@
     :param diffs: Array of diff tuples.
     :param lineArray: Array of unique strings.
     """
-    for x in xrange(len(diffs)):
+    for x in range(len(diffs)):
       text = []
       for char in diffs[x][1]:
         text.append(lineArray[ord(char)])
@@ -317,14 +317,14 @@
     # If the total number of characters is odd, then the front path will
     # collide with the reverse path.
     front = (text1_length + text2_length) % 2
-    for d in xrange(max_d):
+    for d in range(max_d):
       # Bail out if timeout reached.
       if self.Diff_Timeout > 0 and time.time() > s_end:
         return None
 
       # Walk the front path one step.
       v_map1.append({})
-      for k in xrange(-d, d + 1, 2):
+      for k in range(-d, d + 1, 2):
         if k == -d or k != d and v1[k - 1] < v1[k + 1]:
           x = v1[k + 1]
         else:
@@ -363,7 +363,7 @@
       if doubleEnd:
         # Walk the reverse path one step.
         v_map2.append({})
-        for k in xrange(-d, d + 1, 2):
+        for k in range(-d, d + 1, 2):
           if k == -d or k != d and v2[k - 1] < v2[k + 1]:
             x = v2[k + 1]
           else:
@@ -411,7 +411,7 @@
     x = len(text1)
     y = len(text2)
     last_op = None
-    for d in xrange(len(v_map) - 2, -1, -1):
+    for d in range(len(v_map) - 2, -1, -1):
       while True:
         if (x - 1 << 32) + y in v_map[d]:
           x -= 1
@@ -454,7 +454,7 @@
     x = len(text1)
     y = len(text2)
     last_op = None
-    for d in xrange(len(v_map) - 2, -1, -1):
+    for d in range(len(v_map) - 2, -1, -1):
       while True:
         if (x - 1 << 32) + y in v_map[d]:
           x -= 1
@@ -950,7 +950,7 @@
     chars2 = 0
     last_chars1 = 0
     last_chars2 = 0
-    for x in xrange(len(diffs)):
+    for x in range(len(diffs)):
       (op, text) = diffs[x]
       if op != self.DIFF_INSERT:  # Equality or deletion.
         chars1 += len(text)
@@ -1051,13 +1051,13 @@
 
     :returns: Delta text.
     """
-    import urllib
+    import urllib.request, urllib.parse, urllib.error
     text = []
     for (op, data) in diffs:
       if op == self.DIFF_INSERT:
         # High ascii will raise UnicodeDecodeError.  Use Unicode instead.
         data = data.encode("utf-8")
-        text.append("+" + urllib.quote(data, "!~*'();/?:@&=+$,# "))
+        text.append("+" + urllib.parse.quote(data, "!~*'();/?:@&=+$,# "))
       elif op == self.DIFF_DELETE:
         text.append("-%d" % len(data))
       elif op == self.DIFF_EQUAL:
@@ -1075,8 +1075,8 @@
 
     :raise ValueError: If invalid input.
     """
-    import urllib
-    if type(delta) == unicode:
+    import urllib.request, urllib.parse, urllib.error
+    if type(delta) == str:
       # Deltas should be composed of a subset of ascii chars, Unicode not
       # required.  If this encode raises UnicodeEncodeError, delta is invalid.
       delta = delta.encode("ascii")
@@ -1091,7 +1091,7 @@
       # operation of this token (delete, insert, equality).
       param = token[1:]
       if token[0] == "+":
-        param = urllib.unquote(param).decode("utf-8")
+        param = urllib.parse.unquote(param).decode("utf-8")
         diffs.append((self.DIFF_INSERT, param))
       elif token[0] == "-" or token[0] == "=":
         try:
@@ -1197,7 +1197,7 @@
     bin_max = len(pattern) + len(text)
     # Empty initialization added to appease pychecker.
     last_rd = None
-    for d in xrange(len(pattern)):
+    for d in range(len(pattern)):
       # Scan for the best match each iteration allows for one more error.
       # Run a binary search to determine how far from 'loc' we can stray at
       # this error level.
@@ -1215,9 +1215,9 @@
       start = max(1, loc - bin_mid + 1)
       finish = min(loc + bin_mid, len(text)) + len(pattern)
 
-      rd = range(finish + 1)
+      rd = list(range(finish + 1))
       rd.append((1 << d) - 1)
-      for j in xrange(finish, start - 1, -1):
+      for j in range(finish, start - 1, -1):
         if len(text) <= j - 1:
           # Out of range.
           charMatch = 0
@@ -1258,7 +1258,7 @@
     s = {}
     for char in pattern:
       s[char] = 0
-    for i in xrange(len(pattern)):
+    for i in range(len(pattern)):
       s[pattern[i]] |= 1 << (len(pattern) - i - 1)
     return s
 
@@ -1330,7 +1330,7 @@
     text1 = None
     diffs = None
     # Note that texts may arrive as 'str' or 'unicode'.
-    if isinstance(a, basestring) and isinstance(b, basestring) and c is None:
+    if isinstance(a, str) and isinstance(b, str) and c is None:
       # Method 1: text1, text2
       # Compute diffs from text1 and text2.
       text1 = a
@@ -1343,11 +1343,11 @@
       # Compute text1 from diffs.
       diffs = a
       text1 = self.diff_text1(diffs)
-    elif isinstance(a, basestring) and isinstance(b, list) and c is None:
+    elif isinstance(a, str) and isinstance(b, list) and c is None:
       # Method 3: text1, diffs
       text1 = a
       diffs = b
-    elif (isinstance(a, basestring) and isinstance(b, basestring) and
+    elif (isinstance(a, str) and isinstance(b, str) and
           isinstance(c, list)):
       # Method 4: text1, text2, diffs
       # text2 is not used.
@@ -1364,7 +1364,7 @@
     char_count2 = 0  # Number of characters into the text2 string.
     prepatch_text = text1  # Recreate the patches to determine context info.
     postpatch_text = text1
-    for x in xrange(len(diffs)):
+    for x in range(len(diffs)):
       (diff_type, diff_text) = diffs[x]
       if len(patch.diffs) == 0 and diff_type != self.DIFF_EQUAL:
         # A new patch starts here.
@@ -1532,7 +1532,7 @@
     """
     paddingLength = self.Patch_Margin
     nullPadding = ""
-    for x in xrange(1, paddingLength + 1):
+    for x in range(1, paddingLength + 1):
       nullPadding += chr(x)
 
     # Bump all the patches forward.
@@ -1586,7 +1586,7 @@
     """
     if self.Match_MaxBits == 0:
       return
-    for x in xrange(len(patches)):
+    for x in range(len(patches)):
       if patches[x].length1 > self.Match_MaxBits:
         bigpatch = patches[x]
         # Remove the big old patch.
@@ -1683,7 +1683,7 @@
 
     :raises ValueError: If invalid input.
     """
-    if type(textline) == unicode:
+    if type(textline) == str:
       # Patches should be composed of a subset of ascii chars, Unicode not
       # required.  If this encode raises UnicodeEncodeError, patch is invalid.
       textline = textline.encode("ascii")
@@ -1719,13 +1719,13 @@
 
       del text[0]
 
-      import urllib
+      import urllib.request, urllib.parse, urllib.error
       while len(text) != 0:
         if text[0]:
           sign = text[0][0]
         else:
           sign = ''
-        line = urllib.unquote(text[0][1:])
+        line = urllib.parse.unquote(text[0][1:])
         line = line.decode("utf-8")
         if sign == '+':
           # Insertion.
@@ -1769,7 +1769,7 @@
 
     :returns: The GNU diff string.
     """
-    import urllib
+    import urllib.request, urllib.parse, urllib.error
     if self.length1 == 0:
       coords1 = str(self.start1) + ",0"
     elif self.length1 == 1:
@@ -1793,5 +1793,5 @@
         text.append(" ")
       # High ascii will raise UnicodeDecodeError.  Use Unicode instead.
       data = data.encode("utf-8")
-      text.append(urllib.quote(data, "!~*'();/?:@&=+$,# ") + "\n")
+      text.append(urllib.parse.quote(data, "!~*'();/?:@&=+$,# ") + "\n")
     return "".join(text)
--- .\misc\file_discovery.py	(original)
+++ .\misc\file_discovery.py	(refactored)
@@ -39,8 +39,8 @@
         path_parts = [path_parts]
 
     BASE_DIRS = basedirs + [
-        os.path.dirname(unicode(__file__, sys.getfilesystemencoding())),
-        os.path.dirname(unicode(sys.executable, sys.getfilesystemencoding())),
+        os.path.dirname(str(__file__, sys.getfilesystemencoding())),
+        os.path.dirname(str(sys.executable, sys.getfilesystemencoding())),
     ]
 
     # Freedesktop standard
--- .\misc\ini.py	(original)
+++ .\misc\ini.py	(refactored)
@@ -51,7 +51,7 @@
     from sets import Set as set
 
 from iniparse import config
-from ConfigParser import DEFAULTSECT, ParsingError, MissingSectionHeaderError
+from configparser import DEFAULTSECT, ParsingError, MissingSectionHeaderError
 
 class LineType(object):
     line = None
@@ -422,7 +422,7 @@
         self._sections = {}
         if defaults is None: defaults = {}
         self._defaults = INISection(LineContainer(), optionxformsource=self)
-        for name, value in defaults.iteritems():
+        for name, value in defaults.items():
             self._defaults[name] = value
         if fp is not None:
             self.readfp(fp)
--- .\misc\lru.py	(original)
+++ .\misc\lru.py	(refactored)
@@ -53,7 +53,7 @@
         while len(self) >= self.maxsize <= len(self.queue):
             cullsize = max(int(len(self.queue) / self.cullsize), 2)
             try:
-                for i in xrange(cullsize):
+                for i in range(cullsize):
                     self.queue.popleft()
             except IndexError:
                 # queue is empty, bail out.
@@ -65,7 +65,7 @@
             # collected
             if self.aggressive_gc:
                 rounds = min(max(int(self.aggressive_gc), 5), 50)
-                for i in xrange(rounds):
+                for i in range(rounds):
                     gc.collect()
             else:
                 gc.collect()
--- .\misc\multistring.py	(original)
+++ .\misc\multistring.py	(refactored)
@@ -26,7 +26,7 @@
 
 class multistring(autoencode.autoencode):
 
-    def __new__(newtype, string=u"", encoding=None, errors=None):
+    def __new__(newtype, string="", encoding=None, errors=None):
         if isinstance(string, list):
             if not string:
                 raise ValueError("multistring must contain at least one string")
@@ -54,8 +54,8 @@
                 return cmp(self.strings[1:], otherstring.strings[1:])
         elif isinstance(otherstring, autoencode.autoencode):
             return cmp(autoencode.autoencode(self), otherstring)
-        elif isinstance(otherstring, unicode):
-            return cmp(unicode(self), otherstring)
+        elif isinstance(otherstring, str):
+            return cmp(str(self), otherstring)
         elif isinstance(otherstring, str):
             return cmp(str(self), otherstring)
         elif isinstance(otherstring, list) and otherstring:
--- .\misc\optrecurse.py	(original)
+++ .\misc\optrecurse.py	(refactored)
@@ -26,9 +26,9 @@
 import traceback
 import optparse
 try:
-    from cStringIO import StringIO
+    from io import StringIO
 except ImportError:
-    from StringIO import StringIO
+    from io import StringIO
 
 from translate.misc import progressbar
 from translate import __version__
@@ -228,8 +228,8 @@
         templateformats = []
         self.outputoptions = {}
         self.usetemplates = usetemplates
-        for formatgroup, outputoptions in formats.iteritems():
-            if isinstance(formatgroup, (str, unicode)) or formatgroup is None:
+        for formatgroup, outputoptions in formats.items():
+            if isinstance(formatgroup, str) or formatgroup is None:
                 formatgroup = (formatgroup, )
             if not isinstance(formatgroup, tuple):
                 raise ValueError("formatgroups must be tuples or None/str/unicode")
@@ -287,7 +287,7 @@
         }
         progressoption = optparse.Option(None, "--progress", dest="progress",
                 default="bar",
-                choices=self.progresstypes.keys(), metavar="PROGRESS",
+                choices=list(self.progresstypes.keys()), metavar="PROGRESS",
                 help="show progress as: %s" % (", ".join(self.progresstypes)))
         self.define_option(progressoption)
 
@@ -304,7 +304,7 @@
     def getformathelp(self, formats):
         """Make a nice help string for describing formats..."""
         if None in formats:
-            formats = filter(lambda format: format is not None, formats)
+            formats = [format for format in formats if format is not None]
         if len(formats) == 0:
             return ""
         elif len(formats) == 1:
@@ -468,7 +468,7 @@
                 try:
                     self.warning("Output directory does not exist. Attempting to create")
                     os.mkdir(options.output)
-                except IOError, e:
+                except IOError as e:
                     self.error(optparse.OptionValueError("Output directory does not exist, attempt to create failed"))
             if isinstance(options.input, list):
                 inputfiles = self.recurseinputfilelist(options)
@@ -504,7 +504,7 @@
                 fulloutputpath = self.getfulloutputpath(options, outputpath)
                 if options.recursiveoutput and outputpath:
                     self.checkoutputsubdir(options, os.path.dirname(outputpath))
-            except Exception, error:
+            except Exception as error:
                 if isinstance(error, KeyboardInterrupt):
                     raise
                 self.warning("Couldn't handle input file %s" %
@@ -514,7 +514,7 @@
                 success = self.processfile(fileprocessor, options,
                                            fullinputpath, fulloutputpath,
                                            fulltemplatepath)
-            except Exception, error:
+            except Exception as error:
                 if isinstance(error, KeyboardInterrupt):
                     raise
                 self.warning("Error processing: input %s, output %s, template %s" %
--- .\misc\ourdom.py	(original)
+++ .\misc\ourdom.py	(refactored)
@@ -44,7 +44,7 @@
     writer.write(indent + "<" + self.tagName)
 
     attrs = self._get_attributes()
-    a_names = attrs.keys()
+    a_names = list(attrs.keys())
     a_names.sort()
 
     for a_name in a_names:
@@ -114,7 +114,7 @@
 #    else:
 #      return results[0]
     try:
-        result = results.next()
+        result = next(results)
         return result
     except StopIteration:
         return None
@@ -267,7 +267,7 @@
 def parse(file, parser=None, bufsize=None):
     """Parse a file into a DOM by filename or file object."""
     builder = ExpatBuilderNS()
-    if isinstance(file, basestring):
+    if isinstance(file, str):
         fp = open(file, 'rb')
         try:
             result = builder.parseFile(fp)
--- .\misc\profiling.py	(original)
+++ .\misc\profiling.py	(refactored)
@@ -32,7 +32,7 @@
 
     def output(self, out_file):
         self.out_file = out_file
-        print >> out_file, 'events: Ticks'
+        print('events: Ticks', file=out_file)
         self._print_summary()
         for entry in self.data:
             self._entry(entry)
@@ -42,7 +42,7 @@
         for entry in self.data:
             totaltime = int(entry.totaltime * 1000)
             max_cost = max(max_cost, totaltime)
-        print >> self.out_file, 'summary: %d' % (max_cost,)
+        print('summary: %d' % (max_cost,), file=self.out_file)
 
     def _entry(self, entry):
         out_file = self.out_file
@@ -50,14 +50,14 @@
         inlinetime = int(entry.inlinetime * 1000)
         #print >> out_file, 'ob=%s' % (code.co_filename,)
         if isinstance(code, str):
-            print >> out_file, 'fi=~'
+            print('fi=~', file=out_file)
         else:
-            print >> out_file, 'fi=%s' % (code.co_filename,)
-        print >> out_file, 'fn=%s' % (label(code),)
+            print('fi=%s' % (code.co_filename,), file=out_file)
+        print('fn=%s' % (label(code),), file=out_file)
         if isinstance(code, str):
-            print >> out_file, '0 ', inlinetime
+            print('0 ', inlinetime, file=out_file)
         else:
-            print >> out_file, '%d %d' % (code.co_firstlineno, inlinetime)
+            print('%d %d' % (code.co_firstlineno, inlinetime), file=out_file)
         # recursive calls are counted in entry.calls
         if entry.calls:
             calls = entry.calls
@@ -69,22 +69,22 @@
             lineno = code.co_firstlineno
         for subentry in calls:
             self._subentry(lineno, subentry)
-        print >> out_file
+        print(file=out_file)
 
     def _subentry(self, lineno, subentry):
         out_file = self.out_file
         code = subentry.code
         totaltime = int(subentry.totaltime * 1000)
         #print >> out_file, 'cob=%s' % (code.co_filename,)
-        print >> out_file, 'cfn=%s' % (label(code),)
+        print('cfn=%s' % (label(code),), file=out_file)
         if isinstance(code, str):
-            print >> out_file, 'cfi=~'
-            print >> out_file, 'calls=%d 0' % (subentry.callcount,)
+            print('cfi=~', file=out_file)
+            print('calls=%d 0' % (subentry.callcount,), file=out_file)
         else:
-            print >> out_file, 'cfi=%s' % (code.co_filename,)
-            print >> out_file, 'calls=%d %d' % (
-                subentry.callcount, code.co_firstlineno)
-        print >> out_file, '%d %d' % (lineno, totaltime)
+            print('cfi=%s' % (code.co_filename,), file=out_file)
+            print('calls=%d %d' % (
+                subentry.callcount, code.co_firstlineno), file=out_file)
+        print('%d %d' % (lineno, totaltime), file=out_file)
 
 
 def profile_func(filename=None, mode='w+'):
--- .\misc\progressbar.py	(original)
+++ .\misc\progressbar.py	(refactored)
@@ -97,7 +97,7 @@
     def show(self, verbosemessage):
         """displays the progress bar"""
         # pylint: disable=W0613
-        print self
+        print(self)
 
 
 class MessageProgressBar(ProgressBar):
--- .\misc\quote.py	(original)
+++ .\misc\quote.py	(refactored)
@@ -22,9 +22,10 @@
 of delimiters"""
 
 import logging
-import htmlentitydefs
+import html.entities
 
 from translate.misc.typecheck import accepts, returns
+import collections
 
 
 def find_all(searchin, substr):
@@ -137,7 +138,7 @@
     significant_places.sort()
     extracted = ""
     lastpos = 0
-    callable_includeescapes = callable(includeescapes)
+    callable_includeescapes = isinstance(includeescapes, collections.Callable)
     checkescapes = callable_includeescapes or not includeescapes
     for pos in significant_places:
         if instring and pos in enddelim_places and lastpos != pos - lenstart:
@@ -153,7 +154,7 @@
                         replace_escape = includeescapes(section[epos:epos + lenescape + 1])
                         # TODO: deprecate old method of returning boolean from
                         # includeescape, by removing this if block
-                        if not isinstance(replace_escape, basestring):
+                        if not isinstance(replace_escape, str):
                             if replace_escape:
                                 replace_escape = section[epos:epos + lenescape + 1]
                             else:
@@ -190,25 +191,25 @@
     return (extracted, instring)
 
 
-@accepts(unicode)
-@returns(unicode)
+@accepts(str)
+@returns(str)
 def htmlentityencode(source):
--- .\misc\selector.py	(original)
+++ .\misc\selector.py	(refactored)
@@ -133,7 +133,7 @@
         method_dict = dict(method_dict)
         method_dict.update(http_methods)
         if self.wrap is not None:
-            for meth, cbl in method_dict.items():
+            for meth, cbl in list(method_dict.items()):
                 method_dict[meth] = self.wrap(cbl)
         regex = self.parser(self.prefix + path)
         compiled_regex = re.compile(regex, re.DOTALL | re.MULTILINE)
@@ -144,12 +144,12 @@
         app, svars, methods, matched = \
             self.select(environ['PATH_INFO'], environ['REQUEST_METHOD'])
         unnamed, named = [], {}
-        for k, v in svars.iteritems():
+        for k, v in svars.items():
             if k.startswith('__pos'):
                 k = k[5:]
             named[k] = v
         environ['selector.vars'] = dict(named)
-        for k in named.keys():
+        for k in list(named.keys()):
             if k.isdigit():
                 unnamed.append((k, named.pop(k)))
         unnamed.sort(); unnamed = [v for k, v in unnamed]
@@ -169,13 +169,13 @@
         for regex, method_dict in self.mappings:
             match = regex.search(path)
             if match:
-                methods = method_dict.keys()
-                if method_dict.has_key(method):
+                methods = list(method_dict.keys())
+                if method in method_dict:
                     return (method_dict[method], 
                             match.groupdict(), 
                             methods, 
                             match.group(0))
-                elif method_dict.has_key('_ANY_'):
+                elif '_ANY_' in method_dict:
                     return (method_dict['_ANY_'],
                             match.groupdict(), 
                             methods, 
@@ -386,14 +386,14 @@
         """Turn a path expression into regex."""
         if self.ostart in text:
             parts = self.outermost_optionals_split(text)
-            parts = map(self.parse, parts)
+            parts = list(map(self.parse, parts))
             parts[1::2] = ["(%s)?" % p for p in parts[1::2]]
         else:
             parts = [part.split(self.end) 
                      for part in text.split(self.start)]
             parts = [y for x in parts for y in x]
-            parts[::2] = map(re.escape, parts[::2])
-            parts[1::2] = map(self.lookup, parts[1::2])
+            parts[::2] = list(map(re.escape, parts[::2]))
+            parts[1::2] = list(map(self.lookup, parts[1::2]))
         return ''.join(parts)
 
     def __call__(self, url_pattern):
@@ -513,7 +513,7 @@
         args = list(args)
         args.insert(0, start_response)
         args.insert(0, environ)
-        return apply(func, args, dict(kwargs))
+        return func(*args, **dict(kwargs))
     return wsgi_func
 
         
@@ -533,5 +533,5 @@
         args.insert(0, start_response)
         args.insert(0, environ)
         args.insert(0, self)
-        return apply(meth, args, dict(kwargs))
+        return meth(*args, **dict(kwargs))
     return wsgi_meth
--- .\misc\sparse.py	(original)
+++ .\misc\sparse.py	(refactored)
@@ -174,7 +174,7 @@
         """apply a tokenizer to a set of text, flattening the result"""
         tokenizedlists = [tokenizer(text) for text in inputlist]
         joined = []
-        map(joined.extend, tokenizedlists)
+        list(map(joined.extend, tokenizedlists))
         return joined
 
     def applytokenizers(self, inputlist, tokenizers):
--- .\misc\test_autoencode.py	(original)
+++ .\misc\test_autoencode.py	(refactored)
@@ -10,21 +10,21 @@
 
     def test_default_encoding(self):
         """tests that conversion to string uses the encoding attribute"""
-        s = self.type2test(u'unicode string', 'utf-8')
+        s = self.type2test('unicode string', 'utf-8')
         assert s.encoding == 'utf-8'
         assert str(s) == 'unicode string'
-        s = self.type2test(u'\u20ac')
-        assert str(self.type2test(u'\u20ac', 'utf-8')) == '\xe2\x82\xac'
+        s = self.type2test('\u20ac')
+        assert str(self.type2test('\u20ac', 'utf-8')) == '\xe2\x82\xac'
 
     def test_uniqueness(self):
         """tests constructor creates unique objects"""
-        s1 = unicode(u'unicode string')
-        s2 = unicode(u'unicode string')
+        s1 = str('unicode string')
+        s2 = str('unicode string')
         assert s1 == s2
         assert s1 is s2
-        s1 = self.type2test(u'unicode string', 'utf-8')
-        s2 = self.type2test(u'unicode string', 'ascii')
-        s3 = self.type2test(u'unicode string', 'utf-8')
+        s1 = self.type2test('unicode string', 'utf-8')
+        s2 = self.type2test('unicode string', 'ascii')
+        s3 = self.type2test('unicode string', 'utf-8')
         assert s1 == s2 == s3
         assert s1 is not s2
         # even though all the attributes are the same, this is a mutable type
--- .\misc\test_multistring.py	(original)
+++ .\misc\test_multistring.py	(refactored)
@@ -33,22 +33,22 @@
         result = s1.replace("e", "xx")
         assert result == t(["abcdxxf", "dxxf"])
 
-        result = s1.replace("e", u"\xe9")
-        assert result == t([u"abcd\xe9f", u"d\xe9f"])
+        result = s1.replace("e", "\xe9")
+        assert result == t(["abcd\xe9f", "d\xe9f"])
 
         result = s1.replace("e", "\n")
-        assert result == t([u"abcd\nf", u"d\nf"])
+        assert result == t(["abcd\nf", "d\nf"])
 
         result = result.replace("\n", "\\n")
-        assert result == t([u"abcd\\nf", u"d\\nf"])
+        assert result == t(["abcd\\nf", "d\\nf"])
 
         result = result.replace("\\n", "\n")
-        assert result == t([u"abcd\nf", u"d\nf"])
+        assert result == t(["abcd\nf", "d\nf"])
 
         s2 = t(["abcdeef", "deef"])
 
         result = s2.replace("e", "g")
-        assert result == t([u"abcdggf", u"dggf"])
+        assert result == t(["abcdggf", "dggf"])
 
         result = s2.replace("e", "g", 1)
-        assert result == t([u"abcdgef", u"dgef"])
+        assert result == t(["abcdgef", "dgef"])
--- .\misc\test_optrecurse.py	(original)
+++ .\misc\test_optrecurse.py	(refactored)
@@ -16,5 +16,5 @@
         dirname = os.path.join("some", "path", "to")
         fullpath = os.path.join(dirname, filename)
         root = os.path.join(dirname, name)
-        print fullpath
+        print(fullpath)
         assert self.parser.splitext(fullpath) == (root, extension)
--- .\misc\test_quote.py	(original)
+++ .\misc\test_quote.py	(refactored)
@@ -65,28 +65,28 @@
 class TestEncoding:
 
     def test_javepropertiesencode(self):
-        assert quote.javapropertiesencode(u"abc") == u"abc"
--- .\misc\textwrap.py	(original)
+++ .\misc\textwrap.py	(refactored)
@@ -69,7 +69,7 @@
     whitespace_trans = string.maketrans(_whitespace, ' ' * len(_whitespace))
 
     unicode_whitespace_trans = {}
-    uspace = ord(u' ')
+    uspace = ord(' ')
     for x in map(ord, _whitespace):
         unicode_whitespace_trans[x] = uspace
 
@@ -127,7 +127,7 @@
         if self.replace_whitespace:
             if isinstance(text, str):
                 text = text.translate(self.whitespace_trans)
-            elif isinstance(text, unicode):
+            elif isinstance(text, str):
                 text = text.translate(self.unicode_whitespace_trans)
         return text
 
@@ -144,7 +144,7 @@
           'use', ' ', 'the', ' ', '-b', ' ', 'option!'
         """
         chunks = self.wordsep_re.split(text)
-        chunks = filter(None, chunks)
+        chunks = [_f for _f in chunks if _f]
         return chunks
 
     def _fix_sentence_endings(self, chunks):
@@ -375,4 +375,4 @@
 if __name__ == "__main__":
     #print dedent("\tfoo\n\tbar")
     #print dedent("  \thello there\n  \t  how are you?")
-    print dedent("Hello there.\n  This is indented.")
+    print(dedent("Hello there.\n  This is indented."))
--- .\misc\wStringIO.py	(original)
+++ .\misc\wStringIO.py	(refactored)
@@ -21,18 +21,18 @@
 """A wrapper for cStringIO that provides more of the functions of
 StringIO at the speed of cStringIO"""
 
-import cStringIO
+import io
 
 
 class StringIO:
 
     def __init__(self, buf=''):
-        if not isinstance(buf, (str, unicode)):
+        if not isinstance(buf, str):
             buf = str(buf)
-        if isinstance(buf, unicode):
+        if isinstance(buf, str):
             buf = buf.encode('utf-8')
         self.len = len(buf)
-        self.buf = cStringIO.StringIO()
+        self.buf = io.StringIO()
         self.buf.write(buf)
         self.buf.seek(0)
         self.pos = 0
@@ -41,7 +41,7 @@
     def __iter__(self):
         return self
 
-    def next(self):
+    def __next__(self):
         if self.closed:
             raise StopIteration
         r = self.readline()
--- .\misc\xml_helpers.py	(original)
+++ .\misc\xml_helpers.py	(refactored)
@@ -45,9 +45,9 @@
     an optional default to use in case nothing is specified in this node."""
     xml_space = getXMLspace(node, xml_space)
     if xml_space == "default":
-        return unicode(string_xpath_normalized(node))  # specific to lxml.etree
+        return str(string_xpath_normalized(node))  # specific to lxml.etree
     else:
-        return unicode(string_xpath(node))  # specific to lxml.etree
+        return str(string_xpath(node))  # specific to lxml.etree
 
     # If we want to normalise space and only preserve it when the directive
     # xml:space="preserve" is given in node or in parents, consider this code:
@@ -104,7 +104,7 @@
 def normalize_space(text):
     """Normalize the given text for implementation of
     ``xml:space="default"``."""
-    text = MULTIWHITESPACE_RE.sub(u" ", text)
+    text = MULTIWHITESPACE_RE.sub(" ", text)
     return text
 
 
@@ -116,10 +116,10 @@
         return
     if node.text:
         node.text = normalize_space(node.text)
-        if remove_start and node.text[0] == u" ":
+        if remove_start and node.text[0] == " ":
             node.text = node.text.lstrip()
             remove_start = False
-        if len(node.text) > 0 and node.text.endswith(u" "):
+        if len(node.text) > 0 and node.text.endswith(" "):
             remove_start = True
         if len(node) == 0:
             node.text = node.text.rstrip()
--- .\misc\xmlwrapper.py	(original)
+++ .\misc\xmlwrapper.py	(refactored)
@@ -78,7 +78,7 @@
         self.obj = obj
         self.namespace, self.tag = splitnamespace(self.obj.tag)
         self.attrib = {}
-        for fullkey, value in self.obj.attrib.iteritems():
+        for fullkey, value in self.obj.attrib.items():
             namespace, key = splitnamespace(fullkey)
             self.attrib[key] = value
 
--- .\misc\typecheck\__init__.py	(original)
+++ .\misc\typecheck\__init__.py	(refactored)
@@ -9,6 +9,7 @@
 import types
 
 from types import GeneratorType, FunctionType, MethodType, ClassType, TypeType
+import collections
 
 # Controls whether typechecking is on (True) or off (False)
 enable_checking = True
@@ -61,8 +62,8 @@
         try:
             return ", " + self.inner.error_message()
         except:
-            print "'%s'" % self.inner.message
-            raw_input()
+            print("'%s'" % self.inner.message)
+            input()
             raise
 
 class _TC_IndexError(_TC_NestedError):
@@ -264,7 +265,7 @@
 _registered_hooks = dict([(_h, set()) for _h in _hooks])
 
 def _manage_registration(add_remove, reg_type):
-    if not isinstance(reg_type, (types.ClassType, types.TypeType)):
+    if not isinstance(reg_type, type):
         raise ValueError("registered types must be classes or types")
     
     valid = False
@@ -337,7 +338,7 @@
         key_types = set()
         val_types = set()
         
-        for (k,v) in obj.items():
+        for (k,v) in list(obj.items()):
             key_types.add( calculate_type(k) )
             val_types.add( calculate_type(v) )
             
@@ -411,7 +412,7 @@
     name = "Single"
 
     def __init__(self, type):
-        if not isinstance(type, (types.ClassType, types.TypeType)):
+        if not isinstance(type, type):
             raise TypeError("Cannot type-check a %s" % type(type))
         else:
             self.type = type
@@ -437,7 +438,7 @@
         
     @classmethod
     def __typesig__(cls, obj):
-        if isinstance(obj, (types.ClassType, types.TypeType)):
+        if isinstance(obj, type):
             return Single(obj)
 
 ### Provide a way to enforce the empty-ness of iterators    
@@ -470,20 +471,20 @@
         self._types = [key, val]
         
     def __typecheck__(self, func, to_check):
-        if not isinstance(to_check, types.DictType):
+        if not isinstance(to_check, dict):
             raise _TC_TypeError(to_check, self.type)
         
-        for (k, v) in to_check.items():
+        for (k, v) in list(to_check.items()):
             # Check the key
             try:
                 check_type(self.__check_key, func, k)
-            except _TC_Exception, inner:
+            except _TC_Exception as inner:
                 raise _TC_KeyError(k, inner)
 
             # Check the value
             try:
                 check_type(self.__check_val, func, v)
-            except _TC_Exception, inner:
+            except _TC_Exception as inner:
                 raise _TC_KeyValError(k, v, inner)
         
     def __eq__(self, other):
@@ -506,7 +507,7 @@
         if isinstance(obj, dict):
             if len(obj) == 0:
                 return Empty(dict)
-            return Dict(obj.keys()[0], obj.values()[0])
+            return Dict(list(obj.keys())[0], list(obj.values())[0])
 
 ### Provide typechecking for the built-in list() type
 class List(CheckType):
@@ -539,7 +540,7 @@
         for (i, val, type) in type_tuples:
             try:
                 check_type(type, func, val)
-            except _TC_Exception, e:
+            except _TC_Exception as e:
                 raise _TC_IndexError(i, e)
         
     def __eq__(self, other):
@@ -579,13 +580,13 @@
     def __typecheck__(self, func, to_check):
         # Note that tuples of varying length (e.g., (int, int) and (int, int, int))
         # are separate types, not merely differences in length like lists
-        if not isinstance(to_check, types.TupleType) or len(to_check) != len(self._types):
+        if not isinstance(to_check, tuple) or len(to_check) != len(self._types):
             raise _TC_TypeError(to_check, self.type)
 
         for (i, (val, type)) in enumerate(zip(to_check, self._types)):
             try:
                 check_type(type, func, val)
-            except _TC_Exception, inner:
+            except _TC_Exception as inner:
                 raise _TC_IndexError(i, inner)
         
     @classmethod
@@ -616,9 +617,9 @@
     __repr__ = __str__
 
     def __hash__(self):
-        return hash(''.join([str(o) for o in self.__class__
+        return hash(''.join([str(o) for o in (self.__class__
                                            , hash(type(self.type))
-                                           , hash(self.type)]))
+                                           , hash(self.type))]))
 
     def __eq__(self, other):
         if self.__class__ is not other.__class__:
@@ -645,7 +646,7 @@
 
     @classmethod
     def __typesig__(cls, obj):
-        if isinstance(obj, basestring):
+        if isinstance(obj, str):
             return cls(obj)
 
     @classmethod
@@ -692,7 +693,7 @@
             return cls(obj)
             
         # Snag callable class instances (that aren't types or classes)
-        if type(obj) not in (types.ClassType, type) and callable(obj):
+        if type(obj) not in (type, type) and isinstance(obj, collections.Callable):
             return cls(obj)
             
     def __typecheck__(self, func, to_check):
@@ -789,7 +790,7 @@
         for type in self._types:
             try:
                 check_type(type, func, to_check)
-            except _TC_Exception, e:
+            except _TC_Exception as e:
                 raise _TC_TypeError(to_check, self)
 
 class Not(Or):
@@ -847,7 +848,7 @@
         return self.__class__ is other.__class__
     
     def __typecheck__(self, func, to_check):
-        if not callable(to_check):
+        if not isinstance(to_check, collections.Callable):
             raise _TC_TypeError(to_check, 'a callable')
             
 class HasAttr(CheckType):
@@ -863,17 +864,17 @@
                         
         self._attr_types = dict.fromkeys(attr_sets[list], Any())
         
-        for (attr, typ) in attr_sets[dict].items():
+        for (attr, typ) in list(attr_sets[dict].items()):
             self._attr_types[attr] = Type(typ)
         
     def __typecheck__(self, func, to_check):
-        for (attr, typ) in self._attr_types.items():
+        for (attr, typ) in list(self._attr_types.items()):
             if not hasattr(to_check, attr):
                 raise _TC_MissingAttrError(attr)
                 
             try:
                 check_type(typ, func, getattr(to_check, attr))
-            except _TC_Exception, e:
+            except _TC_Exception as e:
                 raise _TC_AttrError(attr, e)
                 
     def __eq__(self, other):
@@ -890,7 +891,7 @@
         
         any = Any()
         
-        for (attr, typ) in self._attr_types.items():
+        for (attr, typ) in list(self._attr_types.items()):
             if typ == any:
                 any_type.append(attr)
             else:
@@ -920,7 +921,7 @@
     __repr__ = __str__
         
     def __typecheck__(self, func, to_check):
-        if not (hasattr(to_check, '__iter__') and callable(to_check.__iter__)):
+        if not (hasattr(to_check, '__iter__') and isinstance(to_check.__iter__, collections.Callable)):
             raise _TC_TypeError(to_check, "an iterable")
             
 class YieldSeq(CheckType):
@@ -1115,8 +1116,8 @@
         for instance in self._instances:
             inst_attrs = []
         
-            for attr, obj in instance.__dict__.items():
-                if callable(obj) and attr not in bad_members:
+            for attr, obj in list(instance.__dict__.items()):
+                if isinstance(obj, collections.Callable) and attr not in bad_members:
                     inst_attrs.append(attr)
             
             if len(self._interface) == 0:
@@ -1133,7 +1134,7 @@
                 raise _TC_MissingAttrError(method)
 
             attr = getattr(to_check, method)
-            if not callable(attr):
+            if not isinstance(attr, collections.Callable):
                 raise _TC_AttrError(method, _TC_TypeError(attr, IsCallable()))
                 
         self._cache.add(to_check.__class__)
@@ -1212,7 +1213,8 @@
 
     return '(' + ', '.join(_rec_tuple_str(o) for o in obj) + ')'
 
-def _gen_arg_to_param(func, (posargs, varargs, varkw, defaults)):
+def _gen_arg_to_param(func, xxx_todo_changeme):
+    (posargs, varargs, varkw, defaults) = xxx_todo_changeme
     sig_args = list()
     dic_args = list()
     
@@ -1235,20 +1237,20 @@
         dic_args.append(('"%s"' % varkw, varkw))
         sig_args.append('**' + varkw)
 
-    func_name = func.func_name + '_'
+    func_name = func.__name__ + '_'
     while func_name in dic_args:
         func_name += '_'
 
-    func_def = 'def %s(' % func.func_name
+    func_def = 'def %s(' % func.__name__
     func_return = func_code \
                 + '\n\treturn {' \
                 + ', '.join('%s: %s' % kv for kv in dic_args) \
                 + '}'
     
     locals = {}
-    exec func_def + ','.join(sig_args) + '):' + func_return in locals
-    func = locals[func.func_name]
-    func.func_defaults = defaults
+    exec(func_def + ','.join(sig_args) + '):' + func_return, locals)
+    func = locals[func.__name__]
+    func.__defaults__ = defaults
     return func
 
 def _validate_tuple(ref, obj):
@@ -1266,7 +1268,8 @@
     except _TS_TupleError:
         raise _TS_TupleError(ref, obj)
 
-def _param_to_type((params, varg_name, kwarg_name), vargs, kwargs):
+def _param_to_type(xxx_todo_changeme1, vargs, kwargs):
+    (params, varg_name, kwarg_name) = xxx_todo_changeme1
     vargs = list(vargs)
     kwargs = dict(kwargs)
     
@@ -1302,10 +1305,10 @@
         # All parameter slots have been filled, but there are still keyword
         # args remaining with no **kwargs parameter present
         if len(params) == 0 and no_double_star:
-            raise _TS_ExtraKeywordError(kwargs.keys()[0])
+            raise _TS_ExtraKeywordError(list(kwargs.keys())[0])
         
         # Match up remaining keyword args with open parameter slots
-        for p, a in kwargs.items():
+        for p, a in list(kwargs.items()):
             if p in param_value:
                 raise _TS_TwiceTypedError(p, a, param_value[p])
             if p not in params and no_double_star:
@@ -1383,7 +1386,7 @@
 
         try:        
             param_types = _param_to_type((param_list, varg_name, kwarg_name), v_sig, kw_sig)
-        except _TS_Exception, e:
+        except _TS_Exception as e:
             raise TypeSignatureError(e)
         
         ### We need to fix-up the types of the *vargs and **kwargs parameters
@@ -1401,7 +1404,7 @@
         
         # Convert the signatures to types now, rather than rebuild them in every function call
         check_param_types = dict()
-        for k, v in param_types.items():
+        for k, v in list(param_types.items()):
             check_param_types[k] = Type(v)
 
         def __check_args(__vargs, __kwargs):
@@ -1412,9 +1415,9 @@
 
                 # Type-check the keyword arguments
                 try:
-                    for name, val in arg_dict.items():
+                    for name, val in list(arg_dict.items()):
                         check_type(check_param_types[name], wrapped_func, val)
-                except _TC_Exception, e:
+                except _TC_Exception as e:
                     str_name = _rec_tuple_str(name)
                     raise TypeCheckError("Argument %s: " % str_name, val, e)
 
@@ -1464,7 +1467,7 @@
         if enable_checking:
             try:
                 check_type(sig_types, func, return_vals)
-            except _TC_Exception, e:
+            except _TC_Exception as e:
                 stop_checking(func)
                 raise TypeCheckError("Return value: ", return_vals, e)
 
@@ -1485,13 +1488,13 @@
         self.__sig_types = Type(signature)
         self.__needs_stopping = True
 
-    def next(self):
+    def __next__(self):
         gen = self.__real_gen
     
         self.__yield_no += 1
 
         try:
-            return_vals = gen.next()
+            return_vals = next(gen)
         except StopIteration:
             if self.__needs_stopping:
                 stop_checking(gen)
@@ -1501,7 +1504,7 @@
         if enable_checking:
             try:
                 check_type(self.__sig_types, gen, return_vals)
-            except _TC_Exception, e:
+            except _TC_Exception as e:
                 # Insert this error into the chain so we can know
                 # which yield the error occurred at
                 middle_exc = _TC_GeneratorError(self.__yield_no, e)
--- .\misc\typecheck\mixins.py	(original)
+++ .\misc\typecheck\mixins.py	(refactored)
@@ -38,7 +38,7 @@
         for i, item in enumerate(to_check):
             try:
                 check_type(self._type, func, item)
-            except _TC_Exception, e:
+            except _TC_Exception as e:
                 raise _TC_IterationError(i, item, e)
 
     @classmethod    
--- .\misc\typecheck\typeclasses.py	(original)
+++ .\misc\typecheck\typeclasses.py	(refactored)
@@ -3,7 +3,7 @@
 ### Number
 ####################################################
 
-_numbers = [int, float, complex, long, bool]
+_numbers = [int, float, complex, int, bool]
 try:
     from decimal import Decimal
     _numbers.append(Decimal)
@@ -17,7 +17,7 @@
 ### String -- subinstance of ImSequence
 ####################################################
 
-String = Typeclass(str, unicode)
+String = Typeclass(str, str)
     
 ### ImSequence -- immutable sequences
 ####################################################
--- .\misc\wsgiserver\__init__.py	(original)
+++ .\misc\wsgiserver\__init__.py	(refactored)
@@ -6,4 +6,4 @@
            'Gateway', 'WSGIGateway', 'WSGIGateway_10', 'WSGIGateway_u0',
            'WSGIPathInfoDispatcher', 'get_ssl_adapter_class']
 
-from wsgiserver import *
+from .wsgiserver import *
--- .\misc\wsgiserver\ssl_pyopenssl.py	(original)
+++ .\misc\wsgiserver\ssl_pyopenssl.py	(refactored)
@@ -67,7 +67,7 @@
                 time.sleep(self.ssl_retry)
             except SSL.WantWriteError:
                 time.sleep(self.ssl_retry)
-            except SSL.SysCallError, e:
+            except SSL.SysCallError as e:
                 if is_reader and e.args == (-1, 'Unexpected EOF'):
                     return ""
 
@@ -75,7 +75,7 @@
                 if is_reader and errnum in wsgiserver.socket_errors_to_ignore:
                     return ""
                 raise socket.error(errnum)
-            except SSL.Error, e:
+            except SSL.Error as e:
                 if is_reader and e.args == (-1, 'Unexpected EOF'):
                     return ""
 
--- .\misc\wsgiserver\wsgiserver.py	(original)
+++ .\misc\wsgiserver\wsgiserver.py	(refactored)
@@ -81,7 +81,7 @@
 try:
     import queue
 except:
-    import Queue as queue
+    import queue as queue
 import re
 import rfc822
 import socket
@@ -89,12 +89,12 @@
 if 'win' in sys.platform and not hasattr(socket, 'IPPROTO_IPV6'):
     socket.IPPROTO_IPV6 = 41
 try:
-    import cStringIO as StringIO
+    import io as StringIO
 except ImportError:
-    import StringIO
+    import io
 DEFAULT_BUFFER_SIZE = -1
 
-_fileobject_uses_str_type = isinstance(socket._fileobject(None)._rbuf, basestring)
+_fileobject_uses_str_type = isinstance(socket._fileobject(None)._rbuf, str)
 
 import threading
 import time
@@ -108,22 +108,22 @@
         etype = value = tb = None
 
 
-from urllib import unquote
-from urlparse import urlparse
+from urllib.parse import unquote
+from urllib.parse import urlparse
 import warnings
 
 if sys.version_info >= (3, 0):
     bytestr = bytes
     unicodestr = str
-    basestring = (bytes, str)
+    str = (bytes, str)
     def ntob(n, encoding='ISO-8859-1'):
         """Return the given native string as a byte string in the given encoding."""
         # In Python 3, the native string type is unicode
         return n.encode(encoding)
 else:
     bytestr = str
-    unicodestr = unicode
-    basestring = basestring
+    unicodestr = str
+    str = str
     def ntob(n, encoding='ISO-8859-1'):
         """Return the given native string as a byte string in the given encoding."""
         # In Python 2, the native string type is bytes. Assume it's already
@@ -304,8 +304,8 @@
         self._check_length()
         return data
 
-    def next(self):
-        data = self.rfile.next()
+    def __next__(self):
+        data = next(self.rfile)
         self.bytes_read += len(data)
         self._check_length()
         return data
@@ -969,7 +969,7 @@
             try:
                 bytes_sent = self.send(data)
                 data = data[bytes_sent:]
-            except socket.error, e:
+            except socket.error as e:
                 if e.args[0] not in socket_errors_nonblocking:
                     raise
 
@@ -990,7 +990,7 @@
                 data = self._sock.recv(size)
                 self.bytes_read += len(data)
                 return data
-            except socket.error, e:
+            except socket.error as e:
                 if (e.args[0] not in socket_errors_nonblocking
                     and e.args[0] not in socket_error_eintr):
                     raise
@@ -1008,7 +1008,7 @@
             buf.seek(0, 2)  # seek end
             if size < 0:
                 # Read until EOF
-                self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
+                self._rbuf = io.StringIO()  # reset _rbuf.  we consume it via buf.
                 while True:
                     data = self.recv(rbufsize)
                     if not data:
@@ -1022,11 +1022,11 @@
                     # Already have size bytes in our buffer?  Extract and return.
                     buf.seek(0)
                     rv = buf.read(size)
-                    self._rbuf = StringIO.StringIO()
+                    self._rbuf = io.StringIO()
                     self._rbuf.write(buf.read())
                     return rv
 
-                self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
+                self._rbuf = io.StringIO()  # reset _rbuf.  we consume it via buf.
                 while True:
                     left = size - buf_len
                     # recv() will malloc the amount of memory given as its
@@ -1064,7 +1064,7 @@
                 buf.seek(0)
                 bline = buf.readline(size)
                 if bline.endswith('\n') or len(bline) == size:
-                    self._rbuf = StringIO.StringIO()
+                    self._rbuf = io.StringIO()
                     self._rbuf.write(buf.read())
                     return bline
                 del bline
@@ -1074,7 +1074,7 @@
                     # Speed up unbuffered case
                     buf.seek(0)
                     buffers = [buf.read()]
-                    self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
+                    self._rbuf = io.StringIO()  # reset _rbuf.  we consume it via buf.
                     data = None
                     recv = self.recv
                     while data != "\n":
@@ -1085,7 +1085,7 @@
                     return "".join(buffers)
 
                 buf.seek(0, 2)  # seek end
-                self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
+                self._rbuf = io.StringIO()  # reset _rbuf.  we consume it via buf.
                 while True:
                     data = self.recv(self._rbufsize)
                     if not data:
@@ -1106,10 +1106,10 @@
                 if buf_len >= size:
                     buf.seek(0)
                     rv = buf.read(size)
-                    self._rbuf = StringIO.StringIO()
+                    self._rbuf = io.StringIO()
                     self._rbuf.write(buf.read())
                     return rv
-                self._rbuf = StringIO.StringIO()  # reset _rbuf.  we consume it via buf.
+                self._rbuf = io.StringIO()  # reset _rbuf.  we consume it via buf.
                 while True:
                     data = self.recv(self._rbufsize)
                     if not data:
@@ -1701,19 +1701,19 @@
             'Threads Idle': lambda s: getattr(self.requests, "idle", None),
             'Socket Errors': 0,
             'Requests': lambda s: (not s['Enabled']) and -1 or sum([w['Requests'](w) for w
-                                       in s['Worker Threads'].values()], 0),
+                                       in list(s['Worker Threads'].values())], 0),
             'Bytes Read': lambda s: (not s['Enabled']) and -1 or sum([w['Bytes Read'](w) for w
-                                         in s['Worker Threads'].values()], 0),
+                                         in list(s['Worker Threads'].values())], 0),
             'Bytes Written': lambda s: (not s['Enabled']) and -1 or sum([w['Bytes Written'](w) for w
-                                            in s['Worker Threads'].values()], 0),
+                                            in list(s['Worker Threads'].values())], 0),
             'Work Time': lambda s: (not s['Enabled']) and -1 or sum([w['Work Time'](w) for w
-                                         in s['Worker Threads'].values()], 0),
+                                         in list(s['Worker Threads'].values())], 0),
             'Read Throughput': lambda s: (not s['Enabled']) and -1 or sum(
                 [w['Bytes Read'](w) / (w['Work Time'](w) or 1e-6)
-                 for w in s['Worker Threads'].values()], 0),
+                 for w in list(s['Worker Threads'].values())], 0),
             'Write Throughput': lambda s: (not s['Enabled']) and -1 or sum(
                 [w['Bytes Written'](w) / (w['Work Time'](w) or 1e-6)
-                 for w in s['Worker Threads'].values()], 0),
+                 for w in list(s['Worker Threads'].values())], 0),
             'Worker Threads': {},
             }
         logging.statistics["CherryPy HTTPServer %d" % id(self)] = self.stats
@@ -1789,7 +1789,7 @@
                     getattr(self, 'ssl_certificate_chain', None))
 
         # Select the appropriate socket
-        if isinstance(self.bind_addr, basestring):
+        if isinstance(self.bind_addr, str):
             # AF_UNIX socket
 
             # So we can reuse the socket...
@@ -1933,7 +1933,7 @@
 
             conn = self.ConnectionClass(self, s, makefile)
 
-            if not isinstance(self.bind_addr, basestring):
+            if not isinstance(self.bind_addr, str):
                 # optional values
                 # Until we do DNS lookups, omit REMOTE_HOST
                 if addr is None: # sometimes this can happen
@@ -1994,7 +1994,7 @@
 
         sock = getattr(self, "socket", None)
         if sock:
-            if not isinstance(self.bind_addr, basestring):
+            if not isinstance(self.bind_addr, str):
                 # Touch our own socket to make accept() return immediately.
                 try:
                     host, port = sock.getsockname()[:2]
@@ -2051,7 +2051,7 @@
 def get_ssl_adapter_class(name='pyopenssl'):
     """Return an SSL adapter class for the given name."""
     adapter = ssl_adapters[name.lower()]
-    if isinstance(adapter, basestring):
+    if isinstance(adapter, str):
         last_dot = adapter.rfind(".")
         attr_name = adapter[last_dot + 1:]
         mod_path = adapter[:last_dot]
@@ -2151,7 +2151,7 @@
         # exc_info tuple."
         if self.req.sent_headers:
             try:
-                raise exc_info[0], exc_info[1], exc_info[2]
+                raise exc_info[0](exc_info[1]).with_traceback(exc_info[2])
             finally:
                 exc_info = None
 
@@ -2233,7 +2233,7 @@
             'wsgi.version': (1, 0),
             }
 
-        if isinstance(req.server.bind_addr, basestring):
+        if isinstance(req.server.bind_addr, str):
             # AF_UNIX. This isn't really allowed by WSGI, which doesn't
             # address unix domain sockets. But it's better than nothing.
             env["SERVER_PORT"] = ""
@@ -2241,7 +2241,7 @@
             env["SERVER_PORT"] = str(req.server.bind_addr[1])
 
         # Request headers
-        for k, v in req.inheaders.iteritems():
+        for k, v in req.inheaders.items():
             env["HTTP_" + k.upper().replace("-", "_")] = v
 
         # CONTENT_TYPE/CONTENT_LENGTH
@@ -2269,19 +2269,19 @@
         """Return a new environ dict targeting the given wsgi.version"""
         req = self.req
         env_10 = WSGIGateway_10.get_environ(self)
-        env = dict([(k.decode('ISO-8859-1'), v) for k, v in env_10.iteritems()])
-        env[u'wsgi.version'] = ('u', 0)
+        env = dict([(k.decode('ISO-8859-1'), v) for k, v in env_10.items()])
+        env['wsgi.version'] = ('u', 0)
 
         # Request-URI
-        env.setdefault(u'wsgi.url_encoding', u'utf-8')
+        env.setdefault('wsgi.url_encoding', 'utf-8')
         try:
-            for key in [u"PATH_INFO", u"SCRIPT_NAME", u"QUERY_STRING"]:
-                env[key] = env_10[str(key)].decode(env[u'wsgi.url_encoding'])
+            for key in ["PATH_INFO", "SCRIPT_NAME", "QUERY_STRING"]:
+                env[key] = env_10[str(key)].decode(env['wsgi.url_encoding'])
         except UnicodeDecodeError:
             # Fall back to latin 1 so apps can transcode if needed.
-            env[u'wsgi.url_encoding'] = u'ISO-8859-1'
-            for key in [u"PATH_INFO", u"SCRIPT_NAME", u"QUERY_STRING"]:
-                env[key] = env_10[str(key)].decode(env[u'wsgi.url_encoding'])
+            env['wsgi.url_encoding'] = 'ISO-8859-1'
+            for key in ["PATH_INFO", "SCRIPT_NAME", "QUERY_STRING"]:
+                env[key] = env_10[str(key)].decode(env['wsgi.url_encoding'])
 
         for k, v in sorted(env.items()):
             if isinstance(v, str) and k not in ('REQUEST_URI', 'wsgi.input'):
--- .\search\lshtein.py	(original)
+++ .\search\lshtein.py	(refactored)
@@ -38,7 +38,7 @@
     l2 = len(b)
     if stopvalue == -1:
         stopvalue = l2
-    current = range(l1 + 1)
+    current = list(range(l1 + 1))
     for i in range(1, l2 + 1):
         previous, current = current, ([i] + [0] * l1)
         least = l2
@@ -159,4 +159,4 @@
 if __name__ == "__main__":
     from sys import argv
     comparer = LevenshteinComparer()
-    print "Similarity:\n%s" % comparer.similarity(argv[1], argv[2], 50)
+    print("Similarity:\n%s" % comparer.similarity(argv[1], argv[2], 50))
--- .\search\match.py	(original)
+++ .\search\match.py	(refactored)
@@ -107,7 +107,7 @@
         """
         if isinstance(units, base.TranslationUnit):
             units = [units]
-        for candidate in itertools.ifilter(self.usable, units):
+        for candidate in filter(self.usable, units):
             simpleunit = base.TranslationUnit("")
             # We need to ensure that we don't pass multistrings futher, since
             # some modules (like the native Levenshtein) can't use it.
@@ -115,8 +115,8 @@
                 if len(candidate.source.strings) > 1:
                     simpleunit.orig_source = candidate.source
                     simpleunit.orig_target = candidate.target
-                simpleunit.source = unicode(candidate.source)
-                simpleunit.target = unicode(candidate.target)
+                simpleunit.source = str(candidate.source)
+                simpleunit.target = str(candidate.target)
             else:
                 simpleunit.source = candidate.source
                 simpleunit.target = candidate.target
@@ -203,7 +203,7 @@
         def notzero(item):
             score = item[0]
             return score != 0
-        bestcandidates = filter(notzero, bestcandidates)
+        bestcandidates = list(filter(notzero, bestcandidates))
         #Sort for use as a general list, and reverse so the best one is at index 0
         bestcandidates.sort(reverse=True)
         return self.buildunits(bestcandidates)
--- .\search\segment.py	(original)
+++ .\search\segment.py	(refactored)
@@ -23,7 +23,7 @@
 #XXX: This module is now deprecated: Use language specific segmenters in the
 # lang package (character_iter, word_iter, sentence_iter, etc.).
 
--- .\search\indexing\CommonIndexer.py	(original)
+++ .\search\indexing\CommonIndexer.py	(refactored)
@@ -162,7 +162,7 @@
         """
         # turn a dict into a list if necessary
         if isinstance(args, dict):
-            args = args.items()
+            args = list(args.items())
         # turn 'args' into a list if necessary
         if not isinstance(args, list):
             args = [args]
@@ -176,19 +176,19 @@
             elif isinstance(query, tuple):
                 field, value = query
                 # perform unicode normalization
-                field = translate.lang.data.normalize(unicode(field))
-                value = translate.lang.data.normalize(unicode(value))
+                field = translate.lang.data.normalize(str(field))
+                value = translate.lang.data.normalize(str(value))
                 # check for the choosen match type
                 if analyzer is None:
                     analyzer = self.get_field_analyzers(field)
                 result.append(self._create_query_for_field(field, value,
                         analyzer=analyzer))
             # parse plaintext queries
-            elif isinstance(query, basestring):
+            elif isinstance(query, str):
                 if analyzer is None:
                     analyzer = self.analyzer
                 # perform unicode normalization
-                query = translate.lang.data.normalize(unicode(query))
+                query = translate.lang.data.normalize(str(query))
                 result.append(self._create_query_for_string(query,
                         require_all=require_all, analyzer=analyzer))
             else:
@@ -288,7 +288,7 @@
         """
         doc = self._create_empty_document()
         if isinstance(data, dict):
-            data = data.items()
+            data = list(data.items())
         # add all data
         for dataset in data:
             if isinstance(dataset, tuple):
@@ -297,7 +297,7 @@
                 if key is None:
                     if isinstance(value, list):
                         terms = value[:]
-                    elif isinstance(value, basestring):
+                    elif isinstance(value, str):
                         terms = [value]
                     else:
                         raise ValueError("Invalid data type to be indexed: %s" \
@@ -313,7 +313,7 @@
                     for one_term in value:
                         self._add_field_term(doc, key, self._decode(one_term),
                                 (analyze_settings & self.ANALYZER_TOKENIZE > 0))
-            elif isinstance(dataset, basestring):
+            elif isinstance(dataset, str):
                 self._add_plain_term(doc, self._decode(dataset),
                         (self.ANALYZER_DEFAULT & self.ANALYZER_TOKENIZE > 0))
             else:
@@ -447,7 +447,7 @@
         if len(ident_list) == 0:
             # no matching items
             return 0
-        if isinstance(ident_list[0], int) or isinstance(ident_list[0], long):
+        if isinstance(ident_list[0], int) or isinstance(ident_list[0], int):
             # create a list of IDs of all successfully removed documents
             success_delete = [match for match in ident_list
                     if self.delete_document_by_id(match)]
@@ -520,9 +520,9 @@
         :type field_analyzers: dict containing field names and analyzers
         :raise TypeError: invalid values in *field_analyzers*
         """
-        for field, analyzer in field_analyzers.items():
+        for field, analyzer in list(field_analyzers.items()):
             # check for invald input types
-            if not isinstance(field, (str, unicode)):
+            if not isinstance(field, str):
                 raise TypeError("field name must be a string")
             if not isinstance(analyzer, int):
                 raise TypeError("the analyzer must be a whole number (int)")
@@ -548,7 +548,7 @@
             # return a copy
             return dict(self.field_analyzers)
         # one field is requested
-        if isinstance(fieldnames, (str, unicode)):
+        if isinstance(fieldnames, str):
             if fieldnames in self.field_analyzers:
                 return self.field_analyzers[fieldnames]
             else:
@@ -566,11 +566,11 @@
         unicode normalization."""
         if isinstance(text, str):
             try:
-                result = unicode(text.decode("UTF-8"))
-            except UnicodeEncodeError, e:
-                result = unicode(text.decode("charmap"))
-        elif not isinstance(text, unicode):
-            result = unicode(text)
+                result = str(text.decode("UTF-8"))
+            except UnicodeEncodeError as e:
+                result = str(text.decode("charmap"))
+        elif not isinstance(text, str):
+            result = str(text)
         else:
             result = text
         # perform unicode normalization
--- .\search\indexing\PyLuceneIndexer.py	(original)
+++ .\search\indexing\PyLuceneIndexer.py	(refactored)
@@ -43,7 +43,7 @@
     PyLucene.initVM(PyLucene.CLASSPATH)
     _COMPILER = 'jcc'
 
-import CommonIndexer
+from . import CommonIndexer
 
 
 UNNAMED_FIELD_NAME = "FieldWithoutAName"
@@ -92,7 +92,7 @@
             # try to open an existing database
             tempreader = PyLucene.IndexReader.open(self.location)
             tempreader.close()
-        except PyLucene.JavaError, err_msg:
+        except PyLucene.JavaError as err_msg:
             # Write an error out, in case this is a real problem instead of an absence of an index
             # TODO: turn the following two lines into debug output
             #errorstr = str(e).strip() + "\n" + self.errorhandler.traceback_str()
@@ -106,7 +106,7 @@
                 if not os.path.isdir(parent_path):
                     # recursively create all directories up to parent_path
                     os.makedirs(parent_path)
-            except IOError, err_msg:
+            except IOError as err_msg:
                 raise OSError("Indexer: failed to create the parent " \
                         + "directory (%s) of the indexing database: %s" \
                         % (parent_path, err_msg))
@@ -114,7 +114,7 @@
                 tempwriter = PyLucene.IndexWriter(self.location,
                         self.pyl_analyzer, True)
                 tempwriter.close()
-            except PyLucene.JavaError, err_msg:
+            except PyLucene.JavaError as err_msg:
                 raise OSError("Indexer: failed to open or create a Lucene" \
                         + " database (%s): %s" % (self.location, err_msg))
         # the indexer is initialized - now we prepare the searcher
@@ -130,7 +130,7 @@
                         self.location)
                     self.searcher = PyLucene.IndexSearcher(self.reader)
                     break
-                except PyLucene.JavaError, e:
+                except PyLucene.JavaError as e:
                     # store error message for possible later re-raise (below)
                     lock_error_msg = e
                     time.sleep(0.01)
@@ -422,7 +422,7 @@
         :return: a list of dicts containing the specified field(s)
         :rtype: list of dicts
         """
-        if isinstance(fieldnames, basestring):
+        if isinstance(fieldnames, str):
             fieldnames = [fieldnames]
         hits = self.searcher.search(query)
         if _COMPILER == 'jcc':
@@ -493,7 +493,7 @@
                 self.reader = PyLucene.IndexReader.open(self.location)
                 self.searcher = PyLucene.IndexSearcher(self.reader)
                 self.index_version = self.reader.getCurrentVersion(self.location)
-        except PyLucene.JavaError, e:
+        except PyLucene.JavaError as e:
             # TODO: add some debugging output?
             #self.errorhandler.logerror("Error attempting to read index - try reindexing: "+str(e))
             pass
--- .\search\indexing\PyLuceneIndexer1.py	(original)
+++ .\search\indexing\PyLuceneIndexer1.py	(refactored)
@@ -26,7 +26,7 @@
 """
 
 # this module is based on PyLuceneIndexer (for PyLucene v2.x)
-import PyLuceneIndexer
+from . import PyLuceneIndexer
 import PyLucene
 
 
@@ -185,7 +185,7 @@
         :return: a list of dicts containing the specified field(s)
         :rtype: list of dicts
         """
-        if isinstance(fieldnames, basestring):
+        if isinstance(fieldnames, str):
             fieldnames = [fieldnames]
         hits = PyLucene.indexSearcher.search(query)
         result = []
--- .\search\indexing\XapianIndexer.py	(original)
+++ .\search\indexing\XapianIndexer.py	(refactored)
@@ -53,7 +53,7 @@
         #FIXME: report is xapian-check command is missing?
         raise ImportError("Running under apache, can't load xapian")
 
-import CommonIndexer
+from . import CommonIndexer
 import xapian
 import os
 import time
@@ -104,7 +104,7 @@
             # try to open an existing database
             try:
                 self.reader = xapian.Database(self.location)
-            except xapian.DatabaseOpeningError, err_msg:
+            except xapian.DatabaseOpeningError as err_msg:
                 raise ValueError("Indexer: failed to open xapian database " \
                         + "(%s) - maybe it is not a xapian database: %s" \
                         % (self.location, str(err_msg)))
@@ -118,7 +118,7 @@
                 if not os.path.isdir(parent_path):
                     # recursively create all directories up to parent_path
                     os.makedirs(parent_path)
-            except IOError, err_msg:
+            except IOError as err_msg:
                 raise OSError("Indexer: failed to create the parent " \
                         + "directory (%s) of the indexing database: %s" \
                         % (parent_path, str(err_msg)))
@@ -126,7 +126,7 @@
                 self.writer = xapian.WritableDatabase(self.location,
                         xapian.DB_CREATE_OR_OPEN)
                 self.flush()
-            except xapian.DatabaseOpeningError, err_msg:
+            except xapian.DatabaseOpeningError as err_msg:
                 raise OSError("Indexer: failed to open or create a xapian " \
                         + "database (%s): %s" % (self.location, str(err_msg)))
 
@@ -384,7 +384,7 @@
         :rtype: list of dicts
         """
         result = []
-        if isinstance(fieldnames, basestring):
+        if isinstance(fieldnames, str):
             fieldnames = [fieldnames]
         try:
             self._walk_matches(query, _extract_fieldvalues,
@@ -412,7 +412,7 @@
             self._delete_stale_lock()
             try:
                 self.writer = xapian.WritableDatabase(self.location, xapian.DB_OPEN)
-            except xapian.DatabaseOpeningError, err_msg:
+            except xapian.DatabaseOpeningError as err_msg:
 
                 raise ValueError("Indexer: failed to open xapian database " \
                                  + "(%s) - maybe it is not a xapian database: %s" \
@@ -435,7 +435,7 @@
                 self.reader = xapian.Database(self.location)
             else:
                 self.reader.reopen()
-        except xapian.DatabaseOpeningError, err_msg:
+        except xapian.DatabaseOpeningError as err_msg:
             raise ValueError("Indexer: failed to open xapian database " \
                              + "(%s) - maybe it is not a xapian database: %s" \
                              % (self.location, str(err_msg)))
@@ -489,7 +489,7 @@
         return term
 
 
-def _extract_fieldvalues(match, (result, fieldnames)):
+def _extract_fieldvalues(match, xxx_todo_changeme):
     """Add a dict of field values to a list.
 
     Usually this function should be used together with :func:`_walk_matches`
@@ -502,7 +502,7 @@
     :param fieldnames: the names of the fields to be added to the dict
     :type fieldnames: list of str
     """
-    # prepare empty dict
+    (result, fieldnames) = xxx_todo_changeme
     item_fields = {}
     # fill the dict
     for term in match["document"].termlist():
--- .\search\indexing\__init__.py	(original)
+++ .\search\indexing\__init__.py	(refactored)
@@ -25,7 +25,8 @@
 import shutil
 import logging
 
-import CommonIndexer
+from . import CommonIndexer
+import collections
 
 """ TODO for indexing engines:
     * get rid of jToolkit.glock dependency
@@ -67,7 +68,7 @@
             continue
         # the module function "is_available" must return "True"
         if not (hasattr(module, "is_available") and \
-                callable(module.is_available) and \
+                isinstance(module.is_available, collections.Callable) and \
                 module.is_available()):
             continue
         for item in dir(module):
@@ -179,4 +180,4 @@
 if __name__ == "__main__":
     # show all supported indexing engines (with fulfilled requirements)
     for ONE_INDEX in _AVAILABLE_INDEXERS:
-        print ONE_INDEX
+        print(ONE_INDEX)
--- .\search\indexing\test_indexers.py	(original)
+++ .\search\indexing\test_indexers.py	(refactored)
@@ -25,8 +25,8 @@
 import shutil
 import pytest
 
-import __init__ as indexing
-import CommonIndexer
+from . import __init__ as indexing
+from . import CommonIndexer
 
 # following block only needs running under pytest; unclear how to detect it?
 
@@ -90,7 +90,7 @@
     # a reasonable foo-bar entry
     database.index_document(["foo", "bar", "med"])
     # and something more for another document with a unicode string
-    database.index_document(["foo", "bar", u"HELO"])
+    database.index_document(["foo", "bar", "HELO"])
     # another similar one - but with "barr" instead of "bar"
     database.index_document(["foo", "barr", "med", "HELO"])
     # some field indexed document data
@@ -436,23 +436,23 @@
     database.flush()
     reader = database.reader
     for index in range(reader.maxDoc()):
-        print reader.document(index).toString().encode("charmap")
+        print(reader.document(index).toString().encode("charmap"))
 
 
 def _show_database_xapian(database):
     import xapian
     doccount = database.reader.get_doccount()
     max_doc_index = database.reader.get_lastdocid()
-    print "Database overview: %d items up to index %d" % (doccount, max_doc_index)
+    print("Database overview: %d items up to index %d" % (doccount, max_doc_index))
     for index in range(1, max_doc_index + 1):
         try:
             document = database.reader.get_document(index)
         except xapian.DocNotFoundError:
             continue
         # print the document's terms and their positions
-        print "\tDocument [%d]: %s" % (index,
+        print("\tDocument [%d]: %s" % (index,
                 str([(one_term.term, [posi for posi in one_term.positer])
-                for one_term in document.termlist()]))
+                for one_term in document.termlist()])))
 
 
 def _get_number_of_docs(database):
@@ -475,8 +475,8 @@
     supposed to fail for a specific indexing engine.
     As this test works now for the engine, the whitelisting should be removed.
     """
-    print "the test '%s' works again for '%s' - please remove the exception" \
-            % (name, get_engine_name(db))
+    print("the test '%s' works again for '%s' - please remove the exception" \
+            % (name, get_engine_name(db)))
 
 
 def report_whitelisted_failure(db, name):
@@ -485,8 +485,8 @@
     Since the test behaves as expected (it fails), this is just for reminding
     developers on these open issues of the indexing engine support.
     """
-    print "the test '%s' fails - as expected for '%s'" % (name,
-            get_engine_name(db))
+    print("the test '%s' fails - as expected for '%s'" % (name,
+            get_engine_name(db)))
 
 
 def assert_whitelisted(db, assert_value, white_list_engines, name_of_check):
@@ -517,11 +517,11 @@
         clean_database()
         engine_name = get_engine_name(_get_indexer(DATABASE))
         if engine_name == default_engine:
-            print "************ running tests for '%s' *****************" \
-                    % engine_name
+            print("************ running tests for '%s' *****************" \
+                    % engine_name)
         else:
-            print "************ SKIPPING tests for '%s' *****************" \
-                    % default_engine
+            print("************ SKIPPING tests for '%s' *****************" \
+                    % default_engine)
             continue
         test_create_database()
         test_open_database()
--- .\services\tmserver.py	(original)
+++ .\services\tmserver.py	(refactored)
@@ -41,7 +41,7 @@
 
     def __init__(self, tmdbfile, tmfiles, max_candidates=3, min_similarity=75,
             max_length=1000, prefix="", source_lang=None, target_lang=None):
-        if not isinstance(tmdbfile, unicode):
+        if not isinstance(tmdbfile, str):
             import sys
             tmdbfile = tmdbfile.decode(sys.getfilesystemencoding())
 
@@ -79,7 +79,7 @@
     def translate_unit(self, environ, start_response, uid, slang, tlang):
         start_response("200 OK", [('Content-type', 'text/plain')])
         candidates = self.tmdb.translate_unit(uid, slang, tlang)
-        logging.debug("candidates: %s", unicode(candidates))
+        logging.debug("candidates: %s", str(candidates))
         response = json.dumps(candidates, indent=4)
         params = parse_qs(environ.get('QUERY_STRING', ''))
         try:
@@ -128,10 +128,10 @@
     @selector.opliant
     def upload_store(self, environ, start_response, sid, slang, tlang):
         """add units from uploaded file to tmdb"""
-        import StringIO
+        import io
         from translate.storage import factory
         start_response("200 OK", [('Content-type', 'text/plain')])
-        data = StringIO.StringIO(environ['wsgi.input'].read(int(environ['CONTENT_LENGTH'])))
+        data = io.StringIO(environ['wsgi.input'].read(int(environ['CONTENT_LENGTH'])))
         data.name = sid
         store = factory.getobject(data)
         count = self.tmdb.add_store(store, slang, tlang)
--- .\storage\_factory_classes.py	(original)
+++ .\storage\_factory_classes.py	(refactored)
@@ -22,20 +22,20 @@
 just for the sake of the Windows installer to easily pick up all the stuff
 that we need and ensure they make it into the installer."""
 
-import csvl10n
-import omegat
-import po
-import mo
-import qm
-import utx
-import wordfast
-import catkeys
-import qph
-import tbx
-import tmx
-import ts2
-import xliff
+from . import csvl10n
+from . import omegat
+from . import po
+from . import mo
+from . import qm
+from . import utx
+from . import wordfast
+from . import catkeys
+from . import qph
+from . import tbx
+from . import tmx
+from . import ts2
+from . import xliff
 try:
-    import trados
-except ImportError, e:
+    from . import trados
+except ImportError as e:
     pass
--- .\storage\aresource.py	(original)
+++ .\storage\aresource.py	(refactored)
@@ -176,13 +176,13 @@
                         max_slice = min(i+5, len(text)-1)
                         codepoint_str = "".join(text[i+1 : max_slice])
                         if len(codepoint_str) < 4:
-                            codepoint_str = u"0" * (4-len(codepoint_str)) + codepoint_str
+                            codepoint_str = "0" * (4-len(codepoint_str)) + codepoint_str
                         try:
                             # We can't trust int() to raise a ValueError,
                             # it will ignore leading/trailing whitespace.
                             if not codepoint_str.isalnum():
                                 raise ValueError(codepoint_str)
-                            codepoint = unichr(int(codepoint_str, 16))
+                            codepoint = chr(int(codepoint_str, 16))
                         except ValueError:
                             raise ValueError('bad unicode escape sequence')
 
@@ -264,9 +264,9 @@
 
     def gettarget(self, lang=None):
         # Grab inner text
-        target = self.unescape(self.xmlelement.text or u'')
+        target = self.unescape(self.xmlelement.text or '')
         # Include markup as well
-        target += u''.join([data.forceunicode(etree.tostring(child, encoding='utf-8')) for child in self.xmlelement.iterchildren()])
+        target += ''.join([data.forceunicode(etree.tostring(child, encoding='utf-8')) for child in self.xmlelement.iterchildren()])
         return target
 
     target = property(gettarget, settarget)
@@ -296,7 +296,7 @@
                     comments.insert(0, prevSibling.text)
                     prevSibling = prevSibling.getprevious()
 
-            return u'\n'.join(comments)
+            return '\n'.join(comments)
         else:
             return super(AndroidResourceUnit, self).getnotes(origin)
 
--- .\storage\base.py	(original)
+++ .\storage\base.py	(refactored)
@@ -21,8 +21,9 @@
 """Base classes for storage interfaces."""
 
 import logging
+import collections
 try:
-    import cPickle as pickle
+    import pickle as pickle
 except ImportError:
     import pickle
 from exceptions import NotImplementedError
@@ -37,11 +38,11 @@
 def force_override(method, baseclass):
     """Forces derived classes to override method."""
 
-    if type(method.im_self) == type(baseclass):
+    if type(method.__self__) == type(baseclass):
         # then this is a classmethod and im_self is the actual class
-        actualclass = method.im_self
+        actualclass = method.__self__
     else:
-        actualclass = method.im_class
+        actualclass = method.__self__.__class__
     if actualclass != baseclass:
         raise NotImplementedError(
             "%s does not reimplement %s as required by %s" % \
@@ -160,7 +161,7 @@
            >>> TranslationUnit.rich_to_multistring(rich)
            multistring(u'foo bar')
         """
-        return multistring([unicode(elem) for elem in elem_list])
+        return multistring([str(elem) for elem in elem_list])
     rich_to_multistring = classmethod(rich_to_multistring)
 
     def multistring_to_rich(self, mulstring):
@@ -335,7 +336,7 @@
 
     def removenotes(self):
         """Remove all the translator's notes."""
-        self.notes = u''
+        self.notes = ''
 
     def adderror(self, errorname, errortext):
         """Adds an error message to this unit.
@@ -442,7 +443,7 @@
     def buildfromunit(cls, unit):
         """Build a native unit from a foreign unit, preserving as much
         information as possible."""
-        if type(unit) == cls and hasattr(unit, "copy") and callable(unit.copy):
+        if type(unit) == cls and hasattr(unit, "copy") and isinstance(unit.copy, collections.Callable):
             return unit.copy()
         newunit = cls(unit.source)
         newunit.target = unit.target
@@ -462,7 +463,7 @@
     def get_state_id(self, n=None):
         if n is None:
             n = self.get_state_n()
-        for state_id, state_range in self.STATE.iteritems():
+        for state_id, state_range in self.STATE.items():
             if state_range[0] <= n < state_range[1]:
                 return state_id
         if self.STATE:
@@ -681,7 +682,7 @@
     def getids(self, filename=None):
         """return a list of unit ids"""
         self.require_index()
-        return self.id_index.keys()
+        return list(self.id_index.keys())
 
     def __getstate__(self):
         odict = self.__dict__.copy()
@@ -765,7 +766,7 @@
 
         for encoding in encodings:
             try:
-                r_text = unicode(text, encoding)
+                r_text = str(text, encoding)
                 r_encoding = encoding
                 break
             except UnicodeDecodeError:
@@ -782,7 +783,7 @@
     def savefile(self, storefile):
         """Write the string representation to the given file (or filename)."""
         storestring = str(self)
-        if isinstance(storefile, basestring):
+        if isinstance(storefile, str):
             mode = 'w'
             if self._binary:
                 mode = 'wb'
@@ -818,7 +819,7 @@
         mode = 'r'
         if cls._binary:
             mode = 'rb'
-        if isinstance(storefile, basestring):
+        if isinstance(storefile, str):
             storefile = open(storefile, mode)
         mode = getattr(storefile, "mode", mode)
         #For some reason GzipFile returns 1, so we have to test for that here
--- .\storage\benchmark.py	(original)
+++ .\storage\benchmark.py	(refactored)
@@ -87,7 +87,7 @@
                 parsedfile = self.StoreClass(open(pofilename, 'r'))
                 count += len(parsedfile.units)
                 self.parsedfiles.append(parsedfile)
-        print "counted %d units" % count
+        print("counted %d units" % count)
 
     def parse_placeables(self):
         """parses placeables"""
@@ -97,7 +97,7 @@
                 placeables.parse(unit.source, placeables.general.parsers)
                 placeables.parse(unit.target, placeables.general.parsers)
             count += len(parsedfile.units)
-        print "counted %d units" % count
+        print("counted %d units" % count)
 
 
 if __name__ == "__main__":
@@ -123,7 +123,7 @@
                             globals(), fromlist=_module)
         storeclass = getattr(module, _class)
     else:
-        print "StoreClass: '%s' is not a base class that the class factory can load" % storetype
+        print("StoreClass: '%s' is not a base class that the class factory can load" % storetype)
         sys.exit()
 
     sample_files = [
@@ -156,10 +156,10 @@
 
         for methodname, methodparam in methods:
             #print methodname, "%d dirs, %d files, %d strings, %d/%d words" % sample_file_sizes
-            print "_______________________________________________________"
+            print("_______________________________________________________")
             statsfile = "%s_%s" % (methodname, storetype) + '_%d_%d_%d_%d_%d.stats' % sample_file_sizes
             cProfile.run('benchmarker.%s(%s)' % (methodname, methodparam), statsfile)
             stats = pstats.Stats(statsfile)
             stats.sort_stats('time').print_stats(20)
-            print "_______________________________________________________"
+            print("_______________________________________________________")
         benchmarker.clear_test_dir()
--- .\storage\bundleprojstore.py	(original)
+++ .\storage\bundleprojstore.py	(refactored)
@@ -79,7 +79,7 @@
             """
         if fname and fname in self.zip.namelist():
             raise ValueError("File already in bundle archive: %s" % (fname))
-        if not fname and isinstance(afile, basestring) and afile in self.zip.namelist():
+        if not fname and isinstance(afile, str) and afile in self.zip.namelist():
             raise ValueError("File already in bundle archive: %s" % (afile))
 
         afile, fname = super(BundleProjectStore, self).append_file(afile, fname, ftype)
@@ -97,7 +97,7 @@
         """Remove the file with the given project name from the project."""
         super(BundleProjectStore, self).remove_file(fname, ftype)
         self._zip_delete([fname])
-        tempfiles = [tmpf for tmpf, prjf in self._tempfiles.iteritems() if prjf == fname]
+        tempfiles = [tmpf for tmpf, prjf in self._tempfiles.items() if prjf == fname]
         if tempfiles:
             for tmpf in tempfiles:
                 try:
@@ -147,7 +147,7 @@
         """Try and find a project file name for the given real file name."""
         try:
             fname = super(BundleProjectStore, self).get_proj_filename(realfname)
-        except ValueError, ve:
+        except ValueError as ve:
             fname = None
         if fname:
             return fname
--- .\storage\catkeys.py	(original)
+++ .\storage\catkeys.py	(refactored)
@@ -71,7 +71,7 @@
 """Default or minimum header entries for a catkeys file"""
 
 _unescape_map = {"\\r": "\r", "\\t": "\t", '\\n': '\n', '\\\\': '\\'}
-_escape_map = dict([(value, key) for (key, value) in _unescape_map.items()])
+_escape_map = dict([(value, key) for (key, value) in list(_unescape_map.items())])
 # We don't yet do escaping correctly, just for lack of time to do it.  The
 # current implementation is just based on something simple that will work with
 # investaged files.  The only escapes found were "\n", "\t", "\\n"
@@ -155,7 +155,7 @@
     def _set_source_or_target(self, key, newvalue):
         if newvalue is None:
             self._dict[key] = None
-        if isinstance(newvalue, unicode):
+        if isinstance(newvalue, str):
             newvalue = newvalue.encode('utf-8')
         newvalue = _escape(newvalue)
         if not key in self._dict or newvalue != self._dict[key]:
@@ -180,7 +180,7 @@
     def getnotes(self, origin=None):
         if not origin or origin in ["programmer", "developer", "source code"]:
             return self._dict["comment"].decode('utf-8')
-        return u""
+        return ""
 
     def getcontext(self):
         return self._dict["context"].decode('utf-8')
@@ -190,14 +190,14 @@
         notes = self.getnotes()
         id = self.source
         if notes:
-            id = u"%s\04%s" % (notes, id)
+            id = "%s\04%s" % (notes, id)
         if context:
-            id = u"%s\04%s" % (context, id)
+            id = "%s\04%s" % (context, id)
         return id
 
     def markfuzzy(self, present=True):
         if present:
-            self.target = u""
+            self.target = ""
 
     def settargetlang(self, newlang):
         self._dict['target-lang'] = newlang
--- .\storage\cpo.py	(original)
+++ .\storage\cpo.py	(refactored)
@@ -43,6 +43,7 @@
 from translate.storage import base, pocommon
 from translate.storage import pypo
 from translate.storage.pocommon import encodingToUse
+import collections
 
 logger = logging.getLogger(__name__)
 
@@ -257,7 +258,7 @@
                 if remainder:
                     return remainder.group(1)
                 else:
-                    return u""
+                    return ""
             else:
                 return text
         singular = remove_msgid_comments((gpo.po_message_msgid(self._gpo_message) or "").decode(self.CPO_ENC))
@@ -270,12 +271,12 @@
             else:
                 return singular
         else:
-            return u""
+            return ""
 
     def setsource(self, source):
         if isinstance(source, multistring):
             source = source.strings
-        if isinstance(source, unicode):
+        if isinstance(source, str):
             source = source.encode(self.CPO_ENC)
         if isinstance(source, list):
             gpo.po_message_set_msgid(self._gpo_message, source[0].encode(self.CPO_ENC))
@@ -298,7 +299,7 @@
             if plurals:
                 multi = multistring(plurals, encoding=self.CPO_ENC)
             else:
-                multi = multistring(u"")
+                multi = multistring("")
         else:
             multi = (gpo.po_message_msgstr(self._gpo_message) or "").decode(self.CPO_ENC)
         return multi
@@ -308,7 +309,7 @@
         if self.hasplural():
             if isinstance(target, multistring):
                 target = target.strings
-            elif isinstance(target, basestring):
+            elif isinstance(target, str):
                 target = [target]
         # for non-plurals: check number of items in 'target'
         elif isinstance(target, (dict, list)):
@@ -332,16 +333,16 @@
         if isinstance(target, list):
             for i in range(len(target)):
                 targetstring = target[i]
-                if isinstance(targetstring, unicode):
+                if isinstance(targetstring, str):
                     targetstring = targetstring.encode(self.CPO_ENC)
                 gpo.po_message_set_msgstr_plural(self._gpo_message, i, targetstring)
         # add the values of a dict
         elif isinstance(target, dict):
-            for i, targetstring in enumerate(target.itervalues()):
+            for i, targetstring in enumerate(target.values()):
                 gpo.po_message_set_msgstr_plural(self._gpo_message, i, targetstring)
         # add a single string
         else:
-            if isinstance(target, unicode):
+            if isinstance(target, str):
                 target = target.encode(self.CPO_ENC)
             if target is None:
                 gpo.po_message_set_msgstr(self._gpo_message, "")
@@ -362,7 +363,7 @@
 #            id = '%s\0%s' % (id, plural)
         context = gpo.po_message_msgctxt(self._gpo_message)
         if context:
-            id = u"%s\04%s" % (context.decode(self.CPO_ENC), id)
+            id = "%s\04%s" % (context.decode(self.CPO_ENC), id)
         return id
 
     def getnotes(self, origin=None):
@@ -512,11 +513,11 @@
             text = (gpo.po_message_msgid(self._gpo_message) or "").decode(self.CPO_ENC)
         if text:
             return pocommon.extract_msgid_comment(text)
-        return u""
+        return ""
 
     def setmsgidcomment(self, msgidcomment):
         if msgidcomment:
-            self.source = u"_: %s\n%s" % (msgidcomment, self.source)
+            self.source = "_: %s\n%s" % (msgidcomment, self.source)
     msgidcomment = property(_extract_msgidcomments, setmsgidcomment)
 
     def __str__(self):
@@ -534,7 +535,7 @@
             if locline == -1:
                 locstring = locname
             else:
-                locstring = locname + u":" + unicode(locline)
+                locstring = locname + ":" + str(locline)
             locations.append(pocommon.unquote_plus(locstring))
             i += 1
             location = gpo.po_message_filepos(self._gpo_message, i)
@@ -567,7 +568,7 @@
     def buildfromunit(cls, unit, encoding=None):
         """Build a native unit from a foreign unit, preserving as much
         information as possible."""
-        if type(unit) == cls and hasattr(unit, "copy") and callable(unit.copy):
+        if type(unit) == cls and hasattr(unit, "copy") and isinstance(unit.copy, collections.Callable):
             return unit.copy()
         elif isinstance(unit, pocommon.pounit):
             newunit = cls(unit.source, encoding)
--- .\storage\csvl10n.py	(original)
+++ .\storage\csvl10n.py	(refactored)
@@ -25,9 +25,9 @@
 import csv
 import codecs
 try:
-    import cStringIO as StringIO
+    import io as StringIO
 except:
-    import StringIO
+    import io
 
 from translate.misc import sparse
 from translate.storage import base
@@ -54,7 +54,7 @@
         else:
             return value
 
-    def next(self):
+    def __next__(self):
         lentokens = len(self.tokens)
         while self.tokenpos < lentokens and self.tokens[self.tokenpos] == "\n":
             self.tokenpos += 1
@@ -106,7 +106,7 @@
 def from_unicode(text, encoding='utf-8'):
     if encoding == 'auto':
         encoding = 'utf-8'
-    if isinstance(text, unicode):
+    if isinstance(text, str):
         return text.encode(encoding)
     return text
 
@@ -114,7 +114,7 @@
 def to_unicode(text, encoding='utf-8'):
     if encoding == 'auto':
         encoding = 'utf-8'
-    if isinstance(text, unicode):
+    if isinstance(text, str):
         return text
     return text.decode(encoding)
 
@@ -140,7 +140,7 @@
         result = self.source
         context = self.context
         if context:
-            result = u"%s\04%s" % (context, result)
+            result = "%s\04%s" % (context, result)
 
         return result
 
@@ -193,7 +193,7 @@
                 self.translator_comments = text
 
     def removenotes(self):
-        self.translator_comments = u''
+        self.translator_comments = ''
 
     def isfuzzy(self):
         if self.fuzzy.lower() in ('1', 'x', 'true', 'yes', 'fuzzy'):
@@ -209,7 +209,7 @@
     def match_header(self):
         """see if unit might be a header"""
         some_value = False
-        for key, value in self.todict().iteritems():
+        for key, value in self.todict().items():
             if value:
                 some_value = True
             if key.lower() != 'fuzzy' and value and key.lower() != value.lower():
@@ -235,7 +235,7 @@
         return source, target
 
     def fromdict(self, cedict, encoding='utf-8'):
-        for key, value in cedict.iteritems():
+        for key, value in cedict.items():
             rkey = fieldname_map.get(key, key)
             if value is None or key is None or key == EXTRA_KEY:
                 continue
@@ -326,7 +326,7 @@
 
 def detect_header(sample, dialect, fieldnames):
     """Test if file has a header or not, also returns number of columns in first row"""
-    inputfile = StringIO.StringIO(sample)
+    inputfile = io.StringIO(sample)
     try:
         reader = csv.reader(inputfile, dialect)
     except csv.Error:
@@ -337,7 +337,7 @@
             inputfile.seek(0)
             reader = csv.reader(inputfile, 'excel')
 
-    header = reader.next()
+    header = next(reader)
     columncount = max(len(header), 3)
     if valid_fieldnames(header):
         return header
@@ -359,7 +359,7 @@
         if not fieldnames:
             self.fieldnames = ['location', 'source', 'target', 'id', 'fuzzy', 'context', 'translator_comments', 'developer_comments']
         else:
-            if isinstance(fieldnames, basestring):
+            if isinstance(fieldnames, str):
                 fieldnames = [fieldname.strip() for fieldname in fieldnames.split(",")]
             self.fieldnames = fieldnames
         self.filename = getattr(inputfile, 'name', '')
@@ -379,7 +379,7 @@
         sniffer = csv.Sniffer()
         # FIXME: maybe we should sniff a smaller sample
         sample = csvsrc[:1024]
-        if isinstance(sample, unicode):
+        if isinstance(sample, str):
             sample = sample.encode('utf-8')
 
         try:
@@ -414,7 +414,7 @@
     def __str__(self):
         """convert to a string. double check that unicode is handled somehow here"""
         source = self.getoutput()
-        if not isinstance(source, unicode):
+        if not isinstance(source, str):
             source = source.decode('utf-8')
         if not self.encoding or self.encoding == 'auto':
             encoding = 'utf-8'
@@ -423,7 +423,7 @@
         return source.encode(encoding)
 
     def getoutput(self):
-        outputfile = StringIO.StringIO()
+        outputfile = io.StringIO()
         writer = csv.DictWriter(outputfile, self.fieldnames, extrasaction='ignore', dialect=self.dialect)
         # write header
         hdict = dict(map(None, self.fieldnames, self.fieldnames))
--- .\storage\dtd.py	(original)
+++ .\storage\dtd.py	(refactored)
@@ -91,7 +91,7 @@
 import warnings
 try:
     from lxml import etree
-    import StringIO
+    import io
 except ImportError:
     etree = None
 
@@ -107,8 +107,8 @@
     """Escapes a line for Android DTD files. """
     # Replace "'" character with the \u0027 escape. Other possible replaces are
     # "\\&apos;" or "\\'".
-    source = source.replace(u"'", u"\\u0027")
-    source = source.replace(u"\"", u"\\&quot;")
+    source = source.replace("'", "\\u0027")
+    source = source.replace("\"", "\\&quot;")
     value = quotefordtd(source)  # value is an UTF-8 encoded string.
     return value
 
@@ -116,9 +116,9 @@
 def unquotefromandroid(source):
     """Unquotes a quoted Android DTD definition."""
     value = unquotefromdtd(source)  # value is an UTF-8 encoded string.
-    value = value.replace(u"\\&apos;", u"'")
-    value = value.replace(u"\\'", u"'")
-    value = value.replace(u"\\u0027", u"'")
+    value = value.replace("\\&apos;", "'")
+    value = value.replace("\\'", "'")
+    value = value.replace("\\u0027", "'")
     value = value.replace("\\\"", "\"")  # This converts \&quot; to ".
     return value
 
@@ -155,7 +155,7 @@
     extracted = extracted.replace("&quot;", "\"")
     extracted = extracted.replace("&#x0022;", "\"")
     # FIXME these should probably be handled with a lookup
--- .\storage\factory.py	(original)
+++ .\storage\factory.py	(refactored)
@@ -61,7 +61,7 @@
 
 def _examine_txt(storefile):
     """Determine the true filetype for a .txt file"""
-    if isinstance(storefile, basestring) and os.path.exists(storefile):
+    if isinstance(storefile, str) and os.path.exists(storefile):
         storefile = open(storefile)
     try:
         start = storefile.read(600).strip()
@@ -117,7 +117,7 @@
     """returns the filename"""
     if storefile is None:
         raise ValueError("This method cannot magically produce a filename when given None as input.")
-    if not isinstance(storefile, basestring):
+    if not isinstance(storefile, str):
         if not hasattr(storefile, "name"):
             storefilename = _getdummyname(storefile)
         else:
@@ -171,7 +171,7 @@
     Specify ignore to ignore some part at the back of the name (like .gz).
     """
 
-    if isinstance(storefile, basestring):
+    if isinstance(storefile, str):
         if os.path.isdir(storefile) or storefile.endswith(os.path.sep):
             from translate.storage import directory
             return directory.Directory(storefile)
--- .\storage\fpo.py	(original)
+++ .\storage\fpo.py	(refactored)
@@ -30,12 +30,13 @@
 
 import re
 import copy
-import cStringIO
+import io
 
 from translate.lang import data
 from translate.misc.multistring import multistring
 from translate.storage import pocommon, base, cpo, poparser
 from translate.storage.pocommon import encodingToUse
+import collections
 
 lsep = " "
 """Separator for #: entries"""
@@ -68,9 +69,9 @@
         pocommon.pounit.__init__(self, source)
         self._encoding = encodingToUse(encoding)
         self._initallcomments(blankall=True)
-        self._msgctxt = u""
-
-        self.target = u""
+        self._msgctxt = ""
+
+        self.target = ""
 
     def _initallcomments(self, blankall=False):
         """Initialises allcomments"""
@@ -79,7 +80,7 @@
             self.automaticcomments = []
             self.sourcecomments = []
             self.typecomments = []
-            self.msgidcomment = u""
+            self.msgidcomment = ""
 
     def getsource(self):
         return self._source
@@ -87,11 +88,11 @@
     def setsource(self, source):
         self._rich_source = None
 #        assert isinstance(source, unicode)
-        source = data.forceunicode(source or u"")
-        source = source or u""
+        source = data.forceunicode(source or "")
+        source = source or ""
         if isinstance(source, multistring):
             self._source = source
-        elif isinstance(source, unicode):
+        elif isinstance(source, str):
             self._source = source
         else:
             #unicode, list, dict
@@ -125,12 +126,12 @@
     def getnotes(self, origin=None):
         """Return comments based on origin value (programmer, developer, source code and translator)"""
         if origin == None:
-            comments = u"\n".join(self.othercomments)
-            comments += u"\n".join(self.automaticcomments)
+            comments = "\n".join(self.othercomments)
+            comments += "\n".join(self.automaticcomments)
         elif origin == "translator":
-            comments = u"\n".join(self.othercomments)
+            comments = "\n".join(self.othercomments)
         elif origin in ["programmer", "developer", "source code"]:
-            comments = u"\n".join(self.automaticcomments)
+            comments = "\n".join(self.automaticcomments)
         else:
             raise ValueError("Comment type not valid")
         return comments
@@ -146,9 +147,9 @@
         if origin in ["programmer", "developer", "source code"]:
             autocomments = True
             commentlist = self.automaticcomments
-        if text.endswith(u'\n'):
+        if text.endswith('\n'):
             text = text[:-1]
-        newcomments = text.split(u"\n")
+        newcomments = text.split("\n")
         if position == "append":
             newcomments = commentlist + newcomments
         elif position == "prepend":
@@ -170,7 +171,7 @@
         # self.__shallow__
         shallow = set(self.__shallow__)
         # Make deep copies of all members which are not in shallow
-        for key, value in self.__dict__.iteritems():
+        for key, value in self.__dict__.items():
             if key not in shallow:
                 setattr(new_unit, key, copy.deepcopy(value))
         # Make shallow copies of all members which are in shallow
@@ -206,7 +207,7 @@
 
         def mergelists(list1, list2, split=False):
             #decode where necessary
-            if unicode in [type(item) for item in list2] + [type(item) for item in list1]:
+            if str in [type(item) for item in list2] + [type(item) for item in list1]:
                 for position, item in enumerate(list1):
                     if isinstance(item, str):
                         list1[position] = item.decode("utf-8")
@@ -283,7 +284,7 @@
     def hastypecomment(self, typecomment):
         """Check whether the given type comment is present"""
         # check for word boundaries properly by using a regular expression...
-        return sum(map(lambda tcline: len(re.findall("\\b%s\\b" % typecomment, tcline)), self.typecomments)) != 0
+        return sum([len(re.findall("\\b%s\\b" % typecomment, tcline)) for tcline in self.typecomments]) != 0
 
     def hasmarkedcomment(self, commentmarker):
         """Check whether the given comment marker is present as # (commentmarker) ..."""
@@ -301,8 +302,8 @@
                 self.typecomments.append("#, %s\n" % typecomment)
             else:
                 # this should handle word boundaries properly ...
-                typecomments = map(lambda tcline: re.sub("\\b%s\\b[ \t,]*" % typecomment, "", tcline), self.typecomments)
-                self.typecomments = filter(lambda tcline: tcline.strip() != "#,", typecomments)
+                typecomments = [re.sub("\\b%s\\b[ \t,]*" % typecomment, "", tcline) for tcline in self.typecomments]
+                self.typecomments = [tcline for tcline in typecomments if tcline.strip() != "#,"]
 
     def istranslated(self):
         return super(pounit, self).istranslated() and not self.isobsolete()
@@ -329,7 +330,7 @@
 
     def parse(self, src):
         raise DeprecationWarning("Should not be parsing with a unit")
-        return poparser.parse_unit(poparser.ParseState(cStringIO.StringIO(src), pounit), self)
+        return poparser.parse_unit(poparser.ParseState(io.StringIO(src), pounit), self)
 
     def __str__(self):
         """convert to a string. double check that unicode is handled somehow here"""
@@ -370,7 +371,7 @@
         return self._msgctxt + self.msgidcomment
 
     def setcontext(self, context):
-        context = data.forceunicode(context or u"")
+        context = data.forceunicode(context or "")
         self._msgctxt = context
 
     def getid(self):
@@ -383,15 +384,15 @@
 #        id = '\0'.join(self.source.strings)
         id = self.source
         if self.msgidcomment:
-            id = u"_: %s\n%s" % (context, id)
+            id = "_: %s\n%s" % (context, id)
         elif context:
-            id = u"%s\04%s" % (context, id)
+            id = "%s\04%s" % (context, id)
         return id
 
     def buildfromunit(cls, unit):
         """Build a native unit from a foreign unit, preserving as much
         information as possible."""
-        if type(unit) == cls and hasattr(unit, "copy") and callable(unit.copy):
+        if type(unit) == cls and hasattr(unit, "copy") and isinstance(unit.copy, collections.Callable):
             return unit.copy()
         elif isinstance(unit, pocommon.pounit):
             newunit = cls(unit.source)
@@ -503,7 +504,7 @@
             del self._cpo_store
             if tmp_header_added:
                 self.units = self.units[1:]
-        except Exception, e:
+        except Exception as e:
             raise base.ParseError(e)
 
     def removeduplicates(self, duplicatestyle="merge"):
@@ -544,7 +545,7 @@
                     if duplicatestyle == "merge":
                         addcomment(thepo)
                     else:
-                        thepo._msgctxt += u" ".join(thepo.getlocations())
+                        thepo._msgctxt += " ".join(thepo.getlocations())
                 id_dict[id] = thepo
                 uniqueunits.append(thepo)
         self.units = uniqueunits
@@ -554,7 +555,7 @@
         self._cpo_store = cpo.pofile(encoding=self._encoding, noheader=True)
         try:
             self._build_cpo_from_self()
-        except UnicodeEncodeError, e:
+        except UnicodeEncodeError as e:
             self._encoding = "utf-8"
             self.updateheader(add=True, Content_Type="text/plain; charset=UTF-8")
             self._build_cpo_from_self()
--- .\storage\html.py	(original)
+++ .\storage\html.py	(refactored)
@@ -22,15 +22,15 @@
 """module for parsing html files for translation"""
 
 import re
-from htmlentitydefs import name2codepoint
-import HTMLParser
+from html.entities import name2codepoint
+import html.parser
 
 from translate.storage import base
 from translate.storage.base import ParseError
 
 # Override the piclose tag from simple > to ?> otherwise we consume HTML
 # within the processing instructions
-HTMLParser.piclose = re.compile('\?>')
+html.parser.piclose = re.compile('\?>')
 
 
 strip_html_re = re.compile(r'''
@@ -116,7 +116,7 @@
         return self.locations
 
 
-class htmlfile(HTMLParser.HTMLParser, base.TranslationStore):
+class htmlfile(html.parser.HTMLParser, base.TranslationStore):
     UnitClass = htmlunit
 
     MARKINGTAGS = ["p", "title", "h1", "h2", "h3", "h4", "h5", "h6", "th",
@@ -130,9 +130,9 @@
     INCLUDEATTRS = ["alt", "summary", "standby", "abbr", "content"]
     """Text from these attributes are extracted"""
 
-    SELF_CLOSING_TAGS = [u"area", u"base", u"basefont", u"br", u"col",
-                         u"frame", u"hr", u"img", u"input", u"link", u"meta",
-                         u"param"]
+    SELF_CLOSING_TAGS = ["area", "base", "basefont", "br", "col",
+                         "frame", "hr", "img", "input", "link", "meta",
+                         "param"]
     """HTML self-closing tags.  Tags that should be specified as <img /> but
     might be <img>.
     `Reference <http://learnwebsitemaking.com/htmlselfclosingtags.html>`_"""
@@ -141,20 +141,20 @@
                  callback=None):
         self.units = []
         self.filename = getattr(inputfile, 'name', None)
-        self.currentblock = u""
-        self.currentcomment = u""
+        self.currentblock = ""
+        self.currentcomment = ""
         self.currenttag = None
         self.currentpos = -1
         self.tag_path = []
-        self.filesrc = u""
-        self.currentsrc = u""
+        self.filesrc = ""
+        self.currentsrc = ""
         self.pidict = {}
         if callback is None:
             self.callback = self._simple_callback
         else:
             self.callback = callback
         self.includeuntaggeddata = includeuntaggeddata
-        HTMLParser.HTMLParser.__init__(self)
+        html.parser.HTMLParser.__init__(self)
 
         if inputfile is not None:
             htmlsrc = inputfile.read()
@@ -216,7 +216,7 @@
 
     def pi_unescape(self, text):
         """Replaces the PHP placeholders in text with the real code"""
-        for pi_escaped, pi in self.pidict.items():
+        for pi_escaped, pi in list(self.pidict.items()):
             text = text.replace(pi_escaped, pi)
         return text
 
@@ -260,16 +260,16 @@
 
     def buildtag(self, tag, attrs=None, startend=False):
         """Create an HTML tag"""
-        selfclosing = u""
+        selfclosing = ""
         if startend:
-            selfclosing = u" /"
+            selfclosing = " /"
         if attrs != [] and attrs is not None:
-            return u"<%(tag)s %(attrs)s%(selfclosing)s>" % \
+            return "<%(tag)s %(attrs)s%(selfclosing)s>" % \
                     {"tag": tag,
                      "attrs": " ".join(['%s="%s"' % pair for pair in attrs]),
                      "selfclosing": selfclosing}
         else:
-            return u"<%(tag)s%(selfclosing)s>" % {"tag": tag,
+            return "<%(tag)s%(selfclosing)s>" % {"tag": tag,
                                                   "selfclosing": selfclosing}
 
 #From here on below, follows the methods of the HTMLParser
@@ -372,21 +372,21 @@
 
     def handle_charref(self, name):
         """Handle entries in the form &#NNNN; e.g. &#8417;"""
-        self.handle_data(unichr(int(name)))
+        self.handle_data(chr(int(name)))
 
     def handle_entityref(self, name):
         """Handle named entities of the form &aaaa; e.g. &rsquo;"""
         if name in ['gt', 'lt', 'amp']:
             self.handle_data("&%s;" % name)
         else:
-            self.handle_data(unichr(name2codepoint.get(name, u"&%s;" % name)))
+            self.handle_data(chr(name2codepoint.get(name, "&%s;" % name)))
 
     def handle_comment(self, data):
         # we can place comments above the msgid as translator comments!
         if self.currentcomment == "":
             self.currentcomment = data
         else:
-            self.currentcomment += u'\n' + data
+            self.currentcomment += '\n' + data
         self.filesrc += "<!--%s-->" % data
 
     def handle_pi(self, data):
--- .\storage\ical.py	(original)
+++ .\storage\ical.py	(refactored)
@@ -53,7 +53,7 @@
     this format to understand those if needed.
 """
 import re
-from StringIO import StringIO
+from io import StringIO
 
 import vobject
 
@@ -121,9 +121,9 @@
             input = inisrc
         if isinstance(input, str):
             input = StringIO(input)
-            self._icalfile = vobject.readComponents(input).next()
+            self._icalfile = next(vobject.readComponents(input))
         else:
-            self._icalfile = vobject.readComponents(open(input)).next()
+            self._icalfile = next(vobject.readComponents(open(input)))
         for component in self._icalfile.components():
             if component.name == "VEVENT":
                 for property in component.getChildren():
--- .\storage\ini.py	(original)
+++ .\storage\ini.py	(refactored)
@@ -31,7 +31,7 @@
 """
 
 import re
-from StringIO import StringIO
+from io import StringIO
 
 from translate.misc.ini import INIConfig
 from translate.storage import base
--- .\storage\jsonl10n.py	(original)
+++ .\storage\jsonl10n.py	(refactored)
@@ -68,7 +68,7 @@
 """
 
 import os
-from StringIO import StringIO
+from io import StringIO
 try:
     import json as json  # available since Python 2.6
 except ImportError:
@@ -151,7 +151,7 @@
         base.TranslationStore.__init__(self, unitclass=unitclass)
         self._filter = filter
         self.filename = ''
-        self._file = u''
+        self._file = ''
         if inputfile is not None:
             self.parse(inputfile)
 
@@ -171,7 +171,7 @@
         :param last_node: the last list or dict
         """
         if isinstance(data, dict):
-            for k, v in data.iteritems():
+            for k, v in data.items():
                 for x in self._extract_translatables(v, stop,
                                                           "%s.%s" % (prev, k),
                                                           k, None, data):
@@ -187,7 +187,7 @@
             or (isinstance(last_node, dict) and name_node in stop) \
             or (isinstance(last_node, list) and name_last_node in stop)):
 
-            if isinstance(data, str) or isinstance(data, unicode):
+            if isinstance(data, str) or isinstance(data, str):
                 yield (prev, data, last_node, name_node)
             elif isinstance(data, bool):
                 yield (prev, str(data), last_node, name_node)
@@ -213,7 +213,7 @@
             input = StringIO(input)
         try:
             self._file = json.load(input)
-        except ValueError, e:
+        except ValueError as e:
             raise base.ParseError(e.message)
 
         for k, data, ref, item in self._extract_translatables(self._file,
--- .\storage\lisa.py	(original)
+++ .\storage\lisa.py	(refactored)
@@ -26,7 +26,7 @@
     from lxml import etree
     from translate.misc.xml_helpers import getText, getXMLlang, setXMLlang, \
                                            getXMLspace, setXMLspace, namespaced
-except ImportError, e:
+except ImportError as e:
     raise ImportError("lxml is not installed. It might be possible to continue without support for XML formats.")
 
 from translate.storage import base
@@ -159,8 +159,8 @@
                 if self.textNode:
                     terms = languageNode.iter(self.namespaced(self.textNode))
                     try:
-                        languageNode = terms.next()
-                    except StopIteration, e:
+                        languageNode = next(terms)
+                    except StopIteration as e:
                         pass
                 languageNode.text = text
         else:
@@ -238,7 +238,7 @@
             if terms is None:
                 return None
             try:
-                return getText(terms.next(), xml_space)
+                return getText(next(terms), xml_space)
             except StopIteration:
                 # didn't have the structure we expected
                 return None
--- .\storage\mo.py	(original)
+++ .\storage\mo.py	(refactored)
@@ -49,14 +49,14 @@
 from translate.storage import po
 from translate.storage import poheader
 
-MO_MAGIC_NUMBER = 0x950412deL
+MO_MAGIC_NUMBER = 0x950412de
 
 
 def mounpack(filename='messages.mo'):
     """Helper to unpack Gettext MO files into a Python string"""
     f = open(filename)
     s = f.read()
-    print "\\x%02x" * len(s) % tuple(map(ord, s))
+    print("\\x%02x" * len(s) % tuple(map(ord, s)))
     f.close()
 
 
@@ -123,7 +123,7 @@
 
     def isheader(self):
         """Is this a header entry?"""
-        return self.source == u""
+        return self.source == ""
 
     def istranslatable(self):
         """Is this message translateable?"""
@@ -192,7 +192,7 @@
                 MESSAGES[source.encode("utf-8")] = target
         # using "I" works for 32- and 64-bit systems, but not for 16-bit!
         hash_table = array.array("I", [0] * hash_size)
-        keys = MESSAGES.keys()
+        keys = list(MESSAGES.keys())
         # the keys are sorted in the .mo file
         keys.sort()
         offsets = []
@@ -203,7 +203,7 @@
             # TODO: We don't do any encoding detection from the PO Header
             add_to_hash_table(id, i)
             string = MESSAGES[id]  # id already encoded for use as dictionary key
-            if isinstance(string, unicode):
+            if isinstance(string, str):
                 string = string.encode('utf-8')
             offsets.append((len(ids), len(id), len(strs), len(string)))
             ids = ids + id + '\0'
--- .\storage\mozilla_lang.py	(original)
+++ .\storage\mozilla_lang.py	(refactored)
@@ -45,8 +45,8 @@
             target = self.target
         if self.getnotes():
             notes = ('\n').join(["# %s" % note for note in self.getnotes('developer').split("\n")])
-            return u"%s\n;%s\n%s%s" % (notes, self.source, target, unchanged)
-        return u";%s\n%s%s" % (self.source, target, unchanged)
+            return "%s\n;%s\n%s%s" % (notes, self.source, target, unchanged)
+        return ";%s\n%s%s" % (self.source, target, unchanged)
 
     def getlocations(self):
         return self.locations
@@ -107,6 +107,6 @@
         ret_string = ""
         if self.is_active or self.mark_active:
             ret_string += "## active ##\n"
-        ret_string += u"\n\n\n".join([unicode(unit) for unit in self.units]).encode('utf-8')
+        ret_string += "\n\n\n".join([str(unit) for unit in self.units]).encode('utf-8')
         ret_string += "\n"
         return ret_string
--- .\storage\omegat.py	(original)
+++ .\storage\omegat.py	(refactored)
@@ -92,7 +92,7 @@
     def _set_field(self, key, newvalue):
         if newvalue is None:
             self._dict[key] = None
-        if isinstance(newvalue, unicode):
+        if isinstance(newvalue, str):
             newvalue = newvalue.encode('utf-8')
         if not key in self._dict or newvalue != self._dict[key]:
             self._dict[key] = newvalue
@@ -102,13 +102,13 @@
 
     def addnote(self, text, origin=None, position="append"):
         currentnote = self._get_field('comment')
-        if position == "append" and currentnote is not None and currentnote != u'':
+        if position == "append" and currentnote is not None and currentnote != '':
             self._set_field('comment', currentnote + '\n' + text)
         else:
             self._set_field('comment', text)
 
     def removenotes(self):
-        self._set_field('comment', u'')
+        self._set_field('comment', '')
 
     def getsource(self):
         return self._get_field('source')
--- .\storage\oo.py	(original)
+++ .\storage\oo.py	(refactored)
@@ -43,7 +43,7 @@
 
 normalfilenamechars = "/#.0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ"
 normalizetable = ""
-for i in map(chr, range(256)):
+for i in map(chr, list(range(256))):
     if i in normalfilenamechars:
         normalizetable += i
     else:
@@ -58,7 +58,7 @@
             self.normalchars[ord(char)] = char
 
     def __getitem__(self, key):
-        return self.normalchars.get(key, u"_")
+        return self.normalchars.get(key, "_")
 
 unormalizetable = unormalizechar(normalfilenamechars.decode("ascii"))
 
@@ -161,7 +161,7 @@
 
 def encode_if_needed_utf8(text):
     """Encode a Unicode string the the specified encoding"""
-    if isinstance(text, unicode):
+    if isinstance(text, str):
         return text.encode('UTF-8')
     return text
 
@@ -371,7 +371,7 @@
 
     def listsubfiles(self):
         """returns a list of subfiles in the file"""
-        return self.subfilelines.keys()
+        return list(self.subfilelines.keys())
 
     def __iter__(self):
         """iterates through the subfile names"""
--- .\storage\php.py	(original)
+++ .\storage\php.py	(refactored)
@@ -138,7 +138,7 @@
     def __str__(self):
         """Convert to a string. Double check that unicode is handled somehow."""
         source = self.getoutput()
-        if isinstance(source, unicode):
+        if isinstance(source, str):
             return source.encode(getattr(self, "encoding", "UTF-8"))
         return source
 
--- .\storage\pocommon.py	(original)
+++ .\storage\pocommon.py	(refactored)
@@ -19,7 +19,7 @@
 # along with this program; if not, see <http://www.gnu.org/licenses/>.
 
 import re
-import urllib
+import urllib.request, urllib.parse, urllib.error
 
 from translate.storage import base
 from translate.storage import poheader
@@ -37,23 +37,23 @@
     msgidcomment = msgid_comment_re.match(text)
     if msgidcomment:
         return msgidcomment.group(1)
-    return u""
+    return ""
 
 
 def quote_plus(text):
     """Quote the query fragment of a URL; replacing ' ' with '+'"""
-    return urllib.quote_plus(text.encode("utf-8"))
-
-
-@accepts(unicode)
-@returns(unicode)
+    return urllib.parse.quote_plus(text.encode("utf-8"))
+
+
+@accepts(str)
+@returns(str)
 def unquote_plus(text):
     """unquote('%7e/abc+def') -> '~/abc def'"""
     try:
-        if isinstance(text, unicode):
+        if isinstance(text, str):
             text = text.encode('utf-8')
-        return urllib.unquote_plus(text).decode('utf-8')
-    except UnicodeEncodeError, e:
+        return urllib.parse.unquote_plus(text).decode('utf-8')
+    except UnicodeEncodeError as e:
         # for some reason there is a non-ascii character here. Let's assume it
         # is already unicode (because of originally decoding the file)
         return text
@@ -76,7 +76,7 @@
 
     def adderror(self, errorname, errortext):
         """Adds an error message to this unit."""
-        text = u'(pofilter) %s: %s' % (errorname, errortext)
+        text = '(pofilter) %s: %s' % (errorname, errortext)
         # Don't add the same error twice:
         if text not in self.getnotes(origin='translator'):
             self.addnote(text, origin="translator")
--- .\storage\poheader.py	(original)
+++ .\storage\poheader.py	(refactored)
@@ -76,7 +76,7 @@
     """
     headerargs = dictutils.ordereddict()
     fixedargs = dictutils.cidict()
-    for key, value in kwargs.items():
+    for key, value in list(kwargs.items()):
         key = key.replace("_", "-")
         if key.islower():
             key = key.title()
@@ -91,7 +91,7 @@
             removed.append(key)
         elif add and key in fixedargs:
             headerargs[key] = fixedargs.pop(key)
-    for key, value in existing.iteritems():
+    for key, value in existing.items():
         if not key in removed:
             headerargs[key] = value
     if add:
@@ -220,13 +220,13 @@
                 self._insert_header(header)
         else:
             headeritems = update(self.parseheader(), add, **kwargs)
-            keys = headeritems.keys()
+            keys = list(headeritems.keys())
             if not "Content-Type" in keys or "charset=CHARSET" in headeritems["Content-Type"]:
                 headeritems["Content-Type"] = "text/plain; charset=UTF-8"
             if not "Content-Transfer-Encoding" in keys or "ENCODING" in headeritems["Content-Transfer-Encoding"]:
                 headeritems["Content-Transfer-Encoding"] = "8bit"
             headerString = ""
-            for key, value in headeritems.items():
+            for key, value in list(headeritems.items()):
                 if value is not None:
                     headerString += "%s: %s\n" % (key, value)
             header.target = headerString
@@ -261,7 +261,7 @@
 
     def updateheaderplural(self, nplurals, plural):
         """Update the Plural-Form PO header."""
-        if isinstance(nplurals, basestring):
+        if isinstance(nplurals, str):
             nplurals = int(nplurals)
         self.updateheader(add=True, Plural_Forms="nplurals=%d; plural=%s;" % (nplurals, plural))
 
@@ -301,7 +301,7 @@
         :param lang: the new target language code
         :type lang: str
         """
-        if isinstance(lang, basestring) and len(lang) > 1:
+        if isinstance(lang, str) and len(lang) > 1:
             self.updateheader(add=True, Language=lang, X_Poedit_Language=None, X_Poedit_Country=None)
 
     def getprojectstyle(self):
@@ -374,7 +374,7 @@
         outcontrib = False
         for line in header.getnotes("translator").split('\n'):
             line = line.strip()
-            if line == u"FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.":
+            if line == "FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.":
                 incontrib = True
                 continue
             if author_re.match(line):
@@ -425,7 +425,7 @@
         headerpo.markfuzzy()
         headeritems = self.makeheaderdict(**kwargs)
         headervalue = ""
-        for (key, value) in headeritems.items():
+        for (key, value) in list(headeritems.items()):
             if value is None:
                 continue
             headervalue += "%s: %s\n" % (key, value)
--- .\storage\poparser.py	(original)
+++ .\storage\poparser.py	(refactored)
@@ -62,9 +62,9 @@
         if self.eof:
             return current
         try:
-            self.next_line = self._input_iterator.next()
+            self.next_line = next(self._input_iterator)
             while not self.eof and isspace(self.next_line):
-                self.next_line = self._input_iterator.next()
+                self.next_line = next(self._input_iterator)
         except StopIteration:
             self.next_line = ''
             self.eof = True
@@ -333,7 +333,7 @@
         if isinstance(element, list):
             setattr(unit, attr, decode_list(element, decode))
         else:
-            setattr(unit, attr, dict([(key, decode_list(value, decode)) for key, value in element.items()]))
+            setattr(unit, attr, dict([(key, decode_list(value, decode)) for key, value in list(element.items())]))
 
 
 def parse_header(parse_state, store):
--- .\storage\poxliff.py	(original)
+++ .\storage\poxliff.py	(refactored)
@@ -231,12 +231,13 @@
         """Returns the automatic comments (x-po-autocomment), which corresponds
         to the #. style po comments."""
 
-        def hasautocomment((type, text)):
+        def hasautocomment(xxx_todo_changeme):
+            (type, text) = xxx_todo_changeme
             return type == "x-po-autocomment"
         groups = self.getcontextgroups("po-entry")
         comments = []
         for group in groups:
-            commentpairs = filter(hasautocomment, group)
+            commentpairs = list(filter(hasautocomment, group))
             for (type, text) in commentpairs:
                 comments.append(text)
         return "\n".join(comments)
@@ -245,12 +246,13 @@
         """Returns the translator comments (x-po-trancomment), which corresponds
         to the # style po comments."""
 
-        def hastrancomment((type, text)):
+        def hastrancomment(xxx_todo_changeme1):
+            (type, text) = xxx_todo_changeme1
             return type == "x-po-trancomment"
         groups = self.getcontextgroups("po-entry")
         comments = []
         for group in groups:
-            commentpairs = filter(hastrancomment, group)
+            commentpairs = list(filter(hastrancomment, group))
             for (type, text) in commentpairs:
                 comments.append(text)
         return "\n".join(comments)
@@ -376,25 +378,25 @@
         root_node = self.document.getroot()
         assert root_node.tag == self.namespaced(self.rootNode)
         groups = root_node.iterdescendants(self.namespaced("group"))
-        pluralgroups = filter(ispluralgroup, groups)
+        pluralgroups = list(filter(ispluralgroup, groups))
         termEntries = root_node.iterdescendants(self.namespaced(self.UnitClass.rootNode))
 
-        singularunits = filter(isnonpluralunit, termEntries)
+        singularunits = list(filter(isnonpluralunit, termEntries))
         if len(singularunits) == 0:
             return
         pluralunit_iter = pluralunits(pluralgroups)
         try:
-            nextplural = pluralunit_iter.next()
+            nextplural = next(pluralunit_iter)
         except StopIteration:
             nextplural = None
 
         for entry in singularunits:
             term = self.UnitClass.createfromxmlElement(entry, namespace=self.namespace)
-            if nextplural and unicode(term.getid()) == ("%s[0]" % nextplural.getid()):
+            if nextplural and str(term.getid()) == ("%s[0]" % nextplural.getid()):
                 self.addunit(nextplural, new=False)
                 try:
-                    nextplural = pluralunit_iter.next()
-                except StopIteration, i:
+                    nextplural = next(pluralunit_iter)
+                except StopIteration as i:
                     nextplural = None
             else:
                 self.addunit(term, new=False)
--- .\storage\project.py	(original)
+++ .\storage\project.py	(refactored)
@@ -100,7 +100,7 @@
             raise ValueError('Cannot convert a target document further: %s' % (input_fname))
 
         templ_fname = None
-        if isinstance(template, basestring):
+        if isinstance(template, str):
             template, templ_fname = self.get_file(template)
 
         if template and not templ_fname:
@@ -116,7 +116,7 @@
                 # inputfile is a translatable file, so it needed to be converted
                 # from some input document. Let's try and use that document as a
                 # template for this conversion.
-                for in_name, (out_name, tmpl_name) in self.store.convert_map.items():
+                for in_name, (out_name, tmpl_name) in list(self.store.convert_map.items()):
                     if input_fname == out_name:
                         template, templ_fname = self.get_file(in_name), in_name
                         break
--- .\storage\projstore.py	(original)
+++ .\storage\projstore.py	(refactored)
@@ -19,7 +19,7 @@
 # along with this program; if not, see <http://www.gnu.org/licenses/>.
 
 import os
-from StringIO import StringIO
+from io import StringIO
 
 from lxml import etree
 
@@ -112,7 +112,7 @@
                 lhs in self._targetfiles or \
                 lhs in self._transfiles or \
                 lhs in self._files or \
-                lhs in self._files.values()
+                lhs in list(self._files.values())
 
     # METHODS #
     def append_file(self, afile, fname, ftype='trans', delete_orig=False):
@@ -128,7 +128,7 @@
         if not ftype in self.TYPE_INFO['f_prefix']:
             raise ValueError('Invalid file type: %s' % (ftype))
 
-        if isinstance(afile, basestring) and os.path.isfile(afile) and not fname:
+        if isinstance(afile, str) and os.path.isfile(afile) and not fname:
             # Try and use afile as the file name
             fname, afile = afile, open(afile)
 
@@ -179,7 +179,7 @@
             raise FileNotInProjectError(fname)
         if not ftype:
             # Guess file type (source/trans/target)
-            for ft, prefix in self.TYPE_INFO['f_prefix'].items():
+            for ft, prefix in list(self.TYPE_INFO['f_prefix'].items()):
                 if fname.startswith(prefix):
                     ftype = ft
                     break
@@ -218,7 +218,7 @@
             raise FileNotInProjectError(fname)
 
         rfile = self._files[fname]
-        if isinstance(rfile, basestring):
+        if isinstance(rfile, str):
             rfile = open(rfile, 'rb')
         # Check that the file is actually open
         if getattr(rfile, 'closed', False):
@@ -300,7 +300,7 @@
         # Add conversion mappings
         if self.convert_map:
             conversions_el = etree.Element('conversions')
-            for in_fname, (out_fname, templ_fname) in self.convert_map.iteritems():
+            for in_fname, (out_fname, templ_fname) in self.convert_map.items():
                 if in_fname not in self._files or out_fname not in self._files:
                     continue
                 conv_el = etree.Element('conv')
@@ -324,7 +324,7 @@
         # Add options to settings
         if 'options' in self.settings:
             options_el = etree.Element('options')
-            for option, value in self.settings['options'].items():
+            for option, value in list(self.settings['options'].items()):
                 opt_el = etree.Element('option')
                 opt_el.attrib['name'] = option
                 opt_el.text = value
--- .\storage\properties.py	(original)
+++ .\storage\properties.py	(refactored)
@@ -124,8 +124,8 @@
 eol = "\n"
 
 
-@accepts(unicode, [unicode])
-@returns(IsOneOf(type(None), unicode), int)
+@accepts(str, [str])
+@returns(IsOneOf(type(None), str), int)
 def _find_delimiter(line, delimiters):
     """Find the type and position of the delimiter in a property line.
 
@@ -145,34 +145,34 @@
         delimiter_dict[delimiter] = -1
     delimiters = delimiter_dict
     # Find the position of each delimiter type
-    for delimiter, pos in delimiters.iteritems():
+    for delimiter, pos in delimiters.items():
         prewhitespace = len(line) - len(line.lstrip())
         pos = line.find(delimiter, prewhitespace)
         while pos != -1:
-            if delimiters[delimiter] == -1 and line[pos-1] != u"\\":
+            if delimiters[delimiter] == -1 and line[pos-1] != "\\":
                 delimiters[delimiter] = pos
                 break
             pos = line.find(delimiter, pos + 1)
     # Find the first delimiter
     mindelimiter = None
     minpos = -1
-    for delimiter, pos in delimiters.iteritems():
-        if pos == -1 or delimiter == u" ":
+    for delimiter, pos in delimiters.items():
+        if pos == -1 or delimiter == " ":
             continue
         if minpos == -1 or pos < minpos:
             minpos = pos
             mindelimiter = delimiter
-    if mindelimiter is None and delimiters.get(u" ", -1) != -1:
+    if mindelimiter is None and delimiters.get(" ", -1) != -1:
         # Use space delimiter if we found nothing else
-        return (u" ", delimiters[" "])
+        return (" ", delimiters[" "])
     if (mindelimiter is not None and
-        u" " in delimiters and
-        delimiters[u" "] < delimiters[mindelimiter]):
+        " " in delimiters and
+        delimiters[" "] < delimiters[mindelimiter]):
         # If space delimiter occurs earlier than ":" or "=" then it is the
         # delimiter only if there are non-whitespace characters between it and
         # the other detected delimiter.
-        if len(line[delimiters[u" "]:delimiters[mindelimiter]].strip()) > 0:
-            return (u" ", delimiters[u" "])
+        if len(line[delimiters[" "]:delimiters[mindelimiter]].strip()) > 0:
+            return (" ", delimiters[" "])
     return (mindelimiter, minpos)
 
 
@@ -185,7 +185,7 @@
     return _find_delimiter(line, DialectJava.delimiters)
 
 
-@accepts(unicode)
+@accepts(str)
 @returns(bool)
 def is_line_continuation(line):
     """Determine whether *line* has a line continuation marker.
@@ -211,7 +211,7 @@
         count += 1
     return (count % 2) == 1  # Odd is a line continuation, even is not
 
-@accepts(unicode)
+@accepts(str)
 @returns(bool)
 def is_comment_one_line(line):
     """Determine whether a *line* is a one-line comment.
@@ -222,16 +222,16 @@
     :rtype: bool
     """
     stripped = line.strip()
-    line_starters = (u'#', u'!', u'//', )
+    line_starters = ('#', '!', '//', )
     for starter in line_starters:
         if stripped.startswith(starter):
             return True
-    if stripped.startswith(u'/*') and stripped.endswith(u'*/'):
+    if stripped.startswith('/*') and stripped.endswith('*/'):
         return True
     return False
 
 
-@accepts(unicode)
+@accepts(str)
 @returns(bool)
 def is_comment_start(line):
     """Determine whether a *line* starts a new multi-line comment.
@@ -245,7 +245,7 @@
     return stripped.startswith('/*') and not stripped.endswith('*/')
 
 
-@accepts(unicode)
+@accepts(str)
 @returns(bool)
 def is_comment_end(line):
     """Determine whether a *line* ends a new multi-line comment.
@@ -259,8 +259,8 @@
     return not stripped.startswith('/*') and stripped.endswith('*/')
 
 
-@accepts(unicode)
-@returns(unicode)
+@accepts(str)
+@returns(str)
 def _key_strip(key):
     """Cleanup whitespace found around a key
 
@@ -292,9 +292,9 @@
     name = None
     default_encoding = 'iso-8859-1'
     delimiters = None
-    pair_terminator = u""
-    key_wrap_char = u""
-    value_wrap_char = u""
+    pair_terminator = ""
+    key_wrap_char = ""
+    value_wrap_char = ""
     drop_comments = []
 
     def encode(cls, string, encoding=None):
@@ -302,8 +302,8 @@
         # FIXME: dialects are a bad idea, not possible for subclasses
         # to override key methods
         if encoding != "utf-8":
-            return quote.javapropertiesencode(string or u"")
-        return string or u""
+            return quote.javapropertiesencode(string or "")
+        return string or ""
     encode = classmethod(encode)
 
     def find_delimiter(cls, line):
@@ -325,17 +325,17 @@
 class DialectJava(Dialect):
     name = "java"
     default_encoding = "iso-8859-1"
-    delimiters = [u"=", u":", u" "]
+    delimiters = ["=", ":", " "]
 register_dialect(DialectJava)
 
 
 class DialectJavaUtf8(DialectJava):
     name = "java-utf8"
     default_encoding = "utf-8"
-    delimiters = [u"=", u":", u" "]
+    delimiters = ["=", ":", " "]
 
     def encode(cls, string, encoding=None):
-        return quote.mozillapropertiesencode(string or u"")
+        return quote.mozillapropertiesencode(string or "")
     encode = classmethod(encode)
 register_dialect(DialectJavaUtf8)
 
@@ -348,23 +348,23 @@
 
 class DialectMozilla(DialectJavaUtf8):
     name = "mozilla"
-    delimiters = [u"="]
+    delimiters = ["="]
 register_dialect(DialectMozilla)
 
 
 class DialectGaia(DialectMozilla):
     name = "gaia"
-    delimiters = [u"="]
+    delimiters = ["="]
 register_dialect(DialectGaia)
 
 
 class DialectSkype(Dialect):
     name = "skype"
     default_encoding = "utf-16"
-    delimiters = [u"="]
+    delimiters = ["="]
 
     def encode(cls, string, encoding=None):
-        return quote.mozillapropertiesencode(string or u"")
+        return quote.mozillapropertiesencode(string or "")
     encode = classmethod(encode)
 register_dialect(DialectSkype)
 
@@ -372,12 +372,12 @@
 class DialectStrings(Dialect):
     name = "strings"
     default_encoding = "utf-16"
-    delimiters = [u"="]
-    pair_terminator = u";"
-    key_wrap_char = u'"'
-    value_wrap_char = u'"'
-    out_ending = u';'
-    out_delimiter_wrappers = u' '
+    delimiters = ["="]
+    pair_terminator = ";"
+    key_wrap_char = '"'
+    value_wrap_char = '"'
+    out_ending = ';'
+    out_delimiter_wrappers = ' '
     drop_comments = ["/* No comment provided by engineer. */"]
 
     def key_strip(cls, key):
@@ -414,23 +414,23 @@
         """construct a blank propunit"""
         self.personality = get_dialect(personality)
         super(propunit, self).__init__(source)
-        self.name = u""
-        self.value = u""
-        self.translation = u""
-        self.delimiter = u"="
+        self.name = ""
+        self.value = ""
+        self.translation = ""
+        self.delimiter = "="
         self.comments = []
         self.source = source
         # a pair of symbols to enclose delimiter on the output
         # (a " " can be used for the sake of convenience)
-        self.out_delimiter_wrappers = getattr(self.personality, 'out_delimiter_wrappers', u'')
+        self.out_delimiter_wrappers = getattr(self.personality, 'out_delimiter_wrappers', '')
         # symbol that should end every property sentence
         # (e.g. ";" is required for Mac OS X strings)
-        self.out_ending = getattr(self.personality, 'out_ending', u'')
+        self.out_ending = getattr(self.personality, 'out_ending', '')
 
     def setsource(self, source):
         self._rich_source = None
         source = data.forceunicode(source)
-        self.value = self.personality.encode(source or u"", self.encoding)
+        self.value = self.personality.encode(source or "", self.encoding)
 
     def getsource(self):
         value = quote.propertiesdecode(self.value)
@@ -441,11 +441,11 @@
     def settarget(self, target):
         self._rich_target = None
         target = data.forceunicode(target)
-        self.translation = self.personality.encode(target or u"", self.encoding)
+        self.translation = self.personality.encode(target or "", self.encoding)
 
     def gettarget(self):
         translation = quote.propertiesdecode(self.translation)
-        translation = re.sub(u"\\\\ ", u" ", translation)
+        translation = re.sub("\\\\ ", " ", translation)
         return translation
 
     target = property(gettarget, settarget)
@@ -461,7 +461,7 @@
         """Convert to a string. Double check that unicode is handled
         somehow here."""
         source = self.getoutput()
-        assert isinstance(source, unicode)
+        assert isinstance(source, str)
         return source.encode(self.encoding)
 
     def getoutput(self):
@@ -469,9 +469,9 @@
         .properties file"""
         notes = self.getnotes()
         if notes:
-            notes += u"\n"
+            notes += "\n"
         if self.isblank():
-            return notes + u"\n"
+            return notes + "\n"
         else:
             self.value = self.personality.encode(self.source, self.encoding)
             self.translation = self.personality.encode(self.target, self.encoding)
@@ -490,7 +490,7 @@
             wrappers = self.out_delimiter_wrappers
             delimiter = '%s%s%s' % (wrappers, self.delimiter, wrappers)
             ending = self.out_ending
-            return u"%(notes)s%(key)s%(del)s%(value)s%(ending)s\n" % {"notes": notes,
+            return "%(notes)s%(key)s%(del)s%(value)s%(ending)s\n" % {"notes": notes,
                                                             "key": key,
                                                             "del": delimiter,
                                                             "value": value,
@@ -509,7 +509,7 @@
 
     def getnotes(self, origin=None):
         if origin in ['programmer', 'developer', 'source code', None]:
-            return u'\n'.join(self.comments)
+            return '\n'.join(self.comments)
         else:
             return super(propunit, self).getnotes(origin)
 
@@ -557,7 +557,7 @@
         inmultilinevalue = False
         inmultilinecomment = False
 
-        for line in propsrc.split(u"\n"):
+        for line in propsrc.split("\n"):
             # handle multiline value if we're in one
             line = quote.rstripeol(line)
             if inmultilinevalue:
@@ -592,7 +592,7 @@
                 newunit.delimiter, delimiter_pos = self.personality.find_delimiter(line)
                 if delimiter_pos == -1:
                     newunit.name = self.personality.key_strip(line)
-                    newunit.value = u""
+                    newunit.value = ""
                     self.addunit(newunit)
                     newunit = propunit("", self.personality.name)
                 else:
@@ -613,7 +613,7 @@
         lines = []
         for unit in self.units:
             lines.append(unit.getoutput())
-        uret = u"".join(lines)
+        uret = "".join(lines)
         return uret.encode(self.encoding)
 
 
--- .\storage\pypo.py	(original)
+++ .\storage\pypo.py	(refactored)
@@ -22,9 +22,9 @@
 files (pofile).
 """
 
-from __future__ import generators
+
 import copy
-import cStringIO
+import io
 import re
 
 from translate.lang import data
@@ -40,7 +40,7 @@
 # general functions for quoting / unquoting po strings
 
 po_unescape_map = {"\\r": "\r", "\\t": "\t", '\\"': '"', '\\n': '\n', '\\\\': '\\'}
-po_escape_map = dict([(value, key) for (key, value) in po_unescape_map.items()])
+po_escape_map = dict([(value, key) for (key, value) in list(po_unescape_map.items())])
 
 
 def escapeforpo(line):
@@ -51,7 +51,7 @@
     special_locations = []
     for special_key in po_escape_map:
         special_locations.extend(quote.find_all(line, special_key))
-    special_locations = dict.fromkeys(special_locations).keys()
+    special_locations = list(dict.fromkeys(special_locations).keys())
     special_locations.sort()
     escaped_line = ""
     last_location = 0
@@ -123,7 +123,7 @@
 
     Quotes on either side should already have been removed.
     """
-    escape_places = quote.find_all(line, u"\\")
+    escape_places = quote.find_all(line, "\\")
     if not escape_places:
         return line
 
@@ -138,7 +138,7 @@
         if true_escape:
             true_escape_places.append(escape_pos)
 
-    extracted = u""
+    extracted = ""
     lastpos = 0
     for pos in true_escape_places:
         # everything leading up to the escape
@@ -152,7 +152,7 @@
 
 
 def unquotefrompo(postr):
-    return u"".join([unescape(line[1:-1]) for line in postr])
+    return "".join([unescape(line[1:-1]) for line in postr])
 
 
 def is_null(lst):
@@ -271,7 +271,7 @@
     def gettarget(self):
         """Returns the unescaped msgstr"""
         if isinstance(self.msgstr, dict):
-            return multistring(map(unquotefrompo, self.msgstr.values()), self._encoding)
+            return multistring(list(map(unquotefrompo, list(self.msgstr.values()))), self._encoding)
         else:
             return unquotefrompo(self.msgstr)
 
@@ -283,7 +283,7 @@
         if self.hasplural():
             if isinstance(target, multistring):
                 target = target.strings
-            elif isinstance(target, basestring):
+            elif isinstance(target, str):
                 target = [target]
         elif isinstance(target, (dict, list)):
             if len(target) == 1:
@@ -296,7 +296,7 @@
         if isinstance(target, list):
             self.msgstr = dict([(i, quoteforpo(target[i])) for i in range(len(target))])
         elif isinstance(target, dict):
-            self.msgstr = dict([(i, quoteforpo(targetstring)) for i, targetstring in target.iteritems()])
+            self.msgstr = dict([(i, quoteforpo(targetstring)) for i, targetstring in target.items()])
         else:
             self.msgstr = quoteforpo(target)
     target = property(gettarget, settarget)
@@ -323,12 +323,12 @@
         :param origin: programmer, developer, source code, translator or None
         """
         if origin == None:
-            comments = u"".join([comment[2:] for comment in self.othercomments])
-            comments += u"".join([comment[3:] for comment in self.automaticcomments])
+            comments = "".join([comment[2:] for comment in self.othercomments])
+            comments += "".join([comment[3:] for comment in self.automaticcomments])
         elif origin == "translator":
-            comments = u"".join([comment[2:] for comment in self.othercomments])
+            comments = "".join([comment[2:] for comment in self.othercomments])
         elif origin in ["programmer", "developer", "source code"]:
-            comments = u"".join([comment[3:] for comment in self.automaticcomments])
+            comments = "".join([comment[3:] for comment in self.automaticcomments])
         else:
             raise ValueError("Comment type not valid")
         # Let's drop the last newline
@@ -373,7 +373,7 @@
         # self.__shallow__
         shallow = set(self.__shallow__)
         # Make deep copies of all members which are not in shallow
-        for key, value in self.__dict__.iteritems():
+        for key, value in self.__dict__.items():
             if key not in shallow:
                 setattr(new_unit, key, copy.deepcopy(value))
         # Make shallow copies of all members which are in shallow
@@ -396,7 +396,7 @@
 
     def _msgstrlen(self):
         if isinstance(self.msgstr, dict):
-            combinedstr = "\n".join(filter(None, [unquotefrompo(msgstr) for msgstr in self.msgstr.itervalues()]))
+            combinedstr = "\n".join([_f for _f in [unquotefrompo(msgstr) for msgstr in self.msgstr.values()] if _f])
             return len(combinedstr)
         else:
             return len(unquotefrompo(self.msgstr))
@@ -410,7 +410,7 @@
 
         def mergelists(list1, list2, split=False):
             #decode where necessary
-            if unicode in [type(item) for item in list2] + [type(item) for item in list1]:
+            if str in [type(item) for item in list2] + [type(item) for item in list1]:
                 for position, item in enumerate(list1):
                     if isinstance(item, str):
                         list1[position] = item.decode("utf-8")
@@ -574,11 +574,11 @@
         return len(self.msgid_plural) > 0
 
     def parse(self, src):
-        return poparser.parse_unit(poparser.ParseState(cStringIO.StringIO(src), pounit), self)
+        return poparser.parse_unit(poparser.ParseState(io.StringIO(src), pounit), self)
 
     def _getmsgpartstr(self, partname, partlines, partcomments=""):
         if isinstance(partlines, dict):
-            partkeys = partlines.keys()
+            partkeys = list(partlines.keys())
             partkeys.sort()
             return "".join([self._getmsgpartstr("%s[%d]" % (partname, partkey), partlines[partkey], partcomments) for partkey in partkeys])
         partstr = partname + " "
@@ -621,7 +621,7 @@
 
     def _encodeifneccessary(self, output):
         """Encodes unicode strings and returns other strings unchanged"""
-        if isinstance(output, unicode):
+        if isinstance(output, str):
             encoding = encodingToUse(getattr(self, "_encoding", "UTF-8"))
             return output.encode(encoding)
         return output
@@ -661,24 +661,24 @@
                 # We need to account for a multiline msgid or msgstr here
                 obsoletelines[index] = obsoleteline.replace('\n"', '\n#~ "')
             lines.extend(obsoletelines)
-            return u"".join(lines)
+            return "".join(lines)
         # if there's no msgid don't do msgid and string, unless we're the
         # header this will also discard any comments other than plain
         # othercomments...
         if is_null(self.msgid):
             if not (self.isheader() or self.getcontext() or self.sourcecomments):
-                return u"".join(lines)
+                return "".join(lines)
         lines.extend(self.automaticcomments)
         lines.extend(self.sourcecomments)
         lines.extend(self.typecomments)
         add_prev_msgid_info(lines, prefix="#|")
         if self.msgctxt:
-            lines.append(self._getmsgpartstr(u"msgctxt", self.msgctxt))
-        lines.append(self._getmsgpartstr(u"msgid", self.msgid, self.msgidcomments))
+            lines.append(self._getmsgpartstr("msgctxt", self.msgctxt))
+        lines.append(self._getmsgpartstr("msgid", self.msgid, self.msgidcomments))
         if self.msgid_plural or self.msgid_pluralcomments:
-            lines.append(self._getmsgpartstr(u"msgid_plural", self.msgid_plural, self.msgid_pluralcomments))
-        lines.append(self._getmsgpartstr(u"msgstr", self.msgstr))
-        postr = u"".join(lines)
+            lines.append(self._getmsgpartstr("msgid_plural", self.msgid_plural, self.msgid_pluralcomments))
+        lines.append(self._getmsgpartstr("msgstr", self.msgstr))
+        postr = "".join(lines)
         return postr
 
     def getlocations(self):
@@ -745,9 +745,9 @@
 #        id = '\0'.join(self.source.strings)
         id = self.source
         if self.msgidcomments:
-            id = u"_: %s\n%s" % (context, id)
+            id = "_: %s\n%s" % (context, id)
         elif context:
-            id = u"%s\04%s" % (context, id)
+            id = "%s\04%s" % (context, id)
         return id
 
 
@@ -764,7 +764,7 @@
             elif not getattr(self, 'filename', ''):
                 self.filename = ''
             if isinstance(input, str):
-                input = cStringIO.StringIO(input)
+                input = io.StringIO(input)
             # clear units to get rid of automatically generated headers before parsing
             self.units = []
             poparser.parse_units(poparser.ParseState(input, pounit), self)
@@ -819,10 +819,10 @@
         """Convert to a string. Double check that unicode is handled somehow
         here"""
         output = self._getoutput()
-        if isinstance(output, unicode):
+        if isinstance(output, str):
             try:
                 return output.encode(getattr(self, "_encoding", "UTF-8"))
-            except UnicodeEncodeError, e:
+            except UnicodeEncodeError as e:
                 self.updateheader(add=True, Content_Type="text/plain; charset=UTF-8")
                 self._encoding = "UTF-8"
                 for unit in self.units:
@@ -835,12 +835,12 @@
         """convert the units back to lines"""
         lines = []
         for unit in self.units:
-            unitsrc = unit._getoutput() + u"\n"
+            unitsrc = unit._getoutput() + "\n"
             lines.append(unitsrc)
-        lines = u"".join(lines).rstrip()
+        lines = "".join(lines).rstrip()
         #After the last pounit we will have \n\n and we only want to end in \n:
         if lines:
-            lines += u"\n"
+            lines += "\n"
         return lines
 
     def encode(self, lines):
@@ -850,7 +850,7 @@
         if encoding is None or encoding.lower() == "charset":
             encoding = 'UTF-8'
         for line in lines:
-            if isinstance(line, unicode):
+            if isinstance(line, str):
                 line = line.encode(encoding)
             newlines.append(line)
         return newlines
@@ -863,7 +863,7 @@
                 self._encoding.lower() != "charset"):
                 try:
                     line = line.decode(self._encoding)
-                except UnicodeError, e:
+                except UnicodeError as e:
                     raise UnicodeError("Error decoding line with encoding %r: %s. Line is %r" %
                                        (self._encoding, e, line))
             newlines.append(line)
--- .\storage\qm.py	(original)
+++ .\storage\qm.py	(refactored)
@@ -68,14 +68,14 @@
 
 logger = logging.getLogger(__name__)
 
-QM_MAGIC_NUMBER = (0x3CB86418L, 0xCAEF9C95L, 0xCD211CBFL, 0x60A1BDDDL)
+QM_MAGIC_NUMBER = (0x3CB86418, 0xCAEF9C95, 0xCD211CBF, 0x60A1BDDD)
 
 
 def qmunpack(file_='messages.qm'):
     """Helper to unpack Qt .qm files into a Python string"""
     f = open(file_)
     s = f.read()
-    print "\\x%02x" * len(s) % tuple(map(ord, s))
+    print("\\x%02x" * len(s) % tuple(map(ord, s)))
     f.close()
 
 
@@ -125,7 +125,7 @@
         sectionheader = 5
 
         def section_debug(name, section_type, startsection, length):
-            print "Section: %s (type: %#x, offset: %#x, length: %d)" % (name, section_type, startsection, length)
+            print("Section: %s (type: %#x, offset: %#x, length: %d)" % (name, section_type, startsection, length))
             return
 
         while startsection < len(input):
@@ -181,7 +181,7 @@
                         target = multistring(string)
                     pos = pos + 4 + length
                 else:
-                    target = u""
+                    target = ""
                     pos = pos + 4
                 #print "Translation: %s" % target.encode('utf-8')
             elif subsection == 0x06:  # SourceText
--- .\storage\qph.py	(original)
+++ .\storage\qph.py	(refactored)
@@ -67,15 +67,15 @@
         def not_none(node):
             return not node is None
 
-        return filter(not_none, [self._getsourcenode(), self._gettargetnode()])
+        return list(filter(not_none, [self._getsourcenode(), self._gettargetnode()]))
 
-    @accepts(Self(), unicode, IsOneOf(String, type(None)), String)
+    @accepts(Self(), str, IsOneOf(String, type(None)), String)
     def addnote(self, text, origin=None, position="append"):
         """Add a note specifically in a "definition" tag"""
         current_notes = self.getnotes(origin)
         self.removenotes()
         note = etree.SubElement(self.xmlelement, self.namespaced("definition"))
-        note.text = "\n".join(filter(None, [current_notes, text.strip()]))
+        note.text = "\n".join([_f for _f in [current_notes, text.strip()] if _f])
 
     def getnotes(self, origin=None):
         #TODO: consider only responding when origin has certain values
--- .\storage\rc.py	(original)
+++ .\storage\rc.py	(refactored)
@@ -85,7 +85,7 @@
     def __str__(self):
         """Convert to a string. Double check that unicode is handled somehow here."""
         source = self.getoutput()
-        if isinstance(source, unicode):
+        if isinstance(source, str):
             return source.encode(getattr(self, "encoding", "UTF-8"))
         return source
 
--- .\storage\statistics.py	(original)
+++ .\storage\statistics.py	(refactored)
@@ -138,12 +138,12 @@
         #TODO: we don't handle checking plurals at all yet, as this is tricky...
         source = unit.source
         target = unit.target
-        if isinstance(source, str) and isinstance(target, unicode):
+        if isinstance(source, str) and isinstance(target, str):
             source = source.decode(getattr(unit, "encoding", "utf-8"))
         #TODO: decoding should not be done here
 #        checkresult = self.checker.run_filters(unit, source, target)
         checkresult = {}
-        for checkname, checkmessage in checkresult.iteritems():
+        for checkname, checkmessage in checkresult.items():
             classes.append("check-" + checkname)
         return classes
 
@@ -190,7 +190,7 @@
         classes = self.classifyunit(unit)
 #        if self.basefile.getsuggestions(item):
 #            classes.append("has-suggestion")
-        for classname, matchingitems in self.classification.items():
+        for classname, matchingitems in list(self.classification.items()):
             if (classname in classes) != (item in matchingitems):
                 if classname in classes:
                     self.classification[classname].append(item)
--- .\storage\statsdb.py	(original)
+++ .\storage\statsdb.py	(refactored)
@@ -31,7 +31,7 @@
 import re
 import sys
 import stat
-import thread
+import _thread
 import logging
 from UserDict import UserDict
 
@@ -40,6 +40,7 @@
 from translate.misc.multistring import multistring
 from translate.storage import factory
 from translate.storage.workflow import StateEnum
+import collections
 
 logger = logging.getLogger(__name__)
 
@@ -106,7 +107,7 @@
         if record_values == None:
             record_values = (0 for _i in record_keys)
         self.record_keys = record_keys
-        self.data = dict(zip(record_keys, record_values))
+        self.data = dict(list(zip(record_keys, record_values)))
         self._compute_derived_values = compute_derived_values
         self._compute_derived_values(self)
 
@@ -115,14 +116,14 @@
 
     def __add__(self, other):
         result = Record(self.record_keys)
-        for key in self.keys():
+        for key in list(self.keys()):
             result[key] = self[key] + other[key]
         self._compute_derived_values(self)
         return result
 
     def __sub__(self, other):
         result = Record(self.record_keys)
-        for key in self.keys():
+        for key in list(self.keys()):
             result[key] = self[key] - other[key]
         self._compute_derived_values(self)
         return result
@@ -286,7 +287,7 @@
     """The current cursor"""
 
     def __new__(cls, statsfile=None):
-        current_thread = thread.get_ident()
+        current_thread = _thread.get_ident()
 
         def make_database(statsfile):
 
@@ -328,7 +329,7 @@
                 if not os.path.exists(cachedir):
                     os.mkdir(cachedir)
                 cachedir = cachedir.decode(sys.getfilesystemencoding())
-                cls.defaultfile = os.path.realpath(os.path.join(cachedir, u"stats.db"))
+                cls.defaultfile = os.path.realpath(os.path.join(cachedir, "stats.db"))
             statsfile = cls.defaultfile
         else:
             statsfile = os.path.realpath(statsfile)
@@ -399,7 +400,7 @@
         store can be a TranslationFile object or a callback that returns one.
         """
         if isinstance(filename, str):
-            filename = unicode(filename, sys.getfilesystemencoding())
+            filename = str(filename, sys.getfilesystemencoding())
         realpath = os.path.realpath(filename)
         self.cur.execute("""SELECT fileid, st_mtime, st_size FROM files
                 WHERE path=?;""", (realpath,))
@@ -417,7 +418,7 @@
                 return fileid
 
         # file wasn't in db at all, lets recache it
-        if callable(store):
+        if isinstance(store, collections.Callable):
             store = store()
         else:
             store = store or factory.getobject(realpath)
@@ -517,7 +518,7 @@
                 if unitindex:
                     index = unitindex
                 failures = checker.run_filters(unit)
-                for checkname, checkmessage in failures.iteritems():
+                for checkname, checkmessage in failures.items():
                     unitvalues.append((index, fileid, configid, checkname, checkmessage))
                     errornames.append("check-" + checkname)
         checker.setsuggestionstore(None)
@@ -608,7 +609,7 @@
 
         # This could happen if we haven't done the checks before, or the
         # file changed, or we are using a different configuration
-        if callable(store):
+        if isinstance(store, collections.Callable):
             store = store()
         else:
             store = store or factory.getobject(filename)
--- .\storage\subtitles.py	(original)
+++ .\storage\subtitles.py	(refactored)
@@ -28,7 +28,7 @@
 """
 
 import os
-from StringIO import StringIO
+from io import StringIO
 import tempfile
 
 try:
@@ -115,7 +115,7 @@
                 newunit._start = subtitle.start
                 newunit._end = subtitle.end
                 newunit._duration = subtitle.duration_seconds
-        except Exception, e:
+        except Exception as e:
             raise base.ParseError(e)
 
     def _parsefile(self, storefile):
@@ -125,7 +125,7 @@
         elif hasattr(storefile, 'filename'):
             self.filename = storefile.filename
             storefile.close()
-        elif isinstance(storefile, basestring):
+        elif isinstance(storefile, str):
             self.filename = storefile
 
         if self.filename and os.path.exists(self.filename):
@@ -141,7 +141,7 @@
         return newstore
 
     def parse(self, input):
-        if isinstance(input, basestring):
+        if isinstance(input, str):
             # Gaupol does not allow parsing from strings
             if self.filename:
                 tmpfile, tmpfilename = tempfile.mkstemp(suffix=self.filename)
--- .\storage\symbian.py	(original)
+++ .\storage\symbian.py	(refactored)
@@ -35,7 +35,7 @@
     def __init__(self, f, charset, read_hook=identity):
         self.f = f
         self.charset = charset
-        self.current_line = u''
+        self.current_line = ''
         self.read_hook = read_hook
         self.read_line()
 
--- .\storage\test_aresource.py	(original)
+++ .\storage\test_aresource.py	(refactored)
@@ -40,8 +40,8 @@
         for string, xml in self.escape_data:
             unit = self.UnitClass("Test String")
             unit.target = string
-            print "unit.target:", repr(unit.target)
-            print "xml:", repr(xml)
+            print("unit.target:", repr(unit.target))
+            print("xml:", repr(xml))
             assert str(unit) == xml
 
     def test_parse(self):
@@ -55,9 +55,9 @@
             translatable = 'translatable="false"' not in xml
             et = etree.fromstring(xml, parser)
             unit = self.UnitClass.createfromxmlElement(et)
-            print "unit.target:", repr(unit.target)
-            print "string:", string
-            print 'translatable:', repr(unit.istranslatable())
+            print("unit.target:", repr(unit.target))
+            print("string:", string)
+            print('translatable:', repr(unit.istranslatable()))
             assert unit.target == string
             assert unit.istranslatable() == translatable
 
--- .\storage\test_base.py	(original)
+++ .\storage\test_base.py	(refactored)
@@ -32,7 +32,7 @@
 
 def headerless_len(units):
     """return count of translatable (non header) units"""
-    return len(filter(lambda x: not x.isheader(), units))
+    return len([x for x in units if not x.isheader()])
 
 
 def first_translatable(store):
@@ -86,7 +86,7 @@
     def test_create(self):
         """tests a simple creation with a source string"""
         unit = self.unit
-        print 'unit.source:', unit.source
+        print('unit.source:', unit.source)
         assert unit.source == "Test String"
 
     def test_eq(self):
@@ -125,12 +125,12 @@
         unit = self.unit
         specials = ['Fish & chips', 'five < six', 'six > five', 'five &lt; six',
                     'Use &nbsp;', 'Use &amp;nbsp;', 'Use &amp;amp;nbsp;',
--- .\storage\test_catkeys.py	(original)
+++ .\storage\test_catkeys.py	(refactored)
@@ -20,8 +20,8 @@
                     '\\\n', '\\\t', '\\\\r', '\\\\"']
         for special in specials:
             unit.source = special
-            print "unit.source:", repr(unit.source) + '|'
-            print "special:", repr(special) + '|'
+            print("unit.source:", repr(unit.source) + '|')
+            print("special:", repr(special) + '|')
             assert unit.source == special
 
     def test_newlines(self):
--- .\storage\test_cpo.py	(original)
+++ .\storage\test_cpo.py	(refactored)
@@ -30,11 +30,11 @@
         assert unit.target.strings == ["Koei", "Koeie"]
         assert unit.target == "Koei"
 
-        unit.target = [u"Sk\u00ear", u"Sk\u00eare"]
+        unit.target = ["Sk\u00ear", "Sk\u00eare"]
         assert isinstance(unit.target, multistring)
-        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
-        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
-        assert unit.target == u"Sk\u00ear"
+        assert unit.target.strings == ["Sk\u00ear", "Sk\u00eare"]
+        assert unit.target.strings == ["Sk\u00ear", "Sk\u00eare"]
+        assert unit.target == "Sk\u00ear"
 
     def test_plural_reduction(self):
         """checks that reducing the number of plurals supplied works"""
@@ -84,8 +84,8 @@
         pofile = self.poparse(posource)
         thepo = pofile.units[0]
         thepo.msgidcomment = "first comment"
-        print pofile
-        print "Blah", thepo.source
+        print(pofile)
+        print("Blah", thepo.source)
         assert thepo.source == "test me"
         thepo.msgidcomment = "second comment"
         assert str(pofile).count("_:") == 1
@@ -97,7 +97,7 @@
         pofile = self.poparse(posource)
         assert len(pofile.units) == 2
         pofile.removeduplicates("msgctxt")
-        print pofile
+        print(pofile)
         assert len(pofile.units) == 2
         assert str(pofile.units[0]).count("source1") == 2
         assert str(pofile.units[1]).count("source2") == 2
@@ -110,8 +110,8 @@
         assert len(pofile.units) == 2
         pofile.removeduplicates("merge")
         assert len(pofile.units) == 2
-        print pofile.units[0].msgidcomments
-        print pofile.units[1].msgidcomments
+        print(pofile.units[0].msgidcomments)
+        print(pofile.units[1].msgidcomments)
         assert po.unquotefrompo(pofile.units[0].msgidcomments) == "_: source1\n"
         assert po.unquotefrompo(pofile.units[1].msgidcomments) == "_: source2\n"
 
@@ -144,14 +144,14 @@
 
     def test_output_str_unicode(self):
         """checks that we can str(pofile) which is in unicode"""
-        posource = u'''#: nb\nmsgid "Norwegian Bokm\xe5l"\nmsgstr ""\n'''
+        posource = '''#: nb\nmsgid "Norwegian Bokm\xe5l"\nmsgstr ""\n'''
         pofile = self.StoreClass(wStringIO.StringIO(posource.encode("UTF-8")), encoding="UTF-8")
         assert len(pofile.units) == 1
-        print str(pofile)
+        print(str(pofile))
         thepo = pofile.units[0]
 #        assert str(pofile) == posource.encode("UTF-8")
         # extra test: what if we set the msgid to a unicode? this happens in prop2po etc
-        thepo.source = u"Norwegian Bokm\xe5l"
+        thepo.source = "Norwegian Bokm\xe5l"
 #        assert str(thepo) == posource.encode("UTF-8")
         # Now if we set the msgstr to Unicode
         # this is an escaped half character (1/2)
@@ -165,7 +165,7 @@
         """checks the content of all the expected sections of a PO message"""
         posource = '# other comment\n#. automatic comment\n#: source comment\n#, fuzzy\nmsgid "One"\nmsgstr "Een"\n'
         pofile = self.poparse(posource)
-        print pofile
+        print(pofile)
         assert len(pofile.units) == 1
         assert str(pofile) == posource
 
@@ -173,8 +173,8 @@
         """Tests for correct output of mulitline obsolete messages"""
         posource = '#~ msgid ""\n#~ "Old thing\\n"\n#~ "Second old thing"\n#~ msgstr ""\n#~ "Ou ding\\n"\n#~ "Tweede ou ding"\n'
         pofile = self.poparse(posource)
-        print "Source:\n%s" % posource
-        print "Output:\n%s" % str(pofile)
+        print("Source:\n%s" % posource)
+        print("Output:\n%s" % str(pofile))
         assert len(pofile.units) == 1
         assert pofile.units[0].isobsolete()
         assert not pofile.units[0].istranslatable()
@@ -184,6 +184,6 @@
         """tests behaviour of unassociated comments."""
         oldsource = '# old lonesome comment\n\nmsgid "one"\nmsgstr "een"\n'
         oldfile = self.poparse(oldsource)
-        print "__str__", str(oldfile)
+        print("__str__", str(oldfile))
         assert len(oldfile.units) == 1
         assert str(oldfile).find("# old lonesome comment\nmsgid") >= 0
--- .\storage\test_directory.py	(original)
+++ .\storage\test_directory.py	(refactored)
@@ -12,7 +12,7 @@
 
     def setup_method(self, method):
         """sets up a test directory"""
-        print "setup_method called on", self.__class__.__name__
+        print("setup_method called on", self.__class__.__name__)
         self.testdir = "%s_testdir" % (self.__class__.__name__)
         self.cleardir(self.testdir)
         os.mkdir(self.testdir)
@@ -46,7 +46,7 @@
 
     def test_created(self):
         """test that the directory actually exists"""
-        print self.testdir
+        print(self.testdir)
         assert os.path.isdir(self.testdir)
 
     def test_basic(self):
--- .\storage\test_dtd.py	(original)
+++ .\storage\test_dtd.py	(refactored)
@@ -62,9 +62,9 @@
     for special in specials:
         quoted_special = dtd.quotefordtd(special)
         unquoted_special = dtd.unquotefromdtd(quoted_special)
-        print "special: %r\nquoted: %r\nunquoted: %r\n" % (special,
+        print("special: %r\nquoted: %r\nunquoted: %r\n" % (special,
                                                            quoted_special,
-                                                           unquoted_special)
+                                                           unquoted_special))
         assert special == unquoted_special
 
 
@@ -127,7 +127,7 @@
     assert dtd.unquotefromdtd('"A &#x0022;thing&#x0022;"') == "A \"thing\""
     assert dtd.unquotefromdtd("'<a href=\"http'") == "<a href=\"http"
     # other chars
--- .\storage\test_html.py	(original)
+++ .\storage\test_html.py	(refactored)
@@ -81,5 +81,5 @@
         interpretted as tags"""
         h = html.htmlfile()
         store = h.parsestring("<p>We are here</p><script>Some </tag>like data<script></p>")
-        print store.units[0].source
+        print(store.units[0].source)
         assert len(store.units) == 1
--- .\storage\test_mo.py	(original)
+++ .\storage\test_mo.py	(refactored)
@@ -2,7 +2,7 @@
 
 import os
 import sys
-import StringIO
+import io
 import subprocess
 
 from translate.storage import factory
@@ -139,8 +139,8 @@
 
     def test_output(self):
         for posource in posources:
-            print "PO source file"
-            print posource
+            print("PO source file")
+            print(posource)
             PO_FILE, MO_MSGFMT, MO_POCOMPILE = self.get_mo_and_po()
 
             out_file = open(PO_FILE, 'w')
@@ -150,7 +150,7 @@
             subprocess.call(['msgfmt', PO_FILE, '-o', MO_MSGFMT])
             subprocess.call(['pocompile', '--errorlevel=traceback', PO_FILE, MO_POCOMPILE])
 
-            store = factory.getobject(StringIO.StringIO(posource))
+            store = factory.getobject(io.StringIO(posource))
             if store.isempty() and not os.path.exists(MO_POCOMPILE):
                 # pocompile doesn't create MO files for empty PO files, so we
                 # can skip the checks here.
@@ -161,11 +161,11 @@
 
             try:
                 mo_msgfmt = mo_msgfmt_f.read()
-                print "msgfmt output:"
-                print repr(mo_msgfmt)
+                print("msgfmt output:")
+                print(repr(mo_msgfmt))
                 mo_pocompile = mo_pocompile_f.read()
-                print "pocompile output:"
-                print repr(mo_pocompile)
+                print("pocompile output:")
+                print(repr(mo_pocompile))
                 assert mo_msgfmt == mo_pocompile
             finally:
                 mo_msgfmt_f.close()
--- .\storage\test_monolingual.py	(original)
+++ .\storage\test_monolingual.py	(refactored)
@@ -41,11 +41,11 @@
             store2unit = store2.units[n]
             match = str(store1unit) == str(store2unit)
             if not match:
-                print "match failed between elements %d of %d" % ((n + 1), len(store1.units))
-                print "store1:"
-                print str(store1)
-                print "store2:"
-                print str(store2)
-                print "store1.units[%d].__dict__:" % n, store1unit.__dict__
-                print "store2.units[%d].__dict__:" % n, store2unit.__dict__
+                print("match failed between elements %d of %d" % ((n + 1), len(store1.units)))
+                print("store1:")
+                print(str(store1))
+                print("store2:")
+                print(str(store2))
+                print("store1.units[%d].__dict__:" % n, store1unit.__dict__)
+                print("store2.units[%d].__dict__:" % n, store2unit.__dict__)
                 assert str(store1unit) == str(store2unit)
--- .\storage\test_oo.py	(original)
+++ .\storage\test_oo.py	(refactored)
@@ -52,7 +52,7 @@
         oofile = self.ooparse(oosource)
         assert len(oofile.units) == 1
         oe = oofile.units[0]
-        assert oe.languages.keys() == ["en-US"]
+        assert list(oe.languages.keys()) == ["en-US"]
         ol = oofile.oolines[0]
         assert ol.getkey() == ('svx', r'source\dialog\numpages.src', 'string', 'RID_SVXPAGE_NUM_OPTIONS', 'STR_BULLET', '')
         assert ol.text == 'Character'
@@ -64,7 +64,7 @@
         oofile = self.ooparse(oosource)
         assert len(oofile.units) == 1
         oe = oofile.units[0]
-        assert oe.languages.keys() == ["en-US"]
+        assert list(oe.languages.keys()) == ["en-US"]
         ol = oofile.oolines[0]
         assert ol.getkey() == ('sd', r'source\ui\dlg\sdobjpal.src', 'imagebutton', 'FLTWIN_SDOBJPALETTE', 'BTN_SYMSIZE', '')
         assert ol.quickhelptext == 'Toggle Symbol Size'
@@ -76,7 +76,7 @@
         oofile = self.ooparse(oosource)
         assert len(oofile.units) == 1
         oe = oofile.units[0]
-        assert oe.languages.keys() == ["en-US"]
+        assert list(oe.languages.keys()) == ["en-US"]
         ol = oofile.oolines[0]
         assert ol.getkey() == ('dbaccess', r'source\ui\dlg\indexdialog.src', 'querybox', 'QUERY_SAVE_CURRENT_INDEX', '', '')
         assert ol.title == 'Exit Index Design'
@@ -96,7 +96,7 @@
         oofile = self.ooparse(oosource)
         assert len(oofile.units) == 1
         oe = oofile.units[0]
-        assert oe.languages.keys() == ["en-US"]
+        assert list(oe.languages.keys()) == ["en-US"]
         ol = oofile.oolines[0]
         assert int(ol.width) == 16
 
--- .\storage\test_php.py	(original)
+++ .\storage\test_php.py	(refactored)
@@ -218,7 +218,7 @@
          'item 3' => 'value3',
       );'''
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 2
         phpunit = phpfile.units[1]
         assert phpunit.name == "$lang->'item 3'"
@@ -229,7 +229,7 @@
         phpsource = """define("_FINISH", "Rematar");
 define('_POSTEDON', 'Enviado o');"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 2
         phpunit = phpfile.units[0]
         assert phpunit.name == 'define("_FINISH"'
@@ -243,7 +243,7 @@
         phpsource = """define( "_FINISH", "Rematar");
 define( '_CM_POSTED', 'Enviado');"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 2
         phpunit = phpfile.units[0]
         assert phpunit.name == 'define( "_FINISH"'
@@ -257,7 +257,7 @@
         phpsource = """define("_RELOAD",       "Recargar");
 define('_CM_POSTED',    'Enviado');"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 2
         phpunit = phpfile.units[0]
         assert phpunit.name == 'define("_RELOAD"'
@@ -273,7 +273,7 @@
         phpsource = """define( "_FINISH",       "Rematar");
 define(  '_UPGRADE_CHARSET',    'Upgrade charset');"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 2
         phpunit = phpfile.units[0]
         assert phpunit.name == 'define( "_FINISH"'
@@ -287,7 +287,7 @@
         phpsource = """define("_POSTEDON","Enviado o");
 define('_UPGRADE_CHARSET','Upgrade charset');"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 2
         phpunit = phpfile.units[0]
         assert phpunit.name == 'define("_POSTEDON"'
@@ -304,7 +304,7 @@
         phpsource = """define( "_FINISH","Rematar");
 define( '_CM_POSTED','Enviado');"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 2
         phpunit = phpfile.units[0]
         assert phpunit.name == 'define( "_FINISH"'
@@ -319,7 +319,7 @@
 define('_YOUR_USERNAME', 'O seu nome de usuario: "cookie"');
 define("_REGISTER", "Register <a href=\"register.php\">here</a>");"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 3
         phpunit = phpfile.units[0]
         assert phpunit.name == "define('_SETTINGS_COOKIEPREFIX'"
@@ -336,7 +336,7 @@
         phpsource = """define("_POSTEDON", "Enviado o");// Keep this short
 define('_CM_POSTED', 'Enviado'); // Posted date"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 2
         phpunit = phpfile.units[0]
         assert phpunit.name == 'define("_POSTEDON"'
@@ -356,7 +356,7 @@
 // It appears besides posts
 define('_CM_POSTED', 'Enviado');"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 2
         phpunit = phpfile.units[0]
         assert phpunit.name == 'define("_FINISH"'
@@ -375,7 +375,7 @@
 define("_FINISH", "Rematar"     );
 define("_RELOAD", "Recargar");"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 3
         phpunit = phpfile.units[0]
         assert phpunit.name == 'define("_POSTEDON"'
@@ -394,7 +394,7 @@
 $month_feb = 'Feb'  ;
 $month_mar = 'Mar';"""
         phpfile = self.phpparse(phpsource)
-        print len(phpfile.units)
+        print(len(phpfile.units))
         assert len(phpfile.units) == 3
         phpunit = phpfile.units[0]
         assert phpunit.name == '$month_jan'
--- .\storage\test_po.py	(original)
+++ .\storage\test_po.py	(refactored)
@@ -19,7 +19,7 @@
     for special in specials:
         quoted_special = pypo.quoteforpo(special)
         unquoted_special = pypo.unquotefrompo(quoted_special)
-        print "special: %r\nquoted: %r\nunquoted: %r\n" % (special, quoted_special, unquoted_special)
+        print("special: %r\nquoted: %r\nunquoted: %r\n" % (special, quoted_special, unquoted_special))
         assert special == unquoted_special
 
 
@@ -58,18 +58,18 @@
         locations_helper("key")
         locations_helper("file.c:100")
         locations_helper("I am a key")
--- .\storage\test_pocommon.py	(original)
+++ .\storage\test_pocommon.py	(refactored)
@@ -11,6 +11,6 @@
         assert quote == quoted
         unquote = pocommon.unquote_plus(quoted)
         assert unquote == text
-    roundtrip_quote_plus(u"abc", u"abc")
-    roundtrip_quote_plus(u"key space", u"key+space")
--- .\storage\test_poheader.py	(original)
+++ .\storage\test_poheader.py	(refactored)
@@ -20,7 +20,7 @@
 item3: three
 '''
     d = poheader.parseheaderstring(source)
-    print type(d)
+    print(type(d))
     assert type(d) == ordereddict
     assert len(d) == 3
     assert d['item1'] == 'one'
@@ -49,10 +49,10 @@
     d['Project-Id-Version'] = 'abc'
     d['POT-Creation-Date'] = 'now'
     d = poheader.update(d, add=True, Test='hello', Report_Msgid_Bugs_To='bugs@list.org')
-    assert d.keys()[0] == "Project-Id-Version"
-    assert d.keys()[1] == "Report-Msgid-Bugs-To"
-    assert d.keys()[2] == "POT-Creation-Date"
-    assert d.keys()[3] == "Test"
+    assert list(d.keys())[0] == "Project-Id-Version"
+    assert list(d.keys())[1] == "Report-Msgid-Bugs-To"
+    assert list(d.keys())[2] == "POT-Creation-Date"
+    assert list(d.keys())[3] == "Test"
 
 
 def poparse(posource):
@@ -144,7 +144,7 @@
 def test_header_blank():
 
     def compare(pofile):
-        print pofile
+        print(pofile)
         assert len(pofile.units) == 1
         header = pofile.header()
         assert header.isheader()
@@ -220,7 +220,7 @@
 '''
     for colon in ("", ";"):
         pofile = poparse(posource % colon)
-        print pofile
+        print(pofile)
         assert len(pofile.units) == 1
         header = pofile.units[0]
         assert header.isheader()
@@ -241,7 +241,7 @@
 "10<=4 && (n%100<10 || n%100>=20) ? 1 : 2);\n"
 '''
     pofile = poparse(posource)
-    print pofile
+    print(pofile)
     assert len(pofile.units) == 1
     header = pofile.units[0]
     assert header.isheader()
@@ -269,7 +269,7 @@
 
     pofile.header().addnote("Khaled Hosny <khaledhosny@domain.org>, 2006, 2007, 2008.")
     pofile.updatecontributor("Khaled Hosny", "khaledhosny@domain.org")
-    print str(pofile)
+    print(str(pofile))
     assert "# Khaled Hosny <khaledhosny@domain.org>, 2006, 2007, 2008, %s." % time.strftime("%Y") in str(pofile)
 
 
--- .\storage\test_poxliff.py	(original)
+++ .\storage\test_poxliff.py	(refactored)
@@ -11,8 +11,8 @@
     def test_plurals(self):
         """Tests that plurals are handled correctly."""
         unit = self.UnitClass(multistring(["Cow", "Cows"]))
-        print type(unit.source)
-        print repr(unit.source)
+        print(type(unit.source))
+        print(repr(unit.source))
         assert isinstance(unit.source, multistring)
         assert unit.source.strings == ["Cow", "Cows"]
         assert unit.source == "Cow"
@@ -22,11 +22,11 @@
         assert unit.target.strings == ["Koei", "Koeie"]
         assert unit.target == "Koei"
 
-        unit.target = [u"Sk\u00ear", u"Sk\u00eare"]
+        unit.target = ["Sk\u00ear", "Sk\u00eare"]
         assert isinstance(unit.target, multistring)
-        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
-        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
-        assert unit.target == u"Sk\u00ear"
+        assert unit.target.strings == ["Sk\u00ear", "Sk\u00eare"]
+        assert unit.target.strings == ["Sk\u00ear", "Sk\u00eare"]
+        assert unit.target == "Sk\u00ear"
 
     def test_ids(self):
         """Tests that ids are assigned correctly, especially for plurals"""
--- .\storage\test_properties.py	(original)
+++ .\storage\test_properties.py	(refactored)
@@ -10,65 +10,65 @@
 
 def test_find_delimiter_pos_simple():
     """Simple tests to find the various delimiters"""
-    assert properties._find_delimiter(u"key=value", [u"=", u":", u" "]) == ('=', 3)
-    assert properties._find_delimiter(u"key:value", [u"=", u":", u" "]) == (':', 3)
-    assert properties._find_delimiter(u"key value", [u"=", u":", u" "]) == (' ', 3)
+    assert properties._find_delimiter("key=value", ["=", ":", " "]) == ('=', 3)
+    assert properties._find_delimiter("key:value", ["=", ":", " "]) == (':', 3)
+    assert properties._find_delimiter("key value", ["=", ":", " "]) == (' ', 3)
     # NOTE this is valid in Java properties, the key is then the empty string
-    assert properties._find_delimiter(u"= value", [u"=", u":", u" "]) == ('=', 0)
+    assert properties._find_delimiter("= value", ["=", ":", " "]) == ('=', 0)
 
 
 def test_find_delimiter_pos_multiple():
     """Find delimiters when multiple potential delimiters are involved"""
-    assert properties._find_delimiter(u"key=value:value", [u"=", u":", u" "]) == ('=', 3)
-    assert properties._find_delimiter(u"key:value=value", [u"=", u":", u" "]) == (':', 3)
-    assert properties._find_delimiter(u"key value=value", [u"=", u":", u" "]) == (' ', 3)
+    assert properties._find_delimiter("key=value:value", ["=", ":", " "]) == ('=', 3)
+    assert properties._find_delimiter("key:value=value", ["=", ":", " "]) == (':', 3)
+    assert properties._find_delimiter("key value=value", ["=", ":", " "]) == (' ', 3)
 
 
 def test_find_delimiter_pos_none():
     """Find delimiters when there isn't one"""
-    assert properties._find_delimiter(u"key", [u"=", u":", u" "]) == (None, -1)
-    assert properties._find_delimiter(u"key\=\:\ ", [u"=", u":", u" "]) == (None, -1)
+    assert properties._find_delimiter("key", ["=", ":", " "]) == (None, -1)
+    assert properties._find_delimiter("key\=\:\ ", ["=", ":", " "]) == (None, -1)
 
 
 def test_find_delimiter_pos_whitespace():
     """Find delimiters when whitespace is involved"""
-    assert properties._find_delimiter(u"key = value", [u"=", u":", u" "]) == ('=', 4)
-    assert properties._find_delimiter(u"key : value", [u"=", u":", u" "]) == (':', 4)
-    assert properties._find_delimiter(u"key   value", [u"=", u":", u" "]) == (' ', 3)
-    assert properties._find_delimiter(u"key value = value", [u"=", u":", u" "]) == (' ', 3)
-    assert properties._find_delimiter(u"key value value", [u"=", u":", u" "]) == (' ', 3)
-    assert properties._find_delimiter(u" key = value", [u"=", u":", u" "]) == ('=', 5)
+    assert properties._find_delimiter("key = value", ["=", ":", " "]) == ('=', 4)
+    assert properties._find_delimiter("key : value", ["=", ":", " "]) == (':', 4)
+    assert properties._find_delimiter("key   value", ["=", ":", " "]) == (' ', 3)
+    assert properties._find_delimiter("key value = value", ["=", ":", " "]) == (' ', 3)
+    assert properties._find_delimiter("key value value", ["=", ":", " "]) == (' ', 3)
+    assert properties._find_delimiter(" key = value", ["=", ":", " "]) == ('=', 5)
 
 
 def test_find_delimiter_pos_escapes():
     """Find delimiters when potential earlier delimiters are escaped"""
-    assert properties._find_delimiter(u"key\:=value", [u"=", u":", u" "]) == ('=', 5)
-    assert properties._find_delimiter(u"key\=: value", [u"=", u":", u" "]) == (':', 5)
-    assert properties._find_delimiter(u"key\   value", [u"=", u":", u" "]) == (' ', 5)
-    assert properties._find_delimiter(u"key\ key\ key\: = value", [u"=", u":", u" "]) == ('=', 16)
+    assert properties._find_delimiter("key\:=value", ["=", ":", " "]) == ('=', 5)
+    assert properties._find_delimiter("key\=: value", ["=", ":", " "]) == (':', 5)
+    assert properties._find_delimiter("key\   value", ["=", ":", " "]) == (' ', 5)
+    assert properties._find_delimiter("key\ key\ key\: = value", ["=", ":", " "]) == ('=', 16)
 
 
 def test_find_delimiter_deprecated_fn():
     """Test that the deprecated function still actually works"""
-    assert properties.find_delimeter(u"key=value") == ('=', 3)
-    deprecated_call(properties.find_delimeter, u"key=value")
+    assert properties.find_delimeter("key=value") == ('=', 3)
+    deprecated_call(properties.find_delimeter, "key=value")
 
 
 def test_is_line_continuation():
-    assert properties.is_line_continuation(u"") == False
-    assert properties.is_line_continuation(u"some text") == False
-    assert properties.is_line_continuation(u"""some text\\""") == True
-    assert properties.is_line_continuation(u"""some text\\\\""") == False  # Escaped \
-    assert properties.is_line_continuation(u"""some text\\\\\\""") == True  # Odd num. \ is line continuation
-    assert properties.is_line_continuation(u"""\\\\\\""") == True
+    assert properties.is_line_continuation("") == False
+    assert properties.is_line_continuation("some text") == False
+    assert properties.is_line_continuation("""some text\\""") == True
+    assert properties.is_line_continuation("""some text\\\\""") == False  # Escaped \
+    assert properties.is_line_continuation("""some text\\\\\\""") == True  # Odd num. \ is line continuation
+    assert properties.is_line_continuation("""\\\\\\""") == True
 
 
 def test_key_strip():
-    assert properties._key_strip(u"key") == "key"
-    assert properties._key_strip(u" key") == "key"
-    assert properties._key_strip(u"\ key") == "\ key"
-    assert properties._key_strip(u"key ") == "key"
-    assert properties._key_strip(u"key\ ") == "key\ "
+    assert properties._key_strip("key") == "key"
+    assert properties._key_strip(" key") == "key"
+    assert properties._key_strip("\ key") == "\ key"
+    assert properties._key_strip("key ") == "key"
+    assert properties._key_strip("key\ ") == "key\ "
 
 
 def test_is_comment_one_line():
@@ -132,7 +132,7 @@
     def test_unicode_escaping(self):
         """check that escaped unicode is converted properly"""
         propsource = "unicode=\u0411\u0416\u0419\u0428"
-        messagevalue = u'\u0411\u0416\u0419\u0428'.encode("UTF-8")
+        messagevalue = '\u0411\u0416\u0419\u0428'.encode("UTF-8")
         propfile = self.propparse(propsource, personality="mozilla")
         assert len(propfile.units) == 1
         propunit = propfile.units[0]
@@ -158,7 +158,7 @@
         for propsource, key, value in whitespaces:
             propfile = self.propparse(propsource)
             propunit = propfile.units[0]
-            print repr(propsource), repr(propunit.name), repr(propunit.source)
+            print(repr(propsource), repr(propunit.name), repr(propunit.source))
             assert propunit.name == key
             assert propunit.source == value
             # let's reparse the output to ensure good serialisation->parsing roundtrip:
@@ -173,7 +173,7 @@
         delimiters = [":", "=", " "]
         for delimiter in delimiters:
             propsource = "key%svalue" % delimiter
-            print "source: '%s'\ndelimiter: '%s'" % (propsource, delimiter)
+            print("source: '%s'\ndelimiter: '%s'" % (propsource, delimiter))
             propfile = self.propparse(propsource)
             assert len(propfile.units) == 1
             propunit = propfile.units[0]
@@ -188,19 +188,19 @@
 key=value
 ''' % comment_marker
             propfile = self.propparse(propsource)
-            print repr(propsource)
-            print "Comment marker: '%s'" % comment_marker
+            print(repr(propsource))
+            print("Comment marker: '%s'" % comment_marker)
             assert len(propfile.units) == 1
             propunit = propfile.units[0]
             assert propunit.comments == ['%s A comment' % comment_marker]
 
     def test_latin1(self):
         """checks that we handle non-escaped latin1 text"""
-        prop_source = u"key=val".encode('latin1')
+        prop_source = "key=val".encode('latin1')
         prop_store = self.propparse(prop_source)
         assert len(prop_store.units) == 1
         unit = prop_store.units[0]
-        assert unit.source == u"val"
+        assert unit.source == "val"
 
     def test_fullspec_delimiters(self):
         """test the full definiation as found in Java docs"""
@@ -208,119 +208,119 @@
         for propsource in proplist:
             propfile = self.propparse(propsource)
             propunit = propfile.units[0]
-            print propunit
+            print(propunit)
             assert propunit.name == "Truth"
             assert propunit.source == "Beauty"
 
     def test_fullspec_escaped_key(self):
         """Escaped delimeters can be in the key"""
-        prop_source = u"\:\="
+        prop_source = "\:\="
         prop_store = self.propparse(prop_source)
         assert len(prop_store.units) == 1
         unit = prop_store.units[0]
-        print unit
-        assert unit.name == u"\:\="
+        print(unit)
+        assert unit.name == "\:\="
 
     def test_fullspec_line_continuation(self):
         """Whitespace delimiter and pre whitespace in line continuation are dropped"""
-        prop_source = ur"""fruits                           apple, banana, pear, \
+        prop_source = r"""fruits                           apple, banana, pear, \
                                   cantaloupe, watermelon, \
                                   kiwi, mango
 """
         prop_store = self.propparse(prop_source)
-        print prop_store
+        print(prop_store)
         assert len(prop_store.units) == 1
         unit = prop_store.units[0]
-        print unit
-        assert properties._find_delimiter(prop_source, [u"=", u":", u" "]) == (' ', 6)
-        assert unit.name == u"fruits"
-        assert unit.source == u"apple, banana, pear, cantaloupe, watermelon, kiwi, mango"
+        print(unit)
+        assert properties._find_delimiter(prop_source, ["=", ":", " "]) == (' ', 6)
+        assert unit.name == "fruits"
+        assert unit.source == "apple, banana, pear, cantaloupe, watermelon, kiwi, mango"
 
     def test_fullspec_key_without_value(self):
         """A key can have no value in which case the value is the empty string"""
-        prop_source = u"cheeses"
+        prop_source = "cheeses"
         prop_store = self.propparse(prop_source)
         assert len(prop_store.units) == 1
         unit = prop_store.units[0]
-        print unit
-        assert unit.name == u"cheeses"
-        assert unit.source == u""
+        print(unit)
+        assert unit.name == "cheeses"
+        assert unit.source == ""
 
     def test_mac_strings(self):
         """test various items used in Mac OS X strings files"""
-        propsource = ur'''"I am a \"key\"" = "I am a \"value\"";'''.encode('utf-16')
-        propfile = self.propparse(propsource, personality="strings")
-        assert len(propfile.units) == 1
-        propunit = propfile.units[0]
-        assert propunit.name == ur'I am a "key"'
-        assert propunit.source.encode('utf-8') == u'I am a "value"'
+        propsource = r'''"I am a \"key\"" = "I am a \"value\"";'''.encode('utf-16')
+        propfile = self.propparse(propsource, personality="strings")
+        assert len(propfile.units) == 1
+        propunit = propfile.units[0]
+        assert propunit.name == r'I am a "key"'
+        assert propunit.source.encode('utf-8') == 'I am a "value"'
 
     def test_mac_strings_unicode(self):
         """Ensure we can handle Unicode"""
-        propsource = ur'''"I am a key" = "I am a value";'''.encode('utf-16')
-        propfile = self.propparse(propsource, personality="strings")
-        assert len(propfile.units) == 1
-        propunit = propfile.units[0]
-        assert propunit.name == ur'I am a key'
-        assert propfile.personality.encode(propunit.source) == u'I am a value'
+        propsource = r'''"I am a key" = "I am a value";'''.encode('utf-16')
+        propfile = self.propparse(propsource, personality="strings")
+        assert len(propfile.units) == 1
+        propunit = propfile.units[0]
+        assert propunit.name == r'I am a key'
+        assert propfile.personality.encode(propunit.source) == 'I am a value'
 
     def test_mac_strings_newlines(self):
         """test newlines \n within a strings files"""
-        propsource = ur'''"key" = "value\nvalue";'''.encode('utf-16')
-        propfile = self.propparse(propsource, personality="strings")
-        assert len(propfile.units) == 1
-        propunit = propfile.units[0]
-        assert propunit.name == u'key'
-        assert propunit.source.encode('utf-8') == u'value\nvalue'
-        assert propfile.personality.encode(propunit.source) == ur'value\nvalue'
+        propsource = r'''"key" = "value\nvalue";'''.encode('utf-16')
+        propfile = self.propparse(propsource, personality="strings")
+        assert len(propfile.units) == 1
+        propunit = propfile.units[0]
+        assert propunit.name == 'key'
+        assert propunit.source.encode('utf-8') == 'value\nvalue'
+        assert propfile.personality.encode(propunit.source) == r'value\nvalue'
 
     def test_mac_strings_comments(self):
         """test .string comment types"""
-        propsource = ur'''/* Comment */
+        propsource = r'''/* Comment */
 // Comment
 "key" = "value";'''.encode('utf-16')
         propfile = self.propparse(propsource, personality="strings")
         assert len(propfile.units) == 1
         propunit = propfile.units[0]
-        assert propunit.name == u'key'
-        assert propunit.source.encode('utf-8') == u'value'
-        assert propunit.getnotes() == u"/* Comment */\n// Comment"
+        assert propunit.name == 'key'
+        assert propunit.source.encode('utf-8') == 'value'
+        assert propunit.getnotes() == "/* Comment */\n// Comment"
 
     def test_mac_strings_multilines_comments(self):
         """test .string multiline comments"""
-        propsource = (u'/* Foo\n'
-                      u'Bar\n'
-                      u'Baz */\n'
-                      u'"key" = "value"').encode('utf-16')
-        propfile = self.propparse(propsource, personality="strings")
-        assert len(propfile.units) == 1
-        propunit = propfile.units[0]
-        assert propunit.name == u'key'
-        assert propunit.source.encode('utf-8') == u'value'
-        assert propunit.getnotes() == u"/* Foo\nBar\nBaz */"
+        propsource = ('/* Foo\n'
+                      'Bar\n'
+                      'Baz */\n'
+                      '"key" = "value"').encode('utf-16')
+        propfile = self.propparse(propsource, personality="strings")
+        assert len(propfile.units) == 1
+        propunit = propfile.units[0]
+        assert propunit.name == 'key'
+        assert propunit.source.encode('utf-8') == 'value'
+        assert propunit.getnotes() == "/* Foo\nBar\nBaz */"
 
     def test_mac_strings_comments_dropping(self):
         """.string generic (and unuseful) comments should be dropped"""
-        propsource = ur'''/* No comment provided by engineer. */
+        propsource = r'''/* No comment provided by engineer. */
 "key" = "value";'''.encode('utf-16')
         propfile = self.propparse(propsource, personality="strings")
         assert len(propfile.units) == 1
         propunit = propfile.units[0]
-        assert propunit.name == u'key'
-        assert propunit.source.encode('utf-8') == u'value'
-        assert propunit.getnotes() == u""
+        assert propunit.name == 'key'
+        assert propunit.source.encode('utf-8') == 'value'
+        assert propunit.getnotes() == ""
 
     def test_mac_strings_quotes(self):
         """test that parser unescapes characters used as wrappers"""
-        propsource = ur'"key with \"quotes\"" = "value with \"quotes\"";'.encode('utf-16')
-        propfile = self.propparse(propsource, personality="strings")
-        propunit = propfile.units[0]
-        assert propunit.name == ur'key with "quotes"'
-        assert propunit.value == ur'value with "quotes"'
+        propsource = r'"key with \"quotes\"" = "value with \"quotes\"";'.encode('utf-16')
+        propfile = self.propparse(propsource, personality="strings")
+        propunit = propfile.units[0]
+        assert propunit.name == r'key with "quotes"'
+        assert propunit.value == r'value with "quotes"'
 
     def test_mac_strings_serialization(self):
         """test that serializer quotes mac strings properly"""
-        propsource = ur'"key with \"quotes\"" = "value with \"quotes\"";'.encode('utf-16')
+        propsource = r'"key with \"quotes\"" = "value with \"quotes\"";'.encode('utf-16')
         propfile = self.propparse(propsource, personality="strings")
         # we don't care about leading and trailing newlines and zero bytes
         # in the assert, we just want to make sure that
@@ -333,26 +333,26 @@
 
     def test_override_encoding(self):
         """test that we can override the encoding of a properties file"""
-        propsource = u"key = value".encode("cp1252")
+        propsource = "key = value".encode("cp1252")
         propfile = self.propparse(propsource, personality="strings", encoding="cp1252")
         assert len(propfile.units) == 1
         propunit = propfile.units[0]
-        assert propunit.name == u'key'
-        assert propunit.source == u'value'
+        assert propunit.name == 'key'
+        assert propunit.source == 'value'
 
     def test_trailing_comments(self):
         """test that we handle non-unit data at the end of a file"""
-        propsource = u"key = value\n# END"
+        propsource = "key = value\n# END"
         propfile = self.propparse(propsource)
         assert len(propfile.units) == 2
         propunit = propfile.units[1]
-        assert propunit.name == u''
-        assert propunit.source == u''
-        assert propunit.getnotes() == u"# END"
+        assert propunit.name == ''
+        assert propunit.source == ''
+        assert propunit.getnotes() == "# END"
 
     def test_utf16_byte_order_mark(self):
         """test that BOM appears in the resulting text once only"""
-        propsource = u"key1 = value1\nkey2 = value2\n".encode('utf-16')
+        propsource = "key1 = value1\nkey2 = value2\n".encode('utf-16')
         propfile = self.propparse(propsource, encoding='utf-16')
         result = str(propfile)
         bom = propsource[:2]
--- .\storage\test_pypo.py	(original)
+++ .\storage\test_pypo.py	(refactored)
@@ -74,11 +74,11 @@
         assert unit.target.strings == ["Koei", "Koeie"]
         assert unit.target == "Koei"
 
-        unit.target = [u"Sk\u00ear", u"Sk\u00eare"]
+        unit.target = ["Sk\u00ear", "Sk\u00eare"]
         assert isinstance(unit.target, multistring)
-        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
-        assert unit.target.strings == [u"Sk\u00ear", u"Sk\u00eare"]
-        assert unit.target == u"Sk\u00ear"
+        assert unit.target.strings == ["Sk\u00ear", "Sk\u00eare"]
+        assert unit.target.strings == ["Sk\u00ear", "Sk\u00eare"]
+        assert unit.target == "Sk\u00ear"
 
     def test_plural_reduction(self):
         """checks that reducing the number of plurals supplied works"""
@@ -127,13 +127,13 @@
         str_max = "123456789 123456789 123456789 123456789 123456789 123456789 123456789 1"
         unit = self.UnitClass(str_max)
         expected = 'msgid "%s"\nmsgstr ""\n' % str_max
-        print expected, str(unit)
+        print(expected, str(unit))
         assert str(unit) == expected
         # at this length we wrap
         str_wrap = str_max + '2'
         unit = self.UnitClass(str_wrap)
         expected = 'msgid ""\n"%s"\nmsgstr ""\n' % str_wrap
-        print expected, str(unit)
+        print(expected, str(unit))
         assert str(unit) == expected
 
     def test_wrap_on_newlines(self):
@@ -142,7 +142,7 @@
         postring = ('"123456789\\n"\n' * 3)[:-1]
         unit = self.UnitClass(string)
         expected = 'msgid ""\n%s\nmsgstr ""\n' % postring
-        print expected, str(unit)
+        print(expected, str(unit))
         assert str(unit) == expected
 
         # Now check for long newlines segments
@@ -157,7 +157,7 @@
 msgstr ""
 '''
         unit = self.UnitClass(longstring)
-        print expected, str(unit)
+        print(expected, str(unit))
         assert str(unit) == expected
 
     def test_wrap_on_max_line_length(self):
@@ -165,10 +165,10 @@
         string = "1 3 5 7 N " * 11
         expected = 'msgid ""\n%s\nmsgstr ""\n' % '"1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 "\n"7 N 1 3 5 7 N 1 3 5 7 N 1 3 5 7 N "'
         unit = self.UnitClass(string)
-        print "Expected:"
-        print expected
-        print "Actual:"
-        print str(unit)
+        print("Expected:")
+        print(expected)
+        print("Actual:")
+        print(str(unit))
         assert str(unit) == expected
 
     def test_spacing_max_line(self):
@@ -181,10 +181,10 @@
 msgstr ""
 '''
         unit = self.UnitClass(idstring)
-        print "Expected:"
-        print expected
-        print "Actual:"
-        print str(unit)
+        print("Expected:")
+        print(expected)
+        print("Actual:")
+        print(str(unit))
         assert str(unit) == expected
 
 
@@ -207,7 +207,7 @@
         pofile = self.poparse(posource)
         assert len(pofile.units) == 2
         pofile.removeduplicates("msgctxt")
-        print pofile
+        print(pofile)
         assert len(pofile.units) == 2
         assert str(pofile.units[0]).count("source1") == 2
         assert str(pofile.units[1]).count("source2") == 2
@@ -219,21 +219,21 @@
         assert len(pofile.units) == 2
         pofile.removeduplicates("merge")
         assert len(pofile.units) == 2
-        print pofile.units[0].msgidcomments
-        print pofile.units[1].msgidcomments
+        print(pofile.units[0].msgidcomments)
+        print(pofile.units[1].msgidcomments)
         assert pypo.unquotefrompo(pofile.units[0].msgidcomments) == "_: source1\n"
         assert pypo.unquotefrompo(pofile.units[1].msgidcomments) == "_: source2\n"
 
     def test_output_str_unicode(self):
         """checks that we can str(element) which is in unicode"""
-        posource = u'''#: nb\nmsgid "Norwegian Bokm\xe5l"\nmsgstr ""\n'''
+        posource = '''#: nb\nmsgid "Norwegian Bokm\xe5l"\nmsgstr ""\n'''
         pofile = self.StoreClass(wStringIO.StringIO(posource.encode("UTF-8")), encoding="UTF-8")
         assert len(pofile.units) == 1
-        print str(pofile)
+        print(str(pofile))
         thepo = pofile.units[0]
         assert str(thepo) == posource.encode("UTF-8")
         # extra test: what if we set the msgid to a unicode? this happens in prop2po etc
-        thepo.source = u"Norwegian Bokm\xe5l"
+        thepo.source = "Norwegian Bokm\xe5l"
         assert str(thepo) == posource.encode("UTF-8")
         # Now if we set the msgstr to Unicode
         # this is an escaped half character (1/2)
@@ -247,7 +247,7 @@
         """checks the content of all the expected sections of a PO message"""
         posource = '# other comment\n#. automatic comment\n#: source comment\n#, fuzzy\nmsgid "One"\nmsgstr "Een"\n'
         pofile = self.poparse(posource)
-        print pofile
+        print(pofile)
         assert len(pofile.units) == 1
         assert str(pofile) == posource
         assert pofile.units[0].othercomments == ["# other comment\n"]
@@ -259,7 +259,7 @@
         """tests behaviour of unassociated comments."""
         oldsource = '# old lonesome comment\n\nmsgid "one"\nmsgstr "een"\n'
         oldfile = self.poparse(oldsource)
-        print str(oldfile)
+        print(str(oldfile))
         assert len(oldfile.units) == 1
 
     def test_prevmsgid_parse(self):
@@ -302,15 +302,15 @@
         pofile = self.poparse(posource)
 
         assert pofile.units[1].prev_msgctxt == []
-        assert pofile.units[1].prev_source == multistring([u"trea"])
+        assert pofile.units[1].prev_source == multistring(["trea"])
 
         assert pofile.units[2].prev_msgctxt == []
-        assert pofile.units[2].prev_source == multistring([u"trea", u"treas"])
-
-        assert pofile.units[3].prev_msgctxt == [u'"context 1"']
-        assert pofile.units[3].prev_source == multistring([u"tast"])
-
-        assert pofile.units[4].prev_msgctxt == [u'"context 2"']
-        assert pofile.units[4].prev_source == multistring([u"tast", u"tasts"])
+        assert pofile.units[2].prev_source == multistring(["trea", "treas"])
+
+        assert pofile.units[3].prev_msgctxt == ['"context 1"']
+        assert pofile.units[3].prev_source == multistring(["tast"])
+
+        assert pofile.units[4].prev_msgctxt == ['"context 2"']
+        assert pofile.units[4].prev_source == multistring(["tast", "tasts"])
 
         assert str(pofile) == posource
--- .\storage\test_qph.py	(original)
+++ .\storage\test_qph.py	(refactored)
@@ -53,7 +53,7 @@
         qphfile.addsourceunit("Bla")
         assert len(qphfile.units) == 1
         newfile = qph.QphFile.parsestring(str(qphfile))
-        print str(qphfile)
+        print(str(qphfile))
         assert len(newfile.units) == 1
         assert newfile.units[0].source == "Bla"
         assert newfile.findunit("Bla").source == "Bla"
@@ -64,7 +64,7 @@
         qphunit = qphfile.addsourceunit("Concept")
         qphunit.source = "Term"
         newfile = qph.QphFile.parsestring(str(qphfile))
-        print str(qphfile)
+        print(str(qphfile))
         assert newfile.findunit("Concept") is None
         assert newfile.findunit("Term") is not None
 
@@ -73,7 +73,7 @@
         qphunit = qphfile.addsourceunit("Concept")
         qphunit.target = "Konsep"
         newfile = qph.QphFile.parsestring(str(qphfile))
-        print str(qphfile)
+        print(str(qphfile))
         assert newfile.findunit("Concept").target == "Konsep"
 
     def test_language(self):
--- .\storage\test_tbx.py	(original)
+++ .\storage\test_tbx.py	(refactored)
@@ -17,7 +17,7 @@
         tbxfile.addsourceunit("Bla")
         assert len(tbxfile.units) == 1
         newfile = tbx.tbxfile.parsestring(str(tbxfile))
-        print str(tbxfile)
+        print(str(tbxfile))
         assert len(newfile.units) == 1
         assert newfile.units[0].source == "Bla"
         assert newfile.findunit("Bla").source == "Bla"
@@ -28,7 +28,7 @@
         tbxunit = tbxfile.addsourceunit("Concept")
         tbxunit.source = "Term"
         newfile = tbx.tbxfile.parsestring(str(tbxfile))
-        print str(tbxfile)
+        print(str(tbxfile))
         assert newfile.findunit("Concept") is None
         assert newfile.findunit("Term") is not None
 
@@ -37,5 +37,5 @@
         tbxunit = tbxfile.addsourceunit("Concept")
         tbxunit.target = "Konsep"
         newfile = tbx.tbxfile.parsestring(str(tbxfile))
-        print str(tbxfile)
+        print(str(tbxfile))
         assert newfile.findunit("Concept").target == "Konsep"
--- .\storage\test_tiki.py	(original)
+++ .\storage\test_tiki.py	(refactored)
@@ -19,12 +19,12 @@
     def test_to_unicode(self):
         unit = tiki.TikiUnit("one")
         unit.settarget('two')
-        assert unicode(unit) == '"one" => "two",\n'
+        assert str(unit) == '"one" => "two",\n'
 
         unit2 = tiki.TikiUnit("one")
         unit2.settarget('two')
         unit2.addlocation('untranslated')
-        assert unicode(unit2) == '// "one" => "two",\n'
+        assert str(unit2) == '// "one" => "two",\n'
 
 
 class TestTikiStore:
--- .\storage\test_tmx.py	(original)
+++ .\storage\test_tmx.py	(refactored)
@@ -35,7 +35,7 @@
     def tmxparse(self, tmxsource):
         """helper that parses tmx source without requiring files"""
         dummyfile = wStringIO.StringIO(tmxsource)
-        print tmxsource
+        print(tmxsource)
         tmxfile = tmx.tmxfile(dummyfile)
         return tmxfile
 
@@ -50,7 +50,7 @@
         tmxfile = tmx.tmxfile()
         tmxfile.addtranslation("A string of characters", "en", "'n String karakters", "af")
         newfile = self.tmxparse(str(tmxfile))
-        print str(tmxfile)
+        print(str(tmxfile))
         assert newfile.translate("A string of characters") == "'n String karakters"
         
     def test_withcomment(self):
@@ -59,7 +59,7 @@
         tmxfile.addtranslation("A string of chars",
                                "en", "'n String karakters", "af", "comment")
         newfile = self.tmxparse(str(tmxfile))
-        print str(tmxfile)
+        print(str(tmxfile))
         assert newfile.findunit("A string of chars").getnotes() == "comment"
 
     def test_withnewlines(self):
@@ -67,7 +67,7 @@
         tmxfile = tmx.tmxfile()
         tmxfile.addtranslation("First line\nSecond line", "en", "Eerste lyn\nTweede lyn", "af")
         newfile = self.tmxparse(str(tmxfile))
-        print str(tmxfile)
+        print(str(tmxfile))
         assert newfile.translate("First line\nSecond line") == "Eerste lyn\nTweede lyn"
 
     def test_xmlentities(self):
@@ -76,8 +76,8 @@
         tmxfile.addtranslation("Mail & News", "en", "Nuus & pos", "af")
         tmxfile.addtranslation("Five < ten", "en", "Vyf < tien", "af")
         xmltext = str(tmxfile)
-        print "The generated xml:"
-        print xmltext
+        print("The generated xml:")
+        print(xmltext)
         assert tmxfile.translate('Mail & News') == 'Nuus & pos'
         assert xmltext.index('Mail &amp; News')
         assert xmltext.find('Mail & News') == -1
--- .\storage\test_trados.py	(original)
+++ .\storage\test_trados.py	(refactored)
@@ -9,14 +9,14 @@
 
 def test_unescape():
     # NBSP
-    assert trados.unescape(ur"Ordre du jour\~:") == u"Ordre du jour\u00a0:"
-    assert trados.unescape(ur"Association for Road Safety \endash  Conference") == u"Association for Road Safety C  Conference"
+    assert trados.unescape(r"Ordre du jour\~:") == "Ordre du jour\u00a0:"
+    assert trados.unescape(r"Association for Road Safety \endash  Conference") == "Association for Road Safety C  Conference"
 
 
 def test_escape():
     # NBSP
-    assert trados.escape(u"Ordre du jour\u00a0:") == ur"Ordre du jour\~:"
-    assert trados.escape(u"Association for Road Safety C  Conference") == ur"Association for Road Safety \endash  Conference"
+    assert trados.escape("Ordre du jour\u00a0:") == r"Ordre du jour\~:"
+    assert trados.escape("Association for Road Safety C  Conference") == r"Association for Road Safety \endash  Conference"
 
 #@mark.xfail(reason="Lots to implement")
 #class TestTradosTxtTmUnit(test_base.TestTranslationUnit):
--- .\storage\test_ts2.py	(original)
+++ .\storage\test_ts2.py	(refactored)
@@ -57,7 +57,7 @@
         tsfile.addsourceunit("Bla")
         assert len(tsfile.units) == 1
         newfile = ts.tsfile.parsestring(str(tsfile))
-        print str(tsfile)
+        print(str(tsfile))
         assert len(newfile.units) == 1
         assert newfile.units[0].source == "Bla"
         assert newfile.findunit("Bla").source == "Bla"
@@ -68,7 +68,7 @@
         tsunit = tsfile.addsourceunit("Concept")
         tsunit.source = "Term"
         newfile = ts.tsfile.parsestring(str(tsfile))
-        print str(tsfile)
+        print(str(tsfile))
         assert newfile.findunit("Concept") is None
         assert newfile.findunit("Term") is not None
 
@@ -77,18 +77,18 @@
         tsunit = tsfile.addsourceunit("Concept")
         tsunit.target = "Konsep"
         newfile = ts.tsfile.parsestring(str(tsfile))
-        print str(tsfile)
+        print(str(tsfile))
         assert newfile.findunit("Concept").target == "Konsep"
 
     def test_plurals(self):
         """Test basic plurals"""
         tsfile = ts.tsfile()
         tsunit = tsfile.addsourceunit("File(s)")
-        tsunit.target = [u"Ler", u"Lers"]
+        tsunit.target = ["Ler", "Lers"]
         newfile = ts.tsfile.parsestring(str(tsfile))
-        print str(tsfile)
+        print(str(tsfile))
         checkunit = newfile.findunit("File(s)")
-        assert checkunit.target == [u"Ler", u"Lers"]
+        assert checkunit.target == ["Ler", "Lers"]
         assert checkunit.hasplural()
 
     def test_language(self):
--- .\storage\test_txt.py	(original)
+++ .\storage\test_txt.py	(refactored)
@@ -35,8 +35,8 @@
         txtsource = '''One\nOne\n\nTwo\n---\n\nThree'''
         txtfile = self.txtparse(txtsource)
         assert len(txtfile.units) == 3
-        print txtsource
-        print str(txtfile)
-        print "*%s*" % txtfile.units[0]
+        print(txtsource)
+        print(str(txtfile))
+        print("*%s*" % txtfile.units[0])
         assert str(txtfile) == txtsource
         assert self.txtregen(txtsource) == txtsource
--- .\storage\test_wordfast.py	(original)
+++ .\storage\test_wordfast.py	(refactored)
@@ -37,8 +37,8 @@
                     '\\\n', '\\\t', '\\\\r', '\\\\"']
         for special in specials:
             unit.source = special
-            print "unit.source:", repr(unit.source) + '|'
-            print "special:", repr(special) + '|'
+            print("unit.source:", repr(unit.source) + '|')
+            print("special:", repr(special) + '|')
             assert unit.source == special
 
     def test_wordfast_escaping(self):
@@ -46,7 +46,7 @@
 
         def compare(real, escaped):
             unit = self.UnitClass(real)
-            print real.encode('utf-8'), unit.source.encode('utf-8')
+            print(real.encode('utf-8'), unit.source.encode('utf-8'))
             assert unit.source == real
             assert unit.dict['source'] == escaped
             unit.target = real
--- .\storage\test_xliff.py	(original)
+++ .\storage\test_xliff.py	(refactored)
@@ -67,7 +67,7 @@
         xlifffile.addsourceunit("Bla")
         assert len(xlifffile.units) == 1
         newfile = xliff.xlifffile.parsestring(str(xlifffile))
-        print str(xlifffile)
+        print(str(xlifffile))
         assert len(newfile.units) == 1
         assert newfile.units[0].source == "Bla"
         assert newfile.findunit("Bla").source == "Bla"
@@ -86,92 +86,92 @@
     </xliff:file>
 </xliff:xliff>'''
         xlifffile = xliff.xlifffile.parsestring(xlfsource)
-        print str(xlifffile)
+        print(str(xlifffile))
         assert xlifffile.units[0].source == "File 1"
 
     def test_rich_source(self):
         xlifffile = xliff.xlifffile()
-        xliffunit = xlifffile.addsourceunit(u'')
+        xliffunit = xlifffile.addsourceunit('')
 
         # Test 1
-        xliffunit.rich_source = [StringElem([u'foo', X(id='bar'), u'baz'])]
+        xliffunit.rich_source = [StringElem(['foo', X(id='bar'), 'baz'])]
         source_dom_node = xliffunit.getlanguageNode(None, 0)
         x_placeable = source_dom_node[0]
 
         assert source_dom_node.text == 'foo'
 
-        assert x_placeable.tag == u'x'
+        assert x_placeable.tag == 'x'
         assert x_placeable.attrib['id'] == 'bar'
         assert x_placeable.tail == 'baz'
 
         xliffunit.rich_source[0].print_tree(2)
-        print xliffunit.rich_source
-        assert xliffunit.rich_source == [StringElem([StringElem(u'foo'), X(id='bar'), StringElem(u'baz')])]
+        print(xliffunit.rich_source)
+        assert xliffunit.rich_source == [StringElem([StringElem('foo'), X(id='bar'), StringElem('baz')])]
 
         # Test 2
-        xliffunit.rich_source = [StringElem([u'foo', u'baz', G(id='oof', sub=[G(id='zab', sub=[u'bar', u'rab'])])])]
+        xliffunit.rich_source = [StringElem(['foo', 'baz', G(id='oof', sub=[G(id='zab', sub=['bar', 'rab'])])])]
         source_dom_node = xliffunit.getlanguageNode(None, 0)
         g_placeable = source_dom_node[0]
         nested_g_placeable = g_placeable[0]
 
-        assert source_dom_node.text == u'foobaz'
-
-        assert g_placeable.tag == u'g'
+        assert source_dom_node.text == 'foobaz'
+
+        assert g_placeable.tag == 'g'
         assert g_placeable.text is None
-        assert g_placeable.attrib[u'id'] == u'oof'
+        assert g_placeable.attrib['id'] == 'oof'
         assert g_placeable.tail is None
 
-        assert nested_g_placeable.tag == u'g'
-        assert nested_g_placeable.text == u'barrab'
-        assert nested_g_placeable.attrib[u'id'] == u'zab'
+        assert nested_g_placeable.tag == 'g'
+        assert nested_g_placeable.text == 'barrab'
+        assert nested_g_placeable.attrib['id'] == 'zab'
         assert nested_g_placeable.tail is None
 
         rich_source = xliffunit.rich_source
         rich_source[0].print_tree(2)
-        assert rich_source == [StringElem([u'foobaz', G(id='oof', sub=[G(id='zab', sub=[u'barrab'])])])]
+        assert rich_source == [StringElem(['foobaz', G(id='oof', sub=[G(id='zab', sub=['barrab'])])])]
 
     def test_rich_target(self):
         xlifffile = xliff.xlifffile()
-        xliffunit = xlifffile.addsourceunit(u'')
+        xliffunit = xlifffile.addsourceunit('')
 
         # Test 1
-        xliffunit.set_rich_target([StringElem([u'foo', X(id='bar'), u'baz'])], u'fr')
+        xliffunit.set_rich_target([StringElem(['foo', X(id='bar'), 'baz'])], 'fr')
         target_dom_node = xliffunit.getlanguageNode(None, 1)
         x_placeable = target_dom_node[0]
 
         assert target_dom_node.text == 'foo'
-        assert x_placeable.tag == u'x'
+        assert x_placeable.tag == 'x'
         assert x_placeable.attrib['id'] == 'bar'
         assert x_placeable.tail == 'baz'
 
         # Test 2
-        xliffunit.set_rich_target([StringElem([u'foo', u'baz', G(id='oof', sub=[G(id='zab', sub=[u'bar', u'rab'])])])], u'fr')
+        xliffunit.set_rich_target([StringElem(['foo', 'baz', G(id='oof', sub=[G(id='zab', sub=['bar', 'rab'])])])], 'fr')
         target_dom_node = xliffunit.getlanguageNode(None, 1)
         g_placeable = target_dom_node[0]
         nested_g_placeable = g_placeable[0]
 
-        assert target_dom_node.text == u'foobaz'
-
-        assert g_placeable.tag == u'g'
-        print 'g_placeable.text: %s (%s)' % (g_placeable.text, type(g_placeable.text))
+        assert target_dom_node.text == 'foobaz'
+
+        assert g_placeable.tag == 'g'
+        print('g_placeable.text: %s (%s)' % (g_placeable.text, type(g_placeable.text)))
         assert g_placeable.text is None
-        assert g_placeable.attrib[u'id'] == u'oof'
+        assert g_placeable.attrib['id'] == 'oof'
         assert g_placeable.tail is None
 
-        assert nested_g_placeable.tag == u'g'
-        assert nested_g_placeable.text == u'barrab'
-        assert nested_g_placeable.attrib[u'id'] == u'zab'
+        assert nested_g_placeable.tag == 'g'
+        assert nested_g_placeable.text == 'barrab'
+        assert nested_g_placeable.attrib['id'] == 'zab'
         assert nested_g_placeable.tail is None
 
         xliffunit.rich_target[0].print_tree(2)
-        assert xliffunit.rich_target == [StringElem([u'foobaz', G(id='oof', sub=[G(id='zab', sub=[u'barrab'])])])]
+        assert xliffunit.rich_target == [StringElem(['foobaz', G(id='oof', sub=[G(id='zab', sub=['barrab'])])])]
 
     def test_source(self):
         xlifffile = xliff.xlifffile()
         xliffunit = xlifffile.addsourceunit("Concept")
         xliffunit.source = "Term"
         newfile = xliff.xlifffile.parsestring(str(xlifffile))
-        print str(xlifffile)
+        print(str(xlifffile))
         assert newfile.findunit("Concept") is None
         assert newfile.findunit("Term") is not None
 
@@ -180,20 +180,20 @@
         xliffunit = xlifffile.addsourceunit("Concept")
         xliffunit.target = "Konsep"
         newfile = xliff.xlifffile.parsestring(str(xlifffile))
-        print str(xlifffile)
+        print(str(xlifffile))
         assert newfile.findunit("Concept").target == "Konsep"
 
     def test_sourcelanguage(self):
         xlifffile = xliff.xlifffile(sourcelanguage="xh")
         xmltext = str(xlifffile)
-        print xmltext
+        print(xmltext)
         assert xmltext.find('source-language="xh"') > 0
         #TODO: test that it also works for new files.
 
     def test_targetlanguage(self):
         xlifffile = xliff.xlifffile(sourcelanguage="zu", targetlanguage="af")
         xmltext = str(xlifffile)
-        print xmltext
+        print(xmltext)
         assert xmltext.find('source-language="zu"') > 0
         assert xmltext.find('target-language="af"') > 0
 
@@ -269,13 +269,13 @@
         # test that the source node is before the target node:
         alt = unit.getalttrans()[0]
         altformat = etree.tostring(alt.xmlelement)
-        print altformat
+        print(altformat)
         assert altformat.find("<source") < altformat.find("<target")
 
         # test that a new target is still before alt-trans (bug 1098)
-        unit.target = u"newester target"
+        unit.target = "newester target"
         unitformat = str(unit)
-        print unitformat
+        print(unitformat)
         assert unitformat.find("<source") < unitformat.find("<target") < unitformat.find("<alt-trans")
 
     def test_fuzzy(self):
@@ -297,7 +297,7 @@
         #be uncommented
         unit.target = None
         assert unit.target is None
-        print unit
+        print(unit)
         unit.markfuzzy(True)
         assert 'approved="no"' in str(unit)
         #assert unit.isfuzzy()
--- .\storage\test_zip.py	(original)
+++ .\storage\test_zip.py	(refactored)
@@ -14,7 +14,7 @@
 
     def setup_method(self, method):
         """sets up a test directory"""
-        print "setup_method called on", self.__class__.__name__
+        print("setup_method called on", self.__class__.__name__)
         self.testzip = "%s_testzip.zip" % (self.__class__.__name__)
         self.cleardir(self.testzip)
         self.zip = ZipFile(self.testzip, mode="w")
@@ -44,7 +44,7 @@
 
     def test_created(self):
         """test that the directory actually exists"""
-        print self.testzip
+        print(self.testzip)
         assert os.path.isfile(self.testzip)
 
     def test_basic(self):
--- .\storage\tiki.py	(original)
+++ .\storage\tiki.py	(refactored)
@@ -66,9 +66,9 @@
 
     def __unicode__(self):
         """Returns a string formatted to be inserted into a tiki language.php file."""
-        ret = u'"%s" => "%s",' % (self.source, self.target)
+        ret = '"%s" => "%s",' % (self.source, self.target)
         if self.location == ["untranslated"]:
-            ret = u'// ' + ret
+            ret = '// ' + ret
         return ret + "\n"
 
     def addlocation(self, location):
@@ -121,29 +121,29 @@
 
         output += "// ### Start of unused words\n"
         for unit in _unused:
-            output += unicode(unit)
+            output += str(unit)
         output += "// ### end of unused words\n\n"
         output += "// ### start of untranslated words\n"
         for unit in _untranslated:
-            output += unicode(unit)
+            output += str(unit)
         output += "// ### end of untranslated words\n\n"
         output += "// ### start of possibly untranslated words\n"
         for unit in _possiblyuntranslated:
-            output += unicode(unit)
+            output += str(unit)
         output += "// ### end of possibly untranslated words\n\n"
         for unit in _translated:
-            output += unicode(unit)
+            output += str(unit)
 
         output += self._tiki_footer()
         return output.encode('UTF-8')
 
     def _tiki_header(self):
         """Returns a tiki-file header string."""
-        return u"<?php // -*- coding:utf-8 -*-\n// Generated from po2tiki on %s\n\n$lang=Array(\n" % datetime.datetime.now()
+        return "<?php // -*- coding:utf-8 -*-\n// Generated from po2tiki on %s\n\n$lang=Array(\n" % datetime.datetime.now()
 
     def _tiki_footer(self):
         """Returns a tiki-file footer string."""
-        return u'"###end###"=>"###end###");\n?>'
+        return '"###end###"=>"###end###");\n?>'
 
     def parse(self, input):
         """Parse the given input into source units.
--- .\storage\tmdb.py	(original)
+++ .\storage\tmdb.py	(refactored)
@@ -57,8 +57,8 @@
         self.min_similarity = min_similarity
         self.max_length = max_length
 
-        if not isinstance(db_file, unicode):
-            db_file = unicode(db_file)  # don't know which encoding
+        if not isinstance(db_file, str):
+            db_file = str(db_file)  # don't know which encoding
         self.db_file = db_file
         # share connections to same database file between different instances
         if db_file not in self._tm_dbs:
@@ -170,7 +170,7 @@
             logging.debug("created fulltext triggers")
             self.fulltext = True
 
-        except dbapi2.OperationalError, e:
+        except dbapi2.OperationalError as e:
             self.fulltext = False
             logging.debug("failed to initialize fts3 support: " + str(e))
             script = """
@@ -277,7 +277,7 @@
     def translate_unit(self, unit_source, source_langs, target_langs):
         """return TM suggestions for unit_source"""
         if isinstance(unit_source, str):
-            unit_source = unicode(unit_source, "utf-8")
+            unit_source = str(unit_source, "utf-8")
         if isinstance(source_langs, list):
             source_langs = [data.normalize_code(lang) for lang in source_langs]
             source_langs = ','.join(source_langs)
@@ -296,7 +296,7 @@
         # split source into words, remove punctuation and special
         # chars, keep words that are at least 3 chars long
         unit_words = STRIP_REGEXP.sub(' ', unit_source).split()
-        unit_words = filter(lambda word: len(word) > 2, unit_words)
+        unit_words = [word for word in unit_words if len(word) > 2]
 
         if self.fulltext and len(unit_words) > 3:
             logging.debug("fulltext matching")
@@ -327,7 +327,7 @@
                 })
         results.sort(key=lambda match: match['quality'], reverse=True)
         results = results[:self.max_candidates]
-        logging.debug("results: %s", unicode(results))
+        logging.debug("results: %s", str(results))
         return results
 
 
--- .\storage\tmx.py	(original)
+++ .\storage\tmx.py	(refactored)
@@ -129,7 +129,7 @@
 </tmx>'''
 
     def addheader(self):
-        headernode = self.document.getroot().iterchildren(self.namespaced("header")).next()
+        headernode = next(self.document.getroot().iterchildren(self.namespaced("header")))
         headernode.set("creationtool", "Translate Toolkit - po2tmx")
         headernode.set("creationtoolversion", __version__.sver)
         headernode.set("segtype", "sentence")
@@ -150,8 +150,8 @@
             unit.addnote(comment)
 
         tuvs = unit.xmlelement.iterdescendants(self.namespaced('tuv'))
-        lisa.setXMLlang(tuvs.next(), srclang)
-        lisa.setXMLlang(tuvs.next(), translang)
+        lisa.setXMLlang(next(tuvs), srclang)
+        lisa.setXMLlang(next(tuvs), translang)
 
     def translate(self, sourcetext, sourcelang=None, targetlang=None):
         """method to test old unit tests"""
--- .\storage\trados.py	(original)
+++ .\storage\trados.py	(refactored)
@@ -52,21 +52,21 @@
 """Time format used by Trados .txt"""
 
 RTF_ESCAPES = {
-ur"\emdash": u"",
-ur"\endash": u"C",
+r"\emdash": "",
+r"\endash": "C",
 # Nonbreaking space equal to width of character "m" in current font.
-ur"\emspace": u"\u2003",
+r"\emspace": "\u2003",
 # Nonbreaking space equal to width of character "n" in current font.
-ur"\enspace": u"\u2002",
+r"\enspace": "\u2002",
 #ur"\qmspace": "",    # One-quarter em space.
--- .\storage\ts.py	(original)
+++ .\storage\ts.py	(refactored)
@@ -111,7 +111,7 @@
         if context is None:
             return self.document.searchElementsByTagName("message", self.messageancestors)
         else:
-            if isinstance(context, (str, unicode)):
+            if isinstance(context, str):
                 # look up the context node by name
                 context = self.getcontextnode(context)
                 if context is None:
--- .\storage\ts2.py	(original)
+++ .\storage\ts2.py	(refactored)
@@ -92,7 +92,7 @@
         S_TRANSLATED: (state.UNREVIEWED, state.MAX),
     }
 
-    statemap_r = dict((i[1], i[0]) for i in statemap.iteritems())
+    statemap_r = dict((i[1], i[0]) for i in statemap.items())
 
     def createlanguageNode(self, lang, text, purpose):
         """Returns an xml Element setup with given parameters."""
@@ -118,7 +118,7 @@
 
         def not_none(node):
             return not node is None
-        return filter(not_none, [self._getsourcenode(), self._gettargetnode()])
+        return list(filter(not_none, [self._getsourcenode(), self._gettargetnode()]))
 
     def getsource(self):
         # TODO: support <byte>. See bug 528.
@@ -156,12 +156,12 @@
             self.xmlelement.set("numerus", "yes")
             for string in strings:
                 numerus = etree.SubElement(targetnode, self.namespaced("numerusform"))
-                numerus.text = data.forceunicode(string) or u""
+                numerus.text = data.forceunicode(string) or ""
                 # manual, nasty pretty printing. See bug 1420.
-                numerus.tail = u"\n        "
-        else:
-            targetnode.text = data.forceunicode(text) or u""
-            targetnode.tail = u"\n    "
+                numerus.tail = "\n        "
+        else:
+            targetnode.text = data.forceunicode(text) or ""
+            targetnode.tail = "\n    "
 
     def gettarget(self):
         targetnode = self._gettargetnode()
@@ -170,9 +170,9 @@
             return None
         if self.hasplural():
             numerus_nodes = targetnode.findall(self.namespaced("numerusform"))
-            return multistring([node.text or u"" for node in numerus_nodes])
-        else:
-            return data.forceunicode(targetnode.text) or u""
+            return multistring([node.text or "" for node in numerus_nodes])
+        else:
+            return data.forceunicode(targetnode.text) or ""
     target = property(gettarget, settarget)
     rich_target = property(base.TranslationUnit._get_rich_target, base.TranslationUnit._set_rich_target)
 
@@ -190,7 +190,7 @@
         else:
             note = etree.SubElement(self.xmlelement, self.namespaced("translatorcomment"))
         if position == "append":
-            note.text = "\n".join(filter(None, [current_notes, text.strip()]))
+            note.text = "\n".join([_f for _f in [current_notes, text.strip()] if _f])
         else:
             note.text = text.strip()
 
@@ -280,7 +280,7 @@
         commentnode = self.xmlelement.find(self.namespaced("comment"))
         if commentnode is not None and commentnode.text is not None:
             contexts.append(commentnode.text)
-        contexts = filter(None, contexts)
+        contexts = [_f for _f in contexts if _f]
         return '\n'.join(contexts)
 
     def addlocation(self, location):
--- .\storage\txt.py	(original)
+++ .\storage\txt.py	(refactored)
@@ -63,8 +63,8 @@
 
     def __str__(self):
         """Convert a txt unit to a string"""
-        string = u"".join([self.pretext, self.source, self.posttext])
-        if isinstance(string, unicode):
+        string = "".join([self.pretext, self.source, self.posttext])
+        if isinstance(string, str):
             return string.encode(self.encoding)
         return string
 
@@ -152,7 +152,7 @@
 
     def __str__(self):
         source = self.getoutput()
-        if isinstance(source, unicode):
+        if isinstance(source, str):
             return source.encode(getattr(self, "encoding", "UTF-8"))
         return source
 
--- .\storage\utx.py	(original)
+++ .\storage\utx.py	(refactored)
@@ -115,7 +115,7 @@
         # FIXME update the header date
         if newvalue is None:
             self._dict[key] = None
-        if isinstance(newvalue, unicode):
+        if isinstance(newvalue, str):
             newvalue = newvalue.encode('utf-8')
         if not key in self._dict or newvalue != self._dict[key]:
             self._dict[key] = newvalue
@@ -125,13 +125,13 @@
 
     def addnote(self, text, origin=None, position="append"):
         currentnote = self._get_field('comment')
-        if position == "append" and currentnote is not None and currentnote != u'':
+        if position == "append" and currentnote is not None and currentnote != '':
             self._set_field('comment', currentnote + '\n' + text)
         else:
             self._set_field('comment', text)
 
     def removenotes(self):
-        self._set_field('comment', u'')
+        self._set_field('comment', '')
 
     def getsource(self):
         return self._get_field('src')
@@ -217,7 +217,7 @@
                    "date": self._header["date_created"],
                   }
         items = []
-        for key, value in self._header.iteritems():
+        for key, value in self._header.items():
             if key in ["version", "source_language", "target_language", "date_created"]:
                 continue
             items.append("%s: %s" % (key, value))
--- .\storage\wordfast.py	(original)
+++ .\storage\wordfast.py	(refactored)
@@ -117,39 +117,39 @@
 # For now these look correct and have been taken from Windows CP1252 and
 # Macintosh code points found for the respective character sets on Linux.
 WF_ESCAPE_MAP = (
-              ("&'26;", u"\u0026"),  # & - Ampersand (must be first to prevent
+              ("&'26;", "\u0026"),  # & - Ampersand (must be first to prevent
                                      #     escaping of escapes)
--- .\storage\workflow.py	(original)
+++ .\storage\workflow.py	(refactored)
@@ -35,6 +35,7 @@
 "translated" in PO and "final" in XLIFF. This allows formats to implicitly
 define similar states.
 """
+import collections
 
 
 class StateEnum:
@@ -64,12 +65,12 @@
         return '<State "%s">' % (self.name)
 
     def enter(self, obj):
-        if not self.enter_action or not callable(self.enter_action):
+        if not self.enter_action or not isinstance(self.enter_action, collections.Callable):
             return
         self.enter_action(obj)
 
     def leave(self, obj):
-        if not self.leave_action or not callable(self.leave_action):
+        if not self.leave_action or not isinstance(self.leave_action, collections.Callable):
             return
         self.leave_action(obj)
 
@@ -133,9 +134,9 @@
 
     # METHODS #
     def add_edge(self, from_state, to_state):
-        if isinstance(from_state, basestring):
+        if isinstance(from_state, str):
             from_state = self.get_state_by_name(from_state)
-        if isinstance(to_state, basestring):
+        if isinstance(to_state, str):
             to_state = self.get_state_by_name(to_state)
         for s in (from_state, to_state):
             if s not in self.states:
@@ -177,7 +178,7 @@
             constraints. The current state's ``leave`` and the new state's
             ``enter`` method is still called. For edge transitions, see the
             ``trans`` method."""
-        if isinstance(state, basestring):
+        if isinstance(state, str):
             state = self.get_state_by_name(state)
         if state not in self.states:
             raise StateNotInWorkflowError(state)
@@ -189,7 +190,7 @@
 
     def set_initial_state(self, state):
         """Sets the initial state, used by the :meth:`.reset` method."""
-        if isinstance(state, basestring):
+        if isinstance(state, str):
             state = self.get_state_by_name(state)
         if not isinstance(state, State):
             raise InvalidStateObjectError(state)
@@ -201,7 +202,7 @@
         """Reset the work flow to the initial state using the given object."""
         self._workflow_obj = wf_obj
         if init_state is not None:
-            if isinstance(init_state, basestring):
+            if isinstance(init_state, str):
                 init_state = self.get_state_by_name(init_state)
             if init_state not in self.states:
                 raise StateNotInWorkflowError()
@@ -218,7 +219,7 @@
             returned by ``get_to_states`` is used."""
         if self._current_state is None:
             raise ValueError('No current state set')
-        if isinstance(to_state, basestring):
+        if isinstance(to_state, str):
             to_state = self.get_state_by_name(to_state)
         if to_state is None:
             to_state = self.get_to_states()
@@ -237,7 +238,7 @@
 def create_unit_workflow(unit, state_names):
     wf = Workflow(unit)
 
-    state_info = unit.STATE.items()
+    state_info = list(unit.STATE.items())
     state_info.sort(key=lambda x: x[0])
 
     init_state, prev_state = None, None
--- .\storage\xliff.py	(original)
+++ .\storage\xliff.py	(refactored)
@@ -33,13 +33,13 @@
 
 # TODO: handle translation types
 
-ID_SEPARATOR = u"\04"
+ID_SEPARATOR = "\04"
 # ID_SEPARATOR is commonly used through toolkit to generate compound
 # unit ids (for instance to concatenate msgctxt and msgid in po), but
 # \04 is an illegal char in XML 1.0, ID_SEPARATOR_SAFE will be used
 # instead when converting between xliff and other toolkit supported
 # formats
-ID_SEPARATOR_SAFE = u"__%04__"
+ID_SEPARATOR_SAFE = "__%04__"
 
 
 class xliffunit(lisa.LISAunit):
@@ -73,7 +73,7 @@
                 "final": S_SIGNED_OFF + 1,
                 }
 
-    statemap_r = dict((i[1], i[0]) for i in statemap.iteritems())
+    statemap_r = dict((i[1], i[0]) for i in statemap.items())
 
     STATE = {
         S_UNTRANSLATED: (state.EMPTY, state.NEEDS_WORK),
@@ -111,8 +111,8 @@
         target = None
         nodes = []
         try:
-            source = self.xmlelement.iterchildren(self.namespaced(self.languageNode)).next()
-            target = self.xmlelement.iterchildren(self.namespaced('target')).next()
+            source = next(self.xmlelement.iterchildren(self.namespaced(self.languageNode)))
+            target = next(self.xmlelement.iterchildren(self.namespaced('target')))
             nodes = [source, target]
         except StopIteration:
             if source is not None:
@@ -124,7 +124,7 @@
     def set_rich_source(self, value, sourcelang='en'):
         sourcelanguageNode = self.get_source_dom()
         if sourcelanguageNode is None:
-            sourcelanguageNode = self.createlanguageNode(sourcelang, u'', "source")
+            sourcelanguageNode = self.createlanguageNode(sourcelang, '', "source")
             self.set_source_dom(sourcelanguageNode)
 
         # Clear sourcelanguageNode first
@@ -149,12 +149,12 @@
     def set_rich_target(self, value, lang='xx', append=False):
         self._rich_target = None
         if value is None:
-            self.set_target_dom(self.createlanguageNode(lang, u'', "target"))
+            self.set_target_dom(self.createlanguageNode(lang, '', "target"))
             return
 
         languageNode = self.get_target_dom()
         if languageNode is None:
-            languageNode = self.createlanguageNode(lang, u'', "target")
+            languageNode = self.createlanguageNode(lang, '', "target")
             self.set_target_dom(languageNode, append)
 
         # Clear languageNode first
@@ -217,14 +217,14 @@
                 # the source tag is optional
                 sourcenode = node.iterdescendants(self.namespaced("source"))
                 try:
-                    newunit.source = lisa.getText(sourcenode.next(),
+                    newunit.source = lisa.getText(next(sourcenode),
                                                   getXMLspace(node, self._default_xml_space))
                 except StopIteration:
                     pass
 
                 # must have one or more targets
                 targetnode = node.iterdescendants(self.namespaced("target"))
-                newunit.target = lisa.getText(targetnode.next(),
+                newunit.target = lisa.getText(next(targetnode),
                                               getXMLspace(node, self._default_xml_space))
                 #TODO: support multiple targets better
                 #TODO: support notes in alt-trans
@@ -414,7 +414,7 @@
         self.xmlelement.set("id", id.replace(ID_SEPARATOR, ID_SEPARATOR_SAFE))
 
     def getid(self):
-        uid = u""
+        uid = ""
         try:
             filename = self.xmlelement.iterancestors(self.namespaced('file')).next().get('original')
             if filename:
@@ -423,14 +423,14 @@
             # unit has no proper file ancestor, probably newly created
             pass
         # hide the fact that we sanitize ID_SEPERATOR
-        uid += unicode(self.xmlelement.get("id") or u"").replace(ID_SEPARATOR_SAFE, ID_SEPARATOR)
+        uid += str(self.xmlelement.get("id") or "").replace(ID_SEPARATOR_SAFE, ID_SEPARATOR)
         return uid
 
     def addlocation(self, location):
         self.setid(location)
 
     def getlocations(self):
-        id_attr = unicode(self.xmlelement.get("id") or u"")
+        id_attr = str(self.xmlelement.get("id") or "")
         # XLIFF files downloaded from PO projects in Pootle
         # might have id equal to .source, so let's avoid
         # that:
@@ -507,7 +507,7 @@
         strings = mstr
         if isinstance(mstr, multistring):
             strings = mstr.strings
-        elif isinstance(mstr, basestring):
+        elif isinstance(mstr, str):
             strings = [mstr]
 
         return [xml_to_strelem(s) for s in strings]
@@ -516,7 +516,7 @@
     def rich_to_multistring(cls, elem_list):
         """Override :meth:`TranslationUnit.rich_to_multistring` which is used
         by the ``rich_source`` and ``rich_target`` properties."""
-        return multistring([unicode(elem) for elem in elem_list])
+        return multistring([str(elem) for elem in elem_list])
     rich_to_multistring = classmethod(rich_to_multistring)
 
 
@@ -548,7 +548,7 @@
 
     def initbody(self):
         # detect the xliff namespace, handle both 1.1 and 1.2
-        for prefix, ns in self.document.getroot().nsmap.items():
+        for prefix, ns in list(self.document.getroot().nsmap.items()):
             if ns and ns.startswith(self.unversioned_namespace):
                 self.namespace = ns
                 break
@@ -560,7 +560,7 @@
         if self._filename:
             filenode = self.getfilenode(self._filename, createifmissing=True)
         else:
-            filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
+            filenode = next(self.document.getroot().iterchildren(self.namespaced('file')))
         self.body = self.getbodynode(filenode, createifmissing=True)
 
     def addheader(self):
@@ -606,7 +606,7 @@
         """returns all filenames in this XLIFF file"""
         filenodes = self.document.getroot().iterchildren(self.namespaced("file"))
         filenames = [self.getfilename(filenode) for filenode in filenodes]
-        filenames = filter(None, filenames)
+        filenames = [_f for _f in filenames if _f]
         if len(filenames) == 1 and filenames[0] == '':
             filenames = []
         return filenames
@@ -631,27 +631,27 @@
         units = (unit for unit in self.units if unit.getid().startswith(prefix))
         for index, unit in enumerate(units):
             self.id_index[unit.getid()[len(prefix):]] = unit
-        return self.id_index.keys()
+        return list(self.id_index.keys())
 
     def setsourcelanguage(self, language):
         if not language:
             return
-        filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
+        filenode = next(self.document.getroot().iterchildren(self.namespaced('file')))
         filenode.set("source-language", language)
 
     def getsourcelanguage(self):
-        filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
+        filenode = next(self.document.getroot().iterchildren(self.namespaced('file')))
         return filenode.get("source-language")
     sourcelanguage = property(getsourcelanguage, setsourcelanguage)
 
     def settargetlanguage(self, language):
         if not language:
             return
-        filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
+        filenode = next(self.document.getroot().iterchildren(self.namespaced('file')))
         filenode.set("target-language", language)
 
     def gettargetlanguage(self):
-        filenode = self.document.getroot().iterchildren(self.namespaced('file')).next()
+        filenode = next(self.document.getroot().iterchildren(self.namespaced('file')))
         return filenode.get("target-language")
     targetlanguage = property(gettargetlanguage, settargetlanguage)
 
@@ -703,7 +703,7 @@
         # TODO: Deprecated?
         headernode = filenode.iterchildren(self.namespaced("header"))
         try:
-            return headernode.next()
+            return next(headernode)
         except StopIteration:
             pass
         if not createifmissing:
@@ -715,7 +715,7 @@
         """finds the body node for the given filenode"""
         bodynode = filenode.iterchildren(self.namespaced("body"))
         try:
-            return bodynode.next()
+            return next(bodynode)
         except StopIteration:
             pass
         if not createifmissing:
--- .\storage\placeables\__init__.py	(original)
+++ .\storage\placeables\__init__.py	(refactored)
@@ -45,14 +45,14 @@
 Please refer to the XLIFF specification to get a better understanding.
 """
 
-import base
-import interfaces
-import general
-import xliff
-from base import *
-from base import __all__ as all_your_base
-from strelem import StringElem
-from parse import parse
+from . import base
+from . import interfaces
+from . import general
+from . import xliff
+from .base import *
+from .base import __all__ as all_your_base
+from .strelem import StringElem
+from .parse import parse
 
 __all__ = [
     'base', 'interfaces', 'general', 'parse', 'StringElem', 'xliff'
--- .\storage\placeables\general.py	(original)
+++ .\storage\placeables\general.py	(refactored)
@@ -70,7 +70,7 @@
     """Placeable for numbers."""
 
     istranslatable = False
-    regex = re.compile(ur"[-+]?[0-9]+([\u00a0.,][0-9]+)*")
+    regex = re.compile(r"[-+]?[0-9]+([\u00a0.,][0-9]+)*")
     parse = classmethod(regex_parse)
 
 
@@ -210,7 +210,7 @@
     # FIXME this should really be a list created as being the inverse of what
     # is available on the translators keyboard.  Or easily expanded by their
     # configuration.
--- .\storage\placeables\lisa.py	(original)
+++ .\storage\placeables\lisa.py	(refactored)
@@ -32,7 +32,7 @@
 def make_empty_replacement_placeable(klass, node, xml_space="preserve"):
     try:
         return klass(
-            id=node.attrib[u'id'],
+            id=node.attrib['id'],
             rid=node.attrib.get('rid', None),
             xid=node.attrib.get('xid', None),
             xml_attrib=node.attrib,
@@ -44,7 +44,7 @@
 
 def make_g_placeable(klass, node, xml_space="default"):
     return klass(
-        id=node.attrib[u'id'],
+        id=node.attrib['id'],
         sub=xml_to_strelem(node, xml_space).sub,
         xml_attrib=node.attrib,
     )
@@ -66,14 +66,14 @@
 
 _class_dictionary = {
     #u'bpt': (xliff.Bpt, not_yet_implemented),
-    u'bx': (xliff.Bx, make_empty_replacement_placeable),
+    'bx': (xliff.Bx, make_empty_replacement_placeable),
     #u'ept': (xliff.Ept, not_yet_implemented),
-    u'ex': (xliff.Ex, make_empty_replacement_placeable),
-    u'g': (xliff.G, make_g_placeable),
+    'ex': (xliff.Ex, make_empty_replacement_placeable),
+    'g': (xliff.G, make_g_placeable),
     #u'it': (xliff.It, not_yet_implemented),
     #u'ph': (xliff.Ph, not_yet_implemented),
     #u'sub': (xliff.Sub, not_yet_implemented),
-    u'x': (xliff.X, make_empty_replacement_placeable),
+    'x': (xliff.X, make_empty_replacement_placeable),
 }
 
 
@@ -87,18 +87,18 @@
 
 
 def as_unicode(string):
-    if isinstance(string, unicode):
+    if isinstance(string, str):
         return string
     elif isinstance(string, StringElem):
-        return unicode(string)
-    else:
-        return unicode(string.decode('utf-8'))
+        return str(string)
+    else:
+        return str(string.decode('utf-8'))
 
 
 def xml_to_strelem(dom_node, xml_space="preserve"):
     if dom_node is None:
         return StringElem()
-    if isinstance(dom_node, basestring):
+    if isinstance(dom_node, str):
         dom_node = etree.fromstring(dom_node)
     normalize_xml_space(dom_node, xml_space, remove_start=True)
     result = StringElem()
@@ -108,15 +108,15 @@
             continue
         sub.append(make_placeable(child_dom_node, xml_space))
         if child_dom_node.tail:
-            sub.append(StringElem(unicode(child_dom_node.tail)))
+            sub.append(StringElem(str(child_dom_node.tail)))
 
     # This is just a strange way of inserting the first text and avoiding a
     # call to .prune() which is very expensive. We assume the tree is optimal.
     node_text = dom_node.text
     if sub and node_text:
-        sub.insert(0, StringElem(unicode(node_text)))
+        sub.insert(0, StringElem(str(node_text)))
     elif node_text:
-        sub.append(unicode(node_text))
+        sub.append(str(node_text))
     return result
 
 # ==========================================================
@@ -132,7 +132,7 @@
         dom_node.attrib['rid'] = placeable.rid
 
     if hasattr(placeable, 'xml_attrib'):
-        for attrib, value in placeable.xml_attrib.items():
+        for attrib, value in list(placeable.xml_attrib.items()):
             dom_node.set(attrib, value)
 
     return dom_node
@@ -176,19 +176,19 @@
 def xml_append_string(node, string):
     if not len(node):
         if not node.text:
-            node.text = unicode(string)
+            node.text = str(string)
         else:
-            node.text += unicode(string)
+            node.text += str(string)
     else:
         lastchild = node.getchildren()[-1]
         if lastchild.tail is None:
             lastchild.tail = ''
-        lastchild.tail += unicode(string)
+        lastchild.tail += str(string)
     return node
 
 
 def strelem_to_xml(parent_node, elem):
-    if isinstance(elem, unicode):
+    if isinstance(elem, str):
         return xml_append_string(parent_node, elem)
     if not isinstance(elem, StringElem):
         return parent_node
@@ -211,7 +211,7 @@
 def parse_xliff(pstr):
     try:
         return xml_to_strelem(etree.fromstring('<source>%s</source>' % (pstr)))
-    except Exception, exc:
+    except Exception as exc:
         raise
         return None
 xliff.parsers = [parse_xliff]
--- .\storage\placeables\parse.py	(original)
+++ .\storage\placeables\parse.py	(refactored)
@@ -47,7 +47,7 @@
                         form the original string. If nothing could be
                         parsed, it should return ``None``.
     """
-    if isinstance(tree, unicode):
+    if isinstance(tree, str):
         tree = StringElem(tree)
     if not parse_funcs:
         return tree
@@ -60,7 +60,7 @@
         if not leaf.istranslatable:
             continue
 
-        unileaf = unicode(leaf)
+        unileaf = str(leaf)
         if not unileaf:
             continue
 
@@ -69,7 +69,7 @@
             if (len(subleaves) == 1 and type(leaf) is type(subleaves[0]) and
                 leaf == subleaves[0]):
                 pass
-            elif isinstance(leaf, unicode):
+            elif isinstance(leaf, str):
                 parent = tree.get_parent_elem(leaf)
                 if parent is not None:
                     if len(parent.sub) == 1:
--- .\storage\placeables\strelem.py	(original)
+++ .\storage\placeables\strelem.py	(refactored)
@@ -25,6 +25,7 @@
 
 import logging
 import sys
+import collections
 
 
 class ElementNotFoundError(ValueError):
@@ -60,11 +61,11 @@
     def __init__(self, sub=None, id=None, rid=None, xid=None, **kwargs):
         if sub is None:
             self.sub = []
-        elif isinstance(sub, (unicode, StringElem)):
+        elif isinstance(sub, (str, StringElem)):
             self.sub = [sub]
         else:
             for elem in sub:
-                if not isinstance(elem, (unicode, StringElem)):
+                if not isinstance(elem, (str, StringElem)):
                     raise ValueError(elem)
             self.sub = sub
             self.prune()
@@ -73,7 +74,7 @@
         self.rid = rid
         self.xid = xid
 
-        for key, value in kwargs.items():
+        for key, value in list(kwargs.items()):
             if hasattr(self, key):
                 raise ValueError('attribute already exists: %s' % (key))
             setattr(self, key, value)
@@ -81,11 +82,11 @@
     # SPECIAL METHODS #
     def __add__(self, rhs):
         """Emulate the ``unicode`` class."""
-        return unicode(self) + rhs
+        return str(self) + rhs
 
     def __contains__(self, item):
         """Emulate the ``unicode`` class."""
-        return item in unicode(self)
+        return item in str(self)
 
     def __eq__(self, rhs):
         """:returns: ``True`` if (and only if) all members as well as sub-trees
@@ -104,19 +105,19 @@
 
     def __ge__(self, rhs):
         """Emulate the ``unicode`` class."""
-        return unicode(self) >= rhs
+        return str(self) >= rhs
 
     def __getitem__(self, i):
         """Emulate the ``unicode`` class."""
-        return unicode(self)[i]
+        return str(self)[i]
 
     def __getslice__(self, i, j):
         """Emulate the ``unicode`` class."""
-        return unicode(self)[i:j]
+        return str(self)[i:j]
 
     def __gt__(self, rhs):
         """Emulate the ``unicode`` class."""
-        return unicode(self) > rhs
+        return str(self) > rhs
 
     def __iter__(self):
         """Create an iterator of this element's sub-elements."""
@@ -125,19 +126,19 @@
 
     def __le__(self, rhs):
         """Emulate the ``unicode`` class."""
-        return unicode(self) <= rhs
+        return str(self) <= rhs
 
     def __len__(self):
         """Emulate the ``unicode`` class."""
-        return len(unicode(self))
+        return len(str(self))
 
     def __lt__(self, rhs):
         """Emulate the ``unicode`` class."""
-        return unicode(self) < rhs
+        return str(self) < rhs
 
     def __mul__(self, rhs):
         """Emulate the ``unicode`` class."""
-        return unicode(self) * rhs
+        return str(self) * rhs
 
     def __ne__(self, rhs):
         return not self.__eq__(rhs)
@@ -163,14 +164,14 @@
     def __str__(self):
         if not self.isvisible:
             return ''
-        return ''.join([unicode(elem).encode('utf-8') for elem in self.sub])
+        return ''.join([str(elem).encode('utf-8') for elem in self.sub])
 
     def __unicode__(self):
-        if callable(self.renderer):
+        if isinstance(self.renderer, collections.Callable):
             return self.renderer(self)
         if not self.isvisible:
-            return u''
-        return u''.join([unicode(elem) for elem in self.sub])
+            return ''
+        return ''.join([str(elem) for elem in self.sub])
 
     # METHODS #
     def apply_to_strings(self, f):
@@ -181,7 +182,7 @@
         """
         for elem in self.flatten():
             for i in range(len(elem.sub)):
-                if isinstance(elem.sub[i], basestring):
+                if isinstance(elem.sub[i], str):
                     elem.sub[i] = f(elem.sub[i])
 
     def copy(self):
@@ -311,7 +312,7 @@
 
             # XXX: This might not have the expected result if start['elem']
             # is a StringElem sub-class instance.
-            newstr = u''.join(start['elem'].sub)
+            newstr = ''.join(start['elem'].sub)
             removed = StringElem(newstr[start['offset']:end['offset']])
             newstr = newstr[:start['offset']] + newstr[end['offset']:]
             parent = self.get_parent_elem(start['elem'])
@@ -366,7 +367,7 @@
         for node in marked_nodes:
             try:
                 self.delete_elem(node)
-            except ElementNotFoundError, e:
+            except ElementNotFoundError as e:
                 pass
 
         if start['elem'] is not end['elem']:
@@ -374,29 +375,29 @@
                 (not start['elem'].iseditable and start['elem'].isfragile)):
                 self.delete_elem(start['elem'])
             elif start['elem'].iseditable:
-                start['elem'].sub = [u''.join(start['elem'].sub)[:start['offset']]]
+                start['elem'].sub = [''.join(start['elem'].sub)[:start['offset']]]
 
             if (end_offset + len(end['elem']) == end['index'] or
                 (not end['elem'].iseditable and end['elem'].isfragile)):
                 self.delete_elem(end['elem'])
             elif end['elem'].iseditable:
-                end['elem'].sub = [u''.join(end['elem'].sub)[end['offset']:]]
+                end['elem'].sub = [''.join(end['elem'].sub)[end['offset']:]]
 
         self.prune()
         return removed, None, None
 
     def depth_first(self, filter=None):
         """Returns a list of the nodes in the tree in depth-first order."""
-        if filter is None or not callable(filter):
+        if filter is None or not isinstance(filter, collections.Callable):
             filter = lambda e: True
         elems = []
-        if filter(self):
+        if list(filter(self)):
             elems.append(self)
 
         for sub in self.sub:
             if not isinstance(sub, StringElem):
                 continue
-            if sub.isleaf() and filter(sub):
+            if sub.isleaf() and list(filter(sub)):
                 elems.append(sub)
             else:
                 elems.extend(sub.depth_first())
@@ -404,7 +405,7 @@
 
     def encode(self, encoding=sys.getdefaultencoding()):
         """More ``unicode`` class emulation."""
-        return unicode(self).encode(encoding)
+        return str(self).encode(encoding)
 
     def elem_offset(self, elem):
         """Find the offset of ``elem`` in the current tree.
@@ -430,10 +431,10 @@
             if e.isleaf():
                 leafoffset = 0
                 for s in e.sub:
-                    if unicode(s) == unicode(elem):
+                    if str(s) == str(elem):
                         return offset + leafoffset
                     else:
-                        leafoffset += len(unicode(s))
+                        leafoffset += len(str(s))
                 offset += len(e)
         return -1
 
@@ -455,22 +456,22 @@
     def find(self, x):
         """Find sub-string ``x`` in this string tree and return the position
             at which it starts."""
-        if isinstance(x, basestring):
-            return unicode(self).find(x)
+        if isinstance(x, str):
+            return str(self).find(x)
         if isinstance(x, StringElem):
-            return unicode(self).find(unicode(x))
+            return str(self).find(str(x))
         return None
 
     def find_elems_with(self, x):
         """Find all elements in the current sub-tree containing ``x``."""
-        return [elem for elem in self.flatten() if x in unicode(elem)]
+        return [elem for elem in self.flatten() if x in str(elem)]
 
     def flatten(self, filter=None):
         """Flatten the tree by returning a depth-first search over the
         tree's leaves."""
-        if filter is None or not callable(filter):
+        if filter is None or not isinstance(filter, collections.Callable):
             filter = lambda e: True
-        return [elem for elem in self.iter_depth_first(lambda e: e.isleaf() and filter(e))]
+        return [elem for elem in self.iter_depth_first(lambda e: e.isleaf() and list(filter(e)))]
 
     def get_ancestor_where(self, child, criteria):
         parent = self.get_parent_elem(child)
@@ -517,14 +518,14 @@
             string (Unicode) representation."""
         if offset < 0 or offset > len(self) + 1:
             raise IndexError('Index out of range: %d' % (offset))
-        if isinstance(text, (str, unicode)):
+        if isinstance(text, str):
             text = StringElem(text)
         if not isinstance(text, StringElem):
             raise ValueError('text must be of type StringElem')
 
         def checkleaf(elem, text):
             if elem.isleaf() and type(text) is StringElem and text.isleaf():
-                return unicode(text)
+                return str(text)
             return text
 
         # There are 4 general cases (including specific cases) where text can
@@ -580,11 +581,11 @@
                 #logging.debug('Case 3')
                 eoffset = offset - self.elem_offset(oelem)
                 if oelem.isleaf():
-                    s = unicode(oelem)  # Collapse all sibling strings into one
+                    s = str(oelem)  # Collapse all sibling strings into one
                     head = s[:eoffset]
                     tail = s[eoffset:]
                     if type(text) is StringElem and text.isleaf():
-                        oelem.sub = [head + unicode(text) + tail]
+                        oelem.sub = [head + str(text) + tail]
                     else:
                         oelem.sub = [StringElem(head), text, StringElem(tail)]
                     return True
@@ -657,7 +658,7 @@
                 raise ValueError('"left" and "right" refer to the same element and is not empty.')
             if not left.iseditable:
                 return False
-        if isinstance(text, unicode):
+        if isinstance(text, str):
             text = StringElem(text)
 
         if left is right:
@@ -759,20 +760,20 @@
         :rtype: bool
         """
         for e in self.sub:
-            if not isinstance(e, (str, unicode)):
+            if not isinstance(e, str):
                 return False
         return True
 
     def iter_depth_first(self, filter=None):
         """Iterate through the nodes in the tree in dept-first order."""
-        if filter is None or not callable(filter):
+        if filter is None or not isinstance(filter, collections.Callable):
             filter = lambda e: True
-        if filter(self):
+        if list(filter(self)):
             yield self
         for sub in self.sub:
             if not isinstance(sub, StringElem):
                 continue
-            if sub.isleaf() and filter(sub):
+            if sub.isleaf() and list(filter(sub)):
                 yield sub
             else:
                 for node in sub.iter_depth_first(filter):
@@ -781,13 +782,13 @@
     def map(self, f, filter=None):
         """Apply ``f`` to all nodes for which ``filter`` returned ``True``
         (optional)."""
-        if filter is not None and not callable(filter):
+        if filter is not None and not isinstance(filter, collections.Callable):
             raise ValueError('filter is not callable or None')
         if filter is None:
             filter = lambda e: True
 
         for elem in self.depth_first():
-            if filter(elem):
+            if list(filter(elem)):
                 f(elem)
 
     @classmethod
@@ -806,19 +807,19 @@
         """Print the tree from the current instance's point in an indented
             manner."""
         indent_prefix = " " * indent * 2
-        out = (u"%s%s [%s]" % (indent_prefix, self.__class__.__name__,
-                               unicode(self))).encode('utf-8')
+        out = ("%s%s [%s]" % (indent_prefix, self.__class__.__name__,
+                               str(self))).encode('utf-8')
         if verbose:
-            out += u' ' + repr(self)
-
-        print out
+            out += ' ' + repr(self)
+
+        print(out)
 
         for elem in self.sub:
             if isinstance(elem, StringElem):
                 elem.print_tree(indent + 1, verbose=verbose)
             else:
-                print (u'%s%s[%s]' % (indent_prefix, indent_prefix,
-                                      elem)).encode('utf-8')
+                print(('%s%s[%s]' % (indent_prefix, indent_prefix,
+                                      elem)).encode('utf-8'))
 
     def prune(self):
         """Remove unnecessary nodes to make the tree optimal."""
@@ -851,18 +852,18 @@
 
             if type(elem) is StringElem and elem.isleaf():
                 # Collapse all strings in this leaf into one string.
-                elem.sub = [u''.join(elem.sub)]
-
-            for i in reversed(range(len(elem.sub))):
+                elem.sub = [''.join(elem.sub)]
+
+            for i in reversed(list(range(len(elem.sub)))):
                 # Remove empty strings or StringElem nodes
                 # (but not StringElem sub-class instances, because they
                 # might contain important (non-rendered) data.
-                if (type(elem.sub[i]) in (StringElem, str, unicode) and
+                if (type(elem.sub[i]) in (StringElem, str, str) and
                     len(elem.sub[i]) == 0):
                     del elem.sub[i]
                     continue
 
-                if type(elem.sub[i]) in (str, unicode) and not elem.isleaf():
+                if type(elem.sub[i]) in (str, str) and not elem.isleaf():
                     elem.sub[i] = StringElem(elem.sub[i])
                     changed = True
 
--- .\storage\placeables\test_base.py	(original)
+++ .\storage\placeables\test_base.py	(refactored)
@@ -21,28 +21,29 @@
 from pytest import mark
 
 from translate.storage.placeables import base, general, parse, xliff, StringElem
+import collections
 
 
 class TestStringElem:
--- .\storage\placeables\test_general.py	(original)
+++ .\storage\placeables\test_general.py	(refactored)
@@ -5,116 +5,116 @@
 
 def test_placeable_numbers():
     """Check the correct functioning of number placeables"""
-    assert general.NumberPlaceable([u"25"]) in general.NumberPlaceable.parse(u"Here is a 25 number")
-    assert general.NumberPlaceable([u"-25"]) in general.NumberPlaceable.parse(u"Here is a -25 number")
-    assert general.NumberPlaceable([u"+25"]) in general.NumberPlaceable.parse(u"Here is a +25 number")
-    assert general.NumberPlaceable([u"25.00"]) in general.NumberPlaceable.parse(u"Here is a 25.00 number")
-    assert general.NumberPlaceable([u"2,500.00"]) in general.NumberPlaceable.parse(u"Here is a 2,500.00 number")
-    assert general.NumberPlaceable([u"1\u00a0000,99"]) in general.NumberPlaceable.parse(u"Here is a 1\u00a0000,99 number")
+    assert general.NumberPlaceable(["25"]) in general.NumberPlaceable.parse("Here is a 25 number")
+    assert general.NumberPlaceable(["-25"]) in general.NumberPlaceable.parse("Here is a -25 number")
+    assert general.NumberPlaceable(["+25"]) in general.NumberPlaceable.parse("Here is a +25 number")
+    assert general.NumberPlaceable(["25.00"]) in general.NumberPlaceable.parse("Here is a 25.00 number")
+    assert general.NumberPlaceable(["2,500.00"]) in general.NumberPlaceable.parse("Here is a 2,500.00 number")
+    assert general.NumberPlaceable(["1\u00a0000,99"]) in general.NumberPlaceable.parse("Here is a 1\u00a0000,99 number")
 
 
 def test_placeable_newline():
-    assert general.NewlinePlaceable.parse(u"A newline\n")[1] == general.NewlinePlaceable([u"\n"])
-    assert general.NewlinePlaceable.parse(u"First\nSecond")[1] == general.NewlinePlaceable([u"\n"])
+    assert general.NewlinePlaceable.parse("A newline\n")[1] == general.NewlinePlaceable(["\n"])
+    assert general.NewlinePlaceable.parse("First\nSecond")[1] == general.NewlinePlaceable(["\n"])
 
 
 def test_placeable_alt_attr():
-    assert general.AltAttrPlaceable.parse(u'Click on the <img src="image.jpg" alt="Image">')[1] == general.AltAttrPlaceable([u'alt="Image"'])
+    assert general.AltAttrPlaceable.parse('Click on the <img src="image.jpg" alt="Image">')[1] == general.AltAttrPlaceable(['alt="Image"'])
 
 
 def test_placeable_qt_formatting():
-    assert general.QtFormattingPlaceable.parse(u'One %1 %99 %L1 are all valid')[1] == general.QtFormattingPlaceable([u'%1'])
-    assert general.QtFormattingPlaceable.parse(u'One %1 %99 %L1 are all valid')[3] == general.QtFormattingPlaceable([u'%99'])
-    assert general.QtFormattingPlaceable.parse(u'One %1 %99 %L1 are all valid')[5] == general.QtFormattingPlaceable([u'%L1'])
+    assert general.QtFormattingPlaceable.parse('One %1 %99 %L1 are all valid')[1] == general.QtFormattingPlaceable(['%1'])
+    assert general.QtFormattingPlaceable.parse('One %1 %99 %L1 are all valid')[3] == general.QtFormattingPlaceable(['%99'])
+    assert general.QtFormattingPlaceable.parse('One %1 %99 %L1 are all valid')[5] == general.QtFormattingPlaceable(['%L1'])
 
 
 def test_placeable_camelcase():
-    assert general.CamelCasePlaceable.parse(u'CamelCase')[0] == general.CamelCasePlaceable([u'CamelCase'])
-    assert general.CamelCasePlaceable.parse(u'iPod')[0] == general.CamelCasePlaceable([u'iPod'])
-    assert general.CamelCasePlaceable.parse(u'DokuWiki')[0] == general.CamelCasePlaceable([u'DokuWiki'])
-    assert general.CamelCasePlaceable.parse(u'KBabel')[0] == general.CamelCasePlaceable([u'KBabel'])
-    assert general.CamelCasePlaceable.parse(u'_Bug') is None
-    assert general.CamelCasePlaceable.parse(u'NOTCAMEL') is None
+    assert general.CamelCasePlaceable.parse('CamelCase')[0] == general.CamelCasePlaceable(['CamelCase'])
+    assert general.CamelCasePlaceable.parse('iPod')[0] == general.CamelCasePlaceable(['iPod'])
+    assert general.CamelCasePlaceable.parse('DokuWiki')[0] == general.CamelCasePlaceable(['DokuWiki'])
+    assert general.CamelCasePlaceable.parse('KBabel')[0] == general.CamelCasePlaceable(['KBabel'])
+    assert general.CamelCasePlaceable.parse('_Bug') is None
+    assert general.CamelCasePlaceable.parse('NOTCAMEL') is None
 
 
 def test_placeable_space():
-    assert general.SpacesPlaceable.parse(u' Space at start')[0] == general.SpacesPlaceable([u' '])
-    assert general.SpacesPlaceable.parse(u'Space at end ')[1] == general.SpacesPlaceable([u' '])
-    assert general.SpacesPlaceable.parse(u'Double  space')[1] == general.SpacesPlaceable([u'  '])
+    assert general.SpacesPlaceable.parse(' Space at start')[0] == general.SpacesPlaceable([' '])
+    assert general.SpacesPlaceable.parse('Space at end ')[1] == general.SpacesPlaceable([' '])
+    assert general.SpacesPlaceable.parse('Double  space')[1] == general.SpacesPlaceable(['  '])
 
 
 def test_placeable_punctuation():
-    assert general.PunctuationPlaceable.parse(u'These, are not. Special: punctuation; marks! Or are "they"?') is None
-    assert general.PunctuationPlaceable.parse(u'Downloading')[1] == general.PunctuationPlaceable([u''])
+    assert general.PunctuationPlaceable.parse('These, are not. Special: punctuation; marks! Or are "they"?') is None
+    assert general.PunctuationPlaceable.parse('Downloading')[1] == general.PunctuationPlaceable([''])
 
 
 def test_placeable_xml_entity():
-    assert general.XMLEntityPlaceable.parse(u'&brandShortName;')[0] == general.XMLEntityPlaceable([u'&brandShortName;'])
-    assert general.XMLEntityPlaceable.parse(u'&#1234;')[0] == general.XMLEntityPlaceable([u'&#1234;'])
-    assert general.XMLEntityPlaceable.parse(u'&xDEAD;')[0] == general.XMLEntityPlaceable([u'&xDEAD;'])
+    assert general.XMLEntityPlaceable.parse('&brandShortName;')[0] == general.XMLEntityPlaceable(['&brandShortName;'])
+    assert general.XMLEntityPlaceable.parse('&#1234;')[0] == general.XMLEntityPlaceable(['&#1234;'])
+    assert general.XMLEntityPlaceable.parse('&xDEAD;')[0] == general.XMLEntityPlaceable(['&xDEAD;'])
 
 
 def test_placeable_xml_tag():
-    assert general.XMLTagPlaceable.parse(u'<a>koei</a>')[0] == general.XMLTagPlaceable([u'<a>'])
-    assert general.XMLTagPlaceable.parse(u'<a>koei</a>')[2] == general.XMLTagPlaceable([u'</a>'])
-    assert general.XMLTagPlaceable.parse(u'<Exif.XResolution>')[0] == general.XMLTagPlaceable([u'<Exif.XResolution>'])
-    assert general.XMLTagPlaceable.parse(u'<tag_a>')[0] == general.XMLTagPlaceable([u'<tag_a>'])
-    assert general.XMLTagPlaceable.parse(u'<img src="koei.jpg" />')[0] == general.XMLTagPlaceable([u'<img src="koei.jpg" />'])
+    assert general.XMLTagPlaceable.parse('<a>koei</a>')[0] == general.XMLTagPlaceable(['<a>'])
+    assert general.XMLTagPlaceable.parse('<a>koei</a>')[2] == general.XMLTagPlaceable(['</a>'])
+    assert general.XMLTagPlaceable.parse('<Exif.XResolution>')[0] == general.XMLTagPlaceable(['<Exif.XResolution>'])
+    assert general.XMLTagPlaceable.parse('<tag_a>')[0] == general.XMLTagPlaceable(['<tag_a>'])
+    assert general.XMLTagPlaceable.parse('<img src="koei.jpg" />')[0] == general.XMLTagPlaceable(['<img src="koei.jpg" />'])
     # We don't want this to be recognised, so we test for None - not sure if that is a stable assumption
-    assert general.XMLTagPlaceable.parse(u'<important word>') is None
-    assert general.XMLTagPlaceable.parse(u'<img ="koei.jpg" />') is None
-    assert general.XMLTagPlaceable.parse(u'<img "koei.jpg" />') is None
-    assert general.XMLTagPlaceable.parse(u'<span xml:space="preserve">')[0] == general.XMLTagPlaceable([u'<span xml:space="preserve">'])
-    assert general.XMLTagPlaceable.parse(u'<img src="http://translate.org.za/blogs/friedel/sites/translate.org.za.blogs.friedel/files/virtaal-7f_help.png" alt="Virtaal met lernaam-pseudovertaling" style="border: 1px dotted grey;" />')[0] == general.XMLTagPlaceable([u'<img src="http://translate.org.za/blogs/friedel/sites/translate.org.za.blogs.friedel/files/virtaal-7f_help.png" alt="Virtaal met lernaam-pseudovertaling" style="border: 1px dotted grey;" />'])
+    assert general.XMLTagPlaceable.parse('<important word>') is None
+    assert general.XMLTagPlaceable.parse('<img ="koei.jpg" />') is None
+    assert general.XMLTagPlaceable.parse('<img "koei.jpg" />') is None
+    assert general.XMLTagPlaceable.parse('<span xml:space="preserve">')[0] == general.XMLTagPlaceable(['<span xml:space="preserve">'])
+    assert general.XMLTagPlaceable.parse('<img src="http://translate.org.za/blogs/friedel/sites/translate.org.za.blogs.friedel/files/virtaal-7f_help.png" alt="Virtaal met lernaam-pseudovertaling" style="border: 1px dotted grey;" />')[0] == general.XMLTagPlaceable(['<img src="http://translate.org.za/blogs/friedel/sites/translate.org.za.blogs.friedel/files/virtaal-7f_help.png" alt="Virtaal met lernaam-pseudovertaling" style="border: 1px dotted grey;" />'])
     # Bug 933
-    assert general.XMLTagPlaceable.parse(u'This entry expires in %days% days. Would you like to <a href="%href%?PHPSESSID=5d59c559cf4eb9f1d278918271fbe68a" title="Renew this Entry Now">Renew this Entry Now</a> ?')[1] == general.XMLTagPlaceable([u'<a href="%href%?PHPSESSID=5d59c559cf4eb9f1d278918271fbe68a" title="Renew this Entry Now">'])
-    assert general.XMLTagPlaceable.parse(u'''<span weight='bold' size='larger'>Your Google Account is locked</span>''')[0] == general.XMLTagPlaceable([u'''<span weight='bold' size='larger'>'''])
+    assert general.XMLTagPlaceable.parse('This entry expires in %days% days. Would you like to <a href="%href%?PHPSESSID=5d59c559cf4eb9f1d278918271fbe68a" title="Renew this Entry Now">Renew this Entry Now</a> ?')[1] == general.XMLTagPlaceable(['<a href="%href%?PHPSESSID=5d59c559cf4eb9f1d278918271fbe68a" title="Renew this Entry Now">'])
+    assert general.XMLTagPlaceable.parse('''<span weight='bold' size='larger'>Your Google Account is locked</span>''')[0] == general.XMLTagPlaceable(['''<span weight='bold' size='larger'>'''])
 
 
 def test_placeable_option():
-    assert general.OptionPlaceable.parse(u'Type --help for this help')[1] == general.OptionPlaceable([u'--help'])
-    assert general.OptionPlaceable.parse(u'Short -S ones also')[1] == general.OptionPlaceable([u'-S'])
+    assert general.OptionPlaceable.parse('Type --help for this help')[1] == general.OptionPlaceable(['--help'])
+    assert general.OptionPlaceable.parse('Short -S ones also')[1] == general.OptionPlaceable(['-S'])
 
 
 def test_placeable_file():
-    assert general.FilePlaceable.parse(u'Store in /home/user')[1] == general.FilePlaceable([u'/home/user'])
-    assert general.FilePlaceable.parse(u'Store in ~/Download directory')[1] == general.FilePlaceable([u'~/Download'])
+    assert general.FilePlaceable.parse('Store in /home/user')[1] == general.FilePlaceable(['/home/user'])
+    assert general.FilePlaceable.parse('Store in ~/Download directory')[1] == general.FilePlaceable(['~/Download'])
 
 
 def test_placeable_email():
-    assert general.EmailPlaceable.parse(u'Send email to info@example.com')[1] == general.EmailPlaceable([u'info@example.com'])
-    assert general.EmailPlaceable.parse(u'Send email to mailto:info@example.com')[1] == general.EmailPlaceable([u'mailto:info@example.com'])
+    assert general.EmailPlaceable.parse('Send email to info@example.com')[1] == general.EmailPlaceable(['info@example.com'])
+    assert general.EmailPlaceable.parse('Send email to mailto:info@example.com')[1] == general.EmailPlaceable(['mailto:info@example.com'])
 
 
 def test_placeable_caps():
-    assert general.CapsPlaceable.parse(u'Use the HTML page')[1] == general.CapsPlaceable([u'HTML'])
-    assert general.CapsPlaceable.parse(u'I am') is None
-    assert general.CapsPlaceable.parse(u'Use the A4 paper') is None
-    assert general.CapsPlaceable.parse(u'In GTK+')[1] == general.CapsPlaceable([u'GTK+'])
+    assert general.CapsPlaceable.parse('Use the HTML page')[1] == general.CapsPlaceable(['HTML'])
+    assert general.CapsPlaceable.parse('I am') is None
+    assert general.CapsPlaceable.parse('Use the A4 paper') is None
+    assert general.CapsPlaceable.parse('In GTK+')[1] == general.CapsPlaceable(['GTK+'])
 #    assert general.CapsPlaceable.parse(u'GNOME-stuff')[0] == general.CapsPlaceable([u'GNOME'])
-    assert general.CapsPlaceable.parse(u'with XDG_USER_DIRS')[1] == general.CapsPlaceable([u'XDG_USER_DIRS'])
+    assert general.CapsPlaceable.parse('with XDG_USER_DIRS')[1] == general.CapsPlaceable(['XDG_USER_DIRS'])
 
 
 def test_placeable_formatting():
     fp = general.FormattingPlaceable
-    assert fp.parse(u'There were %d cows')[1] == fp([u'%d'])
-    assert fp.parse(u'There were %Id cows')[1] == fp([u'%Id'])
-    assert fp.parse(u'There were %d %s')[3] == fp([u'%s'])
-    assert fp.parse(u'%1$s was kicked by %2$s')[0] == fp([u'%1$s'])
-    assert fp.parse(u'There were %Id cows')[1] == fp([u'%Id'])
-    assert fp.parse(u'There were % d cows')[1] == fp([u'% d'])
+    assert fp.parse('There were %d cows')[1] == fp(['%d'])
+    assert fp.parse('There were %Id cows')[1] == fp(['%Id'])
+    assert fp.parse('There were %d %s')[3] == fp(['%s'])
+    assert fp.parse('%1$s was kicked by %2$s')[0] == fp(['%1$s'])
+    assert fp.parse('There were %Id cows')[1] == fp(['%Id'])
+    assert fp.parse('There were % d cows')[1] == fp(['% d'])
     #only a real space is allowed as formatting flag
-    assert fp.parse(u'There were %\u00a0d cows') is None
-    assert fp.parse(u"There were %'f cows")[1] == fp([u"%'f"])
-    assert fp.parse(u"There were %#x cows")[1] == fp([u"%#x"])
+    assert fp.parse('There were %\u00a0d cows') is None
+    assert fp.parse("There were %'f cows")[1] == fp(["%'f"])
+    assert fp.parse("There were %#x cows")[1] == fp(["%#x"])
 
     #field width
-    assert fp.parse(u'There were %3d cows')[1] == fp([u'%3d'])
-    assert fp.parse(u'There were %33d cows')[1] == fp([u'%33d'])
-    assert fp.parse(u'There were %*d cows')[1] == fp([u'%*d'])
+    assert fp.parse('There were %3d cows')[1] == fp(['%3d'])
+    assert fp.parse('There were %33d cows')[1] == fp(['%33d'])
+    assert fp.parse('There were %*d cows')[1] == fp(['%*d'])
 
     #numbered variables
-    assert fp.parse(u'There were %1$d cows')[1] == fp([u'%1$d'])
+    assert fp.parse('There were %1$d cows')[1] == fp(['%1$d'])
 
 
 # TODO: PythonFormattingPlaceable, JavaMessageFormatPlaceable, UrlPlaceable, XMLTagPlaceable
--- .\storage\placeables\test_lisa.py	(original)
+++ .\storage\placeables\test_lisa.py	(refactored)
@@ -26,97 +26,97 @@
 
 
 def test_xml_to_strelem():
-    source = etree.fromstring(u'<source>a</source>')
+    source = etree.fromstring('<source>a</source>')
     elem = lisa.xml_to_strelem(source)
-    assert elem == StringElem(u'a')
+    assert elem == StringElem('a')
 
-    source = etree.fromstring(u'<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>')
+    source = etree.fromstring('<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>')
     elem = lisa.xml_to_strelem(source)
-    assert elem.sub == [StringElem(u'a'), X(id=u'foo[1]/bar[1]/baz[1]')]
+    assert elem.sub == [StringElem('a'), X(id='foo[1]/bar[1]/baz[1]')]
 
-    source = etree.fromstring(u'<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>')
+    source = etree.fromstring('<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>')
     elem = lisa.xml_to_strelem(source)
-    assert elem.sub == [StringElem(u'a'), X(id=u'foo[1]/bar[1]/baz[1]'), StringElem(u'')]
+    assert elem.sub == [StringElem('a'), X(id='foo[1]/bar[1]/baz[1]'), StringElem('')]
 
-    source = etree.fromstring(u'<source>a<g id="foo[2]/bar[2]/baz[2]">b<x id="foo[1]/bar[1]/baz[1]"/>c</g></source>')
+    source = etree.fromstring('<source>a<g id="foo[2]/bar[2]/baz[2]">b<x id="foo[1]/bar[1]/baz[1]"/>c</g></source>')
     elem = lisa.xml_to_strelem(source)
-    assert elem.sub == [StringElem(u'a'), G(id=u'foo[2]/bar[2]/baz[2]', sub=[StringElem(u'b'), X(id=u'foo[1]/bar[1]/baz[1]'), StringElem(u'c')]), StringElem(u'')]
+    assert elem.sub == [StringElem('a'), G(id='foo[2]/bar[2]/baz[2]', sub=[StringElem('b'), X(id='foo[1]/bar[1]/baz[1]'), StringElem('c')]), StringElem('')]
 
 
 def test_xml_space():
-    source = etree.fromstring(u'<source xml:space="default"> a <x id="foo[1]/bar[1]/baz[1]"/> </source>')
+    source = etree.fromstring('<source xml:space="default"> a <x id="foo[1]/bar[1]/baz[1]"/> </source>')
     elem = lisa.xml_to_strelem(source)
-    print elem.sub
-    assert elem.sub == [StringElem(u'a '), X(id=u'foo[1]/bar[1]/baz[1]'), StringElem(u' ')]
+    print(elem.sub)
+    assert elem.sub == [StringElem('a '), X(id='foo[1]/bar[1]/baz[1]'), StringElem(' ')]
 
 
 def test_chunk_list():
-    left = StringElem([u'a', G(id='foo[2]/bar[2]/baz[2]', sub=[u'b', X(id='foo[1]/bar[1]/baz[1]'), u'c']), u''])
-    right = StringElem([u'a', G(id='foo[2]/bar[2]/baz[2]', sub=[u'b', X(id='foo[1]/bar[1]/baz[1]'), u'c']), u''])
+    left = StringElem(['a', G(id='foo[2]/bar[2]/baz[2]', sub=['b', X(id='foo[1]/bar[1]/baz[1]'), 'c']), ''])
+    right = StringElem(['a', G(id='foo[2]/bar[2]/baz[2]', sub=['b', X(id='foo[1]/bar[1]/baz[1]'), 'c']), ''])
     assert left == right
 
 
 def test_set_strelem_to_xml():
-    source = etree.Element(u'source')
-    lisa.strelem_to_xml(source, StringElem(u'a'))
+    source = etree.Element('source')
+    lisa.strelem_to_xml(source, StringElem('a'))
     assert etree.tostring(source, encoding='UTF-8') == '<source>a</source>'
 
-    source = etree.Element(u'source')
-    lisa.strelem_to_xml(source, StringElem([u'a', u'']))
+    source = etree.Element('source')
+    lisa.strelem_to_xml(source, StringElem(['a', '']))
     assert etree.tostring(source, encoding='UTF-8') == '<source>a</source>'
 
-    source = etree.Element(u'source')
+    source = etree.Element('source')
     lisa.strelem_to_xml(source, StringElem(X(id='foo[1]/bar[1]/baz[1]')))
     assert etree.tostring(source, encoding='UTF-8') == '<source><x id="foo[1]/bar[1]/baz[1]"/></source>'
 
-    source = etree.Element(u'source')
-    lisa.strelem_to_xml(source, StringElem([u'a', X(id='foo[1]/bar[1]/baz[1]')]))
+    source = etree.Element('source')
+    lisa.strelem_to_xml(source, StringElem(['a', X(id='foo[1]/bar[1]/baz[1]')]))
     assert etree.tostring(source, encoding='UTF-8') == '<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>'
 
-    source = etree.Element(u'source')
-    lisa.strelem_to_xml(source, StringElem([u'a', X(id='foo[1]/bar[1]/baz[1]'), u'']))
+    source = etree.Element('source')
+    lisa.strelem_to_xml(source, StringElem(['a', X(id='foo[1]/bar[1]/baz[1]'), '']))
     assert etree.tostring(source, encoding='UTF-8') == '<source>a<x id="foo[1]/bar[1]/baz[1]"/></source>'
 
-    source = etree.Element(u'source')
-    lisa.strelem_to_xml(source, StringElem([u'a', G(id='foo[2]/bar[2]/baz[2]', sub=[u'b', X(id='foo[1]/bar[1]/baz[1]'), u'c']), u'']))
+    source = etree.Element('source')
+    lisa.strelem_to_xml(source, StringElem(['a', G(id='foo[2]/bar[2]/baz[2]', sub=['b', X(id='foo[1]/bar[1]/baz[1]'), 'c']), '']))
     assert etree.tostring(source, encoding='UTF-8') == '<source>a<g id="foo[2]/bar[2]/baz[2]">b<x id="foo[1]/bar[1]/baz[1]"/>c</g></source>'
 
 
 def test_unknown_xml_placeable():
     # The XML below is (modified) from the official XLIFF example file Sample_AlmostEverything_1.2_strict.xlf
-    source = etree.fromstring(u"""<source xml:lang="en-us">Text <g id="_1_ski_040">g</g>TEXT<bpt id="_1_ski_139">bpt<sub>sub</sub>
+    source = etree.fromstring("""<source xml:lang="en-us">Text <g id="_1_ski_040">g</g>TEXT<bpt id="_1_ski_139">bpt<sub>sub</sub>
                </bpt>TEXT<ept id="_1_ski_238">ept</ept>TEXT<ph id="_1_ski_337"/>TEXT<it id="_1_ski_436" pos="open">it</it>TEXT<mrk mtype="x-test">mrk</mrk>
                <x id="_1_ski_535"/>TEXT<bx id="_1_ski_634"/>TEXT<ex id="_1_ski_733"/>TEXT.</source>""")
     elem = lisa.xml_to_strelem(source)
 
     from copy import copy
     custom = StringElem([
-        StringElem(u'Text '),
-        G(u'g', id='_1_ski_040'),
-        StringElem(u'TEXT'),
+        StringElem('Text '),
+        G('g', id='_1_ski_040'),
+        StringElem('TEXT'),
         UnknownXML(
             [
-                StringElem(u'bpt'),
-                UnknownXML(u'sub', xml_node=copy(source[1][0])),
-                StringElem(u'\n               '),
+                StringElem('bpt'),
+                UnknownXML('sub', xml_node=copy(source[1][0])),
+                StringElem('\n               '),
             ],
             id='_1_ski_139',
             xml_node=copy(source[3])),
-        StringElem(u'TEXT'),
-        UnknownXML(u'ept', id=u'_1_ski_238', xml_node=copy(source[2])),
-        StringElem(u'TEXT'),
+        StringElem('TEXT'),
+        UnknownXML('ept', id='_1_ski_238', xml_node=copy(source[2])),
+        StringElem('TEXT'),
         UnknownXML(id='_1_ski_337', xml_node=copy(source[3])),  # ph-tag
-        StringElem(u'TEXT'),
-        UnknownXML(u'it', id='_1_ski_436', xml_node=copy(source[4])),
-        StringElem(u'TEXT'),
-        UnknownXML(u'mrk', xml_node=copy(source[5])),
-        StringElem(u'\n               '),
+        StringElem('TEXT'),
+        UnknownXML('it', id='_1_ski_436', xml_node=copy(source[4])),
+        StringElem('TEXT'),
+        UnknownXML('mrk', xml_node=copy(source[5])),
+        StringElem('\n               '),
         X(id='_1_ski_535'),
-        StringElem(u'TEXT'),
+        StringElem('TEXT'),
         Bx(id='_1_ski_634'),
-        StringElem(u'TEXT'),
+        StringElem('TEXT'),
         Ex(id='_1_ski_733'),
-        StringElem(u'TEXT.')
+        StringElem('TEXT.')
     ])
     assert elem == custom
 
--- .\storage\placeables\test_terminology.py	(original)
+++ .\storage\placeables\test_terminology.py	(refactored)
@@ -18,12 +18,13 @@
 # You should have received a copy of the GNU General Public License
 # along with this program; if not, see <http://www.gnu.org/licenses/>.
 
-from StringIO import StringIO
+from io import StringIO
 
 from translate.search.match import terminologymatcher
 from translate.storage.placeables import base, general, parse, StringElem
 from translate.storage.placeables.terminology import parsers as term_parsers, TerminologyPlaceable
 from translate.storage.pypo import pofile
+import collections
 
 
 class TestTerminologyPlaceable:
@@ -44,7 +45,7 @@
     def setup_method(self, method):
         self.term_po = pofile(StringIO(self.TERMINOLOGY))
         self.matcher = terminologymatcher(self.term_po)
-        self.test_string = u'<b>Inpt</b> file name thingy.'
+        self.test_string = '<b>Inpt</b> file name thingy.'
 
     def test_simple_terminology(self):
         TerminologyPlaceable.matchers = [self.matcher]
@@ -57,14 +58,14 @@
         term = tree.sub[3].sub[1]
 
         assert isinstance(term, TerminologyPlaceable)
-        assert unicode(term) == self.term_po.getunits()[2].source
-        assert term.translate() == unicode(self.term_po.getunits()[2].target)
+        assert str(term) == self.term_po.getunits()[2].source
+        assert term.translate() == str(self.term_po.getunits()[2].target)
 
 
 if __name__ == '__main__':
     for test in [TestTerminologyPlaceable()]:
         for method in dir(test):
-            if method.startswith('test_') and callable(getattr(test, method)):
+            if method.startswith('test_') and isinstance(getattr(test, method), collections.Callable):
                 if hasattr(test, 'setup_method'):
                     getattr(test, 'setup_method')(getattr(test, method))
                 getattr(test, method)()
--- .\storage\placeables\xliff.py	(original)
+++ .\storage\placeables\xliff.py	(refactored)
@@ -139,7 +139,7 @@
         base.X: X,
         base.Sub: Sub,
     }
-    for baseclass, xliffclass in classmap.items():
+    for baseclass, xliffclass in list(classmap.items()):
         if isinstance(tree, baseclass):
             newtree = xliffclass()
 
--- .\storage\versioncontrol\__init__.py	(original)
+++ .\storage\versioncontrol\__init__.py	(refactored)
@@ -34,6 +34,7 @@
 import os
 import re
 import subprocess
+import collections
 
 DEFAULT_RCS = ["svn", "cvs", "darcs", "git", "bzr", "hg"]
 """the names of all supported revision control systems
@@ -55,7 +56,7 @@
                     globals(), {}, name)
             # the module function "is_available" must return "True"
             if (hasattr(module, "is_available") and \
-                    callable(module.is_available) and \
+                    isinstance(module.is_available, collections.Callable) and \
                     module.is_available()):
                 # we found an appropriate module
                 rcs_class = getattr(module, name)
@@ -93,7 +94,7 @@
         (output, error) = proc.communicate()
         ret = proc.returncode
         return ret, output, error
-    except OSError, err_msg:
+    except OSError as err_msg:
         # failed to run the program (e.g. the executable was not found)
         return -1, "", err_msg
 
@@ -411,4 +412,4 @@
         import translate.storage.versioncontrol
         # print the names of locally available version control systems
         for rcs in get_available_version_control_systems():
-            print rcs
+            print(rcs)
--- .\storage\versioncontrol\cvs.py	(original)
+++ .\storage\versioncontrol\cvs.py	(refactored)
@@ -80,7 +80,7 @@
         # rename the file to be updated
         try:
             os.rename(filename, filename_backup)
-        except OSError, error:
+        except OSError as error:
             raise IOError("[CVS] could not move the file '%s' to '%s': %s" % \
                     (filename, filename_backup, error))
         command = ["cvs", "-Q", "update", "-C"]
--- .\storage\versioncontrol\darcs.py	(original)
+++ .\storage\versioncontrol\darcs.py	(refactored)
@@ -108,7 +108,7 @@
             darcs_file = open(filename)
             output = darcs_file.read()
             darcs_file.close()
-        except IOError, error:
+        except IOError as error:
             raise IOError("[Darcs] error reading original file '%s': %s" % \
                     (filename, error))
         return output
--- .\storage\versioncontrol\test_helper.py	(original)
+++ .\storage\versioncontrol\test_helper.py	(refactored)
@@ -38,7 +38,7 @@
     def create_files(self, files_dict):
         """Creates file(s) named after the keys, with contents from the values
         of the dictionary."""
-        for name, content in files_dict.iteritems():
+        for name, content in files_dict.items():
             assert not os.path.isabs(name)
             dirs = os.path.dirname(name)
             if dirs:
--- .\storage\versioncontrol\test_svn.py	(original)
+++ .\storage\versioncontrol\test_svn.py	(refactored)
@@ -13,7 +13,7 @@
         run_command(["svn", "co", "file:///%s/repo" % self.path, "checkout"], cwd=self.path)
 
     def test_detection(self):
-        print self.co_path
+        print(self.co_path)
         o = get_versioned_object(self.co_path)
         assert isinstance(o, svn.svn)
         assert o.location_abs == self.co_path
--- .\storage\xml_extract\extract.py	(original)
+++ .\storage\xml_extract\extract.py	(refactored)
@@ -40,7 +40,7 @@
     """A node corresponds to a translatable element. A node may
        have children, which correspond to placeables."""
 
-    @accepts(Self(), unicode, unicode, etree._Element, [IsOneOf(TranslatableClass, unicode)])
+    @accepts(Self(), str, str, etree._Element, [IsOneOf(TranslatableClass, str)])
     def __init__(self, placeable_name, xpath, dom_node, source):
         self.placeable_name = placeable_name
         self.source = source
@@ -68,7 +68,7 @@
         self.inline_elements = inline_elements
         self.is_inline = False
         self.xpath_breadcrumb = xpath_breadcrumb.XPathBreadcrumb()
-        self.placeable_name = u"<top-level>"
+        self.placeable_name = "<top-level>"
         self.nsmap = nsmap
 
 
@@ -80,7 +80,7 @@
     # no translatable is returned. Make a placeable with the name
     # "placeable"
     if len(placeable) == 0:
-        return Translatable(u"placeable", state.xpath_breadcrumb.xpath, dom_node, [])
+        return Translatable("placeable", state.xpath_breadcrumb.xpath, dom_node, [])
     # The ideal situation: we got exactly one translateable back
     # when processing this tree.
     elif len(placeable) == 1:
@@ -98,13 +98,13 @@
 
     source = []
     for child in dom_node:
-        source.extend([_process_placeable(child, state), unicode(child.tail or u"")])
+        source.extend([_process_placeable(child, state), str(child.tail or "")])
     return source
 
 
 @accepts(etree._Element, ParseState)
 def _process_translatable(dom_node, state):
-    source = [unicode(dom_node.text or u"")] + _process_placeables(dom_node, state)
+    source = [str(dom_node.text or "")] + _process_placeables(dom_node, state)
     translatable = Translatable(state.placeable_name, state.xpath_breadcrumb.xpath, dom_node, source)
     translatable.is_inline = state.is_inline
     return [translatable]
@@ -125,9 +125,9 @@
 
 def compact_tag(nsmap, namespace, tag):
     if namespace in nsmap:
-        return u'%s:%s' % (nsmap[namespace], tag)
+        return '%s:%s' % (nsmap[namespace], tag)
     else:
-        return u'{%s}%s' % (namespace, tag)
+        return '{%s}%s' % (namespace, tag)
 
 
 @accepts(etree._Element, ParseState)
@@ -192,10 +192,10 @@
 def _to_placeables(parent_translatable, translatable, id_maker):
     result = []
     for chunk in translatable.source:
-        if isinstance(chunk, unicode):
+        if isinstance(chunk, str):
             result.append(chunk)
         else:
-            id = unicode(id_maker.get_id(chunk))
+            id = str(id_maker.get_id(chunk))
             if chunk.is_inline:
                 result.append(xliff.G(sub=_to_placeables(parent_translatable, chunk, id_maker), id=id))
             else:
@@ -208,7 +208,7 @@
     """Construct a new translation unit, set its source and location
     information and add it to 'store'.
     """
-    unit = store.UnitClass(u'')
+    unit = store.UnitClass('')
     unit.rich_source = [StringElem(_to_placeables(parent_translatable, translatable, id_maker))]
     unit.addlocation(translatable.xpath)
     store.addunit(unit)
@@ -221,8 +221,8 @@
 
     If not, then there's nothing to translate."""
     for chunk in translatable.source:
-        if isinstance(chunk, unicode):
-            if chunk.strip() != u"":
+        if isinstance(chunk, str):
+            if chunk.strip() != "":
                 return True
     return False
 
@@ -254,7 +254,7 @@
 
 
 def reverse_map(a_map):
-    return dict((value, key) for key, value in a_map.iteritems())
+    return dict((value, key) for key, value in a_map.items())
 
 
 @accepts(lambda obj: hasattr(obj, "read"), base.TranslationStore, ParseState, Nullable(IsCallable()))
--- .\storage\xml_extract\generate.py	(original)
+++ .\storage\xml_extract\generate.py	(refactored)
@@ -53,7 +53,7 @@
 @accepts(etree._Element, unit_tree.XPathTree, IsCallable())
 def apply_translations(dom_node, unit_node, do_translate):
     tag_array = _get_tag_arrays(dom_node)
-    for unit_child_index, unit_child in unit_node.children.iteritems():
+    for unit_child_index, unit_child in unit_node.children.items():
         tag, index = unit_child_index
         try:
             dom_child = tag_array[XmlNamer(dom_node).name(tag)][index]
@@ -165,8 +165,8 @@
     def map_id_to_dom_node(parent_node, node, id_to_dom_node):
         # If this DOM node has an 'id' attribute, then add an id -> node
         # mapping to 'id_to_dom_node'.
-        if u'id' in node.attrib:
-            id_to_dom_node[node.attrib[u'id']] = node
+        if 'id' in node.attrib:
+            id_to_dom_node[node.attrib['id']] = node
         return id_to_dom_node
 
     # Build a mapping of id attributes to the DOM nodes which have these ids.
@@ -174,8 +174,8 @@
 
     def map_target_dom_to_source_dom_aux(parent_node, node, target_dom_to_source_dom):
         #
-        if u'id' in node.attrib and node.attrib[u'id'] in id_to_dom_node:
-            target_dom_to_source_dom[id_to_dom_node[node.attrib[u'id']]] = node
+        if 'id' in node.attrib and node.attrib['id'] in id_to_dom_node:
+            target_dom_to_source_dom[id_to_dom_node[node.attrib['id']]] = node
         return target_dom_to_source_dom
 
     # For each node in the DOM tree rooted at source_dom_node:
--- .\storage\xml_extract\misc.py	(original)
+++ .\storage\xml_extract\misc.py	(refactored)
@@ -55,7 +55,7 @@
     which have corresponding keys in right will have their keys mapped
     to values in right. """
     result_map = {}
-    for left_key, left_val in left.iteritems():
+    for left_key, left_val in left.items():
         try:
             result_map[left_key] = right[left_val]
         except KeyError:
@@ -72,6 +72,6 @@
     """
     match = tag_pattern.match(full_tag)
     if match is not None:
-        return unicode(match.groupdict()['namespace']), unicode(match.groupdict()['tag'])
+        return str(match.groupdict()['namespace']), str(match.groupdict()['tag'])
     else:
         raise Exception('Passed an invalid tag')
--- .\storage\xml_extract\test_misc.py	(original)
+++ .\storage\xml_extract\test_misc.py	(refactored)
@@ -23,10 +23,10 @@
 
 # reduce_tree
 
-test_tree_1 = (u'a',
-               [(u'b', []),
-                (u'c', [(u'd', []), (u'e', [])]),
-                (u'f', [(u'g', [(u'h', [])])])])
+test_tree_1 = ('a',
+               [('b', []),
+                ('c', [('d', []), ('e', [])]),
+                ('f', [('g', [('h', [])])])])
 
 test_tree_2 = (1,
                [(2, []),
@@ -43,7 +43,7 @@
     def concatenate(parent_node, node, string):
         return string + node[0]
 
-    assert u'abcdefgh' == misc.reduce_tree(concatenate, test_tree_1, test_tree_1, get_children, u'')
+    assert 'abcdefgh' == misc.reduce_tree(concatenate, test_tree_1, test_tree_1, get_children, '')
 
     def get_even_and_total(parent_node, node, even_lst, total):
         num = node[0]
@@ -55,8 +55,8 @@
 
 # compose_mappings
 
-left_mapping = {1: u'a', 2: u'b', 3: u'c', 4: u'd', 5: u'e'}
-right_mapping = {u'a': -1, u'b': -2, u'd': -4, u'e': -5, u'f': -6}
+left_mapping = {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}
+right_mapping = {'a': -1, 'b': -2, 'd': -4, 'e': -5, 'f': -6}
 
 composed_mapping = {1: -1, 2: -2, 4: -4, 5: -5}
 
@@ -68,8 +68,8 @@
 
 
 def test_parse_tag():
-    assert (u'some-urn', u'some-tag') == \
-        misc.parse_tag(u'{some-urn}some-tag')
+    assert ('some-urn', 'some-tag') == \
+        misc.parse_tag('{some-urn}some-tag')
 
-    assert (u'urn:oasis:names:tc:opendocument:xmlns:office:1.0', u'document-content') == \
-        misc.parse_tag(u'{urn:oasis:names:tc:opendocument:xmlns:office:1.0}document-content')
+    assert ('urn:oasis:names:tc:opendocument:xmlns:office:1.0', 'document-content') == \
+        misc.parse_tag('{urn:oasis:names:tc:opendocument:xmlns:office:1.0}document-content')
--- .\storage\xml_extract\test_unit_tree.py	(original)
+++ .\storage\xml_extract\test_unit_tree.py	(refactored)
@@ -26,14 +26,14 @@
 
 
 def test__split_xpath_component():
-    assert (u'some-tag', 0) == unit_tree._split_xpath_component(u'some-tag[0]')
+    assert ('some-tag', 0) == unit_tree._split_xpath_component('some-tag[0]')
 
 # _split_xpath
 
 
 def test__split_xpath():
-    assert [(u'p', 4), (u'text', 3), (u'body', 2), (u'document-content', 1)] == \
-        unit_tree._split_xpath(u'document-content[1]/body[2]/text[3]/p[4]')
+    assert [('p', 4), ('text', 3), ('body', 2), ('document-content', 1)] == \
+        unit_tree._split_xpath('document-content[1]/body[2]/text[3]/p[4]')
 
 # _add_unit_to_tree
 
@@ -42,17 +42,17 @@
     root = unit_tree.XPathTree()
     node = root
 
-    node.children[u'document-content', 1] = unit_tree.XPathTree()
-    node = node.children[u'document-content', 1]
+    node.children['document-content', 1] = unit_tree.XPathTree()
+    node = node.children['document-content', 1]
 
-    node.children[u'body', 1] = unit_tree.XPathTree()
-    node = node.children[u'body', 1]
+    node.children['body', 1] = unit_tree.XPathTree()
+    node = node.children['body', 1]
 
-    node.children[u'text', 1] = unit_tree.XPathTree()
-    node = node.children[u'text', 1]
+    node.children['text', 1] = unit_tree.XPathTree()
+    node = node.children['text', 1]
 
-    node.children[u'p', 1] = unit_tree.XPathTree()
-    node = node.children[u'p', 1]
+    node.children['p', 1] = unit_tree.XPathTree()
+    node = node.children['p', 1]
 
     node.unit = unit
 
@@ -61,16 +61,16 @@
 
 def make_tree_2(unit_1, unit_2):
     root = make_tree_1(unit_1)
-    node = root.children[u'document-content', 1]
+    node = root.children['document-content', 1]
 
-    node.children[u'body', 2] = unit_tree.XPathTree()
-    node = node.children[u'body', 2]
+    node.children['body', 2] = unit_tree.XPathTree()
+    node = node.children['body', 2]
 
-    node.children[u'text', 3] = unit_tree.XPathTree()
-    node = node.children[u'text', 3]
+    node.children['text', 3] = unit_tree.XPathTree()
+    node = node.children['text', 3]
 
-    node.children[u'p', 4] = unit_tree.XPathTree()
-    node = node.children[u'p', 4]
+    node.children['p', 4] = unit_tree.XPathTree()
+    node = node.children['p', 4]
 
     node.unit = unit_2
 
@@ -84,8 +84,8 @@
 
     # Add the first unit
 
-    unit_1 = xliff_file.UnitClass(u'Hello')
-    xpath_1 = u'document-content[1]/body[1]/text[1]/p[1]'
+    unit_1 = xliff_file.UnitClass('Hello')
+    xpath_1 = 'document-content[1]/body[1]/text[1]/p[1]'
 
     constructed_tree_1 = unit_tree.XPathTree()
     unit_tree._add_unit_to_tree(constructed_tree_1,
@@ -96,8 +96,8 @@
 
     # Add another unit
 
-    unit_2 = xliff_file.UnitClass(u'World')
-    xpath_2 = u'document-content[1]/body[2]/text[3]/p[4]'
+    unit_2 = xliff_file.UnitClass('World')
+    xpath_2 = 'document-content[1]/body[2]/text[3]/p[4]'
 
     constructed_tree_2 = make_tree_1(unit_1)
     unit_tree._add_unit_to_tree(constructed_tree_2,
--- .\storage\xml_extract\test_xpath_breadcrumb.py	(original)
+++ .\storage\xml_extract\test_xpath_breadcrumb.py	(refactored)
@@ -19,30 +19,30 @@
 # along with this program; if not, see <http://www.gnu.org/licenses/>.
 #
 
-import xpath_breadcrumb
+from . import xpath_breadcrumb
 
 
 def test_breadcrumb():
     xb = xpath_breadcrumb.XPathBreadcrumb()
-    assert xb.xpath == u''
+    assert xb.xpath == ''
 
-    xb.start_tag(u'a')
-    assert xb.xpath == u'a[0]'
+    xb.start_tag('a')
+    assert xb.xpath == 'a[0]'
 
-    xb.start_tag(u'b')
-    assert xb.xpath == u'a[0]/b[0]'
+    xb.start_tag('b')
+    assert xb.xpath == 'a[0]/b[0]'
     xb.end_tag()
 
-    assert xb.xpath == u'a[0]'
+    assert xb.xpath == 'a[0]'
 
-    xb.start_tag(u'b')
-    assert xb.xpath == u'a[0]/b[1]'
+    xb.start_tag('b')
+    assert xb.xpath == 'a[0]/b[1]'
     xb.end_tag()
 
-    assert xb.xpath == u'a[0]'
+    assert xb.xpath == 'a[0]'
     xb.end_tag()
 
-    assert xb.xpath == u''
+    assert xb.xpath == ''
 
-    xb.start_tag(u'a')
-    assert xb.xpath == u'a[1]'
+    xb.start_tag('a')
+    assert xb.xpath == 'a[1]'
--- .\storage\xml_extract\unit_tree.py	(original)
+++ .\storage\xml_extract\unit_tree.py	(refactored)
@@ -39,21 +39,21 @@
             self.children == other.children
 
 
-@accepts(unicode)
+@accepts(str)
 def _split_xpath_component(xpath_component):
     """Split an xpath component into a tag-index tuple.
 
     >>> split_xpath_component('{urn:oasis:names:tc:opendocument:xmlns:office:1.0}document-content[0]')
     ('{urn:oasis:names:tc:opendocument:xmlns:office:1.0}document-content', 0).
     """
-    lbrac = xpath_component.rfind(u'[')
-    rbrac = xpath_component.rfind(u']')
+    lbrac = xpath_component.rfind('[')
+    rbrac = xpath_component.rfind(']')
     tag = xpath_component[:lbrac]
     index = int(xpath_component[lbrac+1:rbrac])
     return tag, index
 
 
-@accepts(unicode)
+@accepts(str)
 def _split_xpath(xpath):
     """Split an 'xpath' string separated by / into a reversed list of its components. Thus:
 
@@ -65,12 +65,12 @@
     """
     if xliff.ID_SEPARATOR in xpath:
         xpath = xpath.split(xliff.ID_SEPARATOR)[-1]
-    components = xpath.split(u'/')
+    components = xpath.split('/')
     components = [_split_xpath_component(component) for component in components]
     return list(reversed(components))
 
 
-@accepts(IsOneOf(etree._Element, XPathTree), [(unicode, Number)], base.TranslationUnit)
+@accepts(IsOneOf(etree._Element, XPathTree), [(str, Number)], base.TranslationUnit)
 def _add_unit_to_tree(node, xpath_components, unit):
     """Walk down the tree rooted a node, and follow nodes which correspond to the
     components of xpath_components. When reaching the end of xpath_components,
--- .\storage\xml_extract\xpath_breadcrumb.py	(original)
+++ .\storage\xml_extract\xpath_breadcrumb.py	(refactored)
@@ -59,7 +59,7 @@
         self._xpath = []
         self._tagtally = [{}]
 
-    @accepts(Self(), unicode)
+    @accepts(Self(), str)
     def start_tag(self, tag):
         tally_dict = self._tagtally[-1]
         tally = tally_dict.get(tag, -1) + 1
@@ -75,7 +75,7 @@
 
         def str_component(component):
             tag, pos = component
-            return u"%s[%d]" % (tag, pos)
-        return u"/".join(str_component(component) for component in self._xpath)
+            return "%s[%d]" % (tag, pos)
+        return "/".join(str_component(component) for component in self._xpath)
 
     xpath = property(_get_xpath)
--- .\tools\build_tmdb.py	(original)
+++ .\tools\build_tmdb.py	(refactored)
@@ -51,15 +51,15 @@
     def handlefile(self, filename):
         try:
             store = factory.getobject(filename)
-        except Exception, e:
+        except Exception as e:
             logger.error(str(e))
             return
         # do something useful with the store and db
         try:
             self.tmdb.add_store(store, self.source_lang, self.target_lang, commit=False)
-        except Exception, e:
-            print e
-        print "File added:", filename
+        except Exception as e:
+            print(e)
+        print("File added:", filename)
 
     def handlefiles(self, dirname, filenames):
         for filename in filenames:
--- .\tools\pocompile.py	(original)
+++ .\tools\pocompile.py	(refactored)
@@ -30,7 +30,7 @@
 
 
 def _do_msgidcomment(string):
-    return u"_: %s\n" % string
+    return "_: %s\n" % string
 
 
 class POCompile:
--- .\tools\poconflicts.py	(original)
+++ .\tools\poconflicts.py	(refactored)
@@ -97,7 +97,7 @@
             fullinputpath = self.getfullinputpath(options, inputpath)
             try:
                 success = self.processfile(None, options, fullinputpath)
-            except Exception, error:
+            except Exception as error:
                 if isinstance(error, KeyboardInterrupt):
                     raise
                 self.warning("Error processing: input %s" % (fullinputpath), options, sys.exc_info())
@@ -146,7 +146,7 @@
     def buildconflictmap(self):
         """work out which strings are conflicting"""
         self.conflictmap = {}
-        for source, translations in self.textmap.iteritems():
+        for source, translations in self.textmap.items():
             source = self.flatten(source, " ")
             if len(source) <= 1:
                 continue
@@ -157,13 +157,13 @@
 
     def outputconflicts(self, options):
         """saves the result of the conflict match"""
-        print "%d/%d different strings have conflicts" % (len(self.conflictmap), len(self.textmap))
+        print("%d/%d different strings have conflicts" % (len(self.conflictmap), len(self.textmap)))
         reducedmap = {}
 
         def str_len(x):
             return len(x)
 
-        for source, translations in self.conflictmap.iteritems():
+        for source, translations in self.conflictmap.items():
             words = source.split()
             words.sort(key=str_len)
             source = words[-1]
@@ -173,9 +173,9 @@
         for word in reducedmap:
             if word + "s" in reducedmap:
                 plurals[word] = word + "s"
-        for word, pluralword in plurals.iteritems():
+        for word, pluralword in plurals.items():
             reducedmap[word].extend(reducedmap.pop(pluralword))
-        for source, translations in reducedmap.iteritems():
+        for source, translations in reducedmap.items():
             flatsource = self.flatten(source, "-")
             fulloutputpath = os.path.join(options.output, flatsource + os.extsep + "po")
             conflictfile = po.pofile()
--- .\tools\pocount.py	(original)
+++ .\tools\pocount.py	(refactored)
@@ -37,7 +37,7 @@
 logger = logging.getLogger(__name__)
 
 # define style constants
-style_full, style_csv, style_short_strings, style_short_words = range(4)
+style_full, style_csv, style_short_strings, style_short_words = list(range(4))
 
 # default output style
 default_style = style_full
@@ -49,17 +49,17 @@
     # ignore totally blank or header units
     try:
         store = factory.getobject(filename)
-    except ValueError, e:
+    except ValueError as e:
         logger.warning(e)
         return {}
-    units = filter(lambda unit: unit.istranslatable(), store.units)
+    units = [unit for unit in store.units if unit.istranslatable()]
     translated = translatedmessages(units)
     fuzzy = fuzzymessages(units)
-    review = filter(lambda unit: unit.isreview(), units)
+    review = [unit for unit in units if unit.isreview()]
     untranslated = untranslatedmessages(units)
-    wordcounts = dict(map(lambda unit: (unit, statsdb.wordsinunit(unit)), units))
-    sourcewords = lambda elementlist: sum(map(lambda unit: wordcounts[unit][0], elementlist))
-    targetwords = lambda elementlist: sum(map(lambda unit: wordcounts[unit][1], elementlist))
+    wordcounts = dict([(unit, statsdb.wordsinunit(unit)) for unit in units])
+    sourcewords = lambda elementlist: sum([wordcounts[unit][0] for unit in elementlist])
+    targetwords = lambda elementlist: sum([wordcounts[unit][1] for unit in elementlist])
     stats = {}
 
     #units
@@ -111,79 +111,79 @@
         return 1
 
     if (style == style_csv):
-        print "%s, " % title,
-        print "%d, %d, %d," % (stats["translated"],
+        print("%s, " % title, end=' ')
+        print("%d, %d, %d," % (stats["translated"],
                                stats["translatedsourcewords"],
-                               stats["translatedtargetwords"]),
-        print "%d, %d," % (stats["fuzzy"], stats["fuzzysourcewords"]),
-        print "%d, %d," % (stats["untranslated"],
-                           stats["untranslatedsourcewords"]),
-        print "%d, %d" % (stats["total"], stats["totalsourcewords"]),
+                               stats["translatedtargetwords"]), end=' ')
+        print("%d, %d," % (stats["fuzzy"], stats["fuzzysourcewords"]), end=' ')
+        print("%d, %d," % (stats["untranslated"],
+                           stats["untranslatedsourcewords"]), end=' ')
+        print("%d, %d" % (stats["total"], stats["totalsourcewords"]), end=' ')
         if stats["review"] > 0:
-            print ", %d, %d" % (stats["review"], stats["reviewsourdcewords"]),
-        print
+            print(", %d, %d" % (stats["review"], stats["reviewsourdcewords"]), end=' ')
+        print()
     elif (style == style_short_strings):
         spaces = " " * (indent - len(title))
-        print "%s%s strings: total: %d\t| %dt\t%df\t%du\t| %d%%t\t%d%%f\t%d%%u" % (title, spaces, \
+        print("%s%s strings: total: %d\t| %dt\t%df\t%du\t| %d%%t\t%d%%f\t%d%%u" % (title, spaces, \
               stats["total"], stats["translated"], stats["fuzzy"], stats["untranslated"], \
               percent(stats["translated"], stats["total"]), \
               percent(stats["fuzzy"], stats["total"]), \
-              percent(stats["untranslated"], stats["total"]))
+              percent(stats["untranslated"], stats["total"])))
     elif (style == style_short_words):
         spaces = " " * (indent - len(title))
-        print "%s%s source words: total: %d\t| %dt\t%df\t%du\t| %d%%t\t%d%%f\t%d%%u" % (title, spaces, \
+        print("%s%s source words: total: %d\t| %dt\t%df\t%du\t| %d%%t\t%d%%f\t%d%%u" % (title, spaces, \
               stats["totalsourcewords"], stats["translatedsourcewords"], stats["fuzzysourcewords"], stats["untranslatedsourcewords"], \
               percent(stats["translatedsourcewords"], stats["totalsourcewords"]), \
               percent(stats["fuzzysourcewords"], stats["totalsourcewords"]), \
-              percent(stats["untranslatedsourcewords"], stats["totalsourcewords"]))
+              percent(stats["untranslatedsourcewords"], stats["totalsourcewords"])))
     else:  # style == style_full
-        print title
-        print "type              strings      words (source)    words (translation)"
-        print "translated:   %5d (%3d%%) %10d (%3d%%) %15d" % \
+        print(title)
+        print("type              strings      words (source)    words (translation)")
+        print("translated:   %5d (%3d%%) %10d (%3d%%) %15d" % \
                 (stats["translated"], \
                 percent(stats["translated"], stats["total"]), \
                 stats["translatedsourcewords"], \
                 percent(stats["translatedsourcewords"], stats["totalsourcewords"]), \
-                stats["translatedtargetwords"])
-        print "fuzzy:        %5d (%3d%%) %10d (%3d%%)             n/a" % \
+                stats["translatedtargetwords"]))
+        print("fuzzy:        %5d (%3d%%) %10d (%3d%%)             n/a" % \
                 (stats["fuzzy"], \
                 percent(stats["fuzzy"], stats["total"]), \
                 stats["fuzzysourcewords"], \
-                percent(stats["fuzzysourcewords"], stats["totalsourcewords"]))
-        print "untranslated: %5d (%3d%%) %10d (%3d%%)             n/a" % \
+                percent(stats["fuzzysourcewords"], stats["totalsourcewords"])))
+        print("untranslated: %5d (%3d%%) %10d (%3d%%)             n/a" % \
                 (stats["untranslated"], \
                 percent(stats["untranslated"], stats["total"]), \
                 stats["untranslatedsourcewords"], \
-                percent(stats["untranslatedsourcewords"], stats["totalsourcewords"]))
-        print "Total:        %5d %17d %22d" % \
+                percent(stats["untranslatedsourcewords"], stats["totalsourcewords"])))
+        print("Total:        %5d %17d %22d" % \
                 (stats["total"], \
                 stats["totalsourcewords"], \
-                stats["translatedtargetwords"])
+                stats["translatedtargetwords"]))
         if "extended" in stats:
-            print ""
-            for state, e_stats in stats["extended"].iteritems():
-                print "%s:    %5d (%3d%%) %10d (%3d%%) %15d" % (
+            print("")
+            for state, e_stats in stats["extended"].items():
+                print("%s:    %5d (%3d%%) %10d (%3d%%) %15d" % (
                     state, e_stats["units"], percent(e_stats["units"], stats["total"]),
                     e_stats["sourcewords"], percent(e_stats["sourcewords"], stats["totalsourcewords"]),
-                    e_stats["targetwords"])
+                    e_stats["targetwords"]))
 
         if stats["review"] > 0:
-            print "review:       %5d %17d                    n/a" % \
-                    (stats["review"], stats["reviewsourcewords"])
-        print
+            print("review:       %5d %17d                    n/a" % \
+                    (stats["review"], stats["reviewsourcewords"]))
+        print()
     return 0
 
 
 def fuzzymessages(units):
-    return filter(lambda unit: unit.isfuzzy() and unit.target, units)
+    return [unit for unit in units if unit.isfuzzy() and unit.target]
 
 
 def translatedmessages(units):
-    return filter(lambda unit: unit.istranslated(), units)
+    return [unit for unit in units if unit.istranslated()]
 
 
 def untranslatedmessages(units):
-    return filter(lambda unit: not (unit.istranslated() or unit.isfuzzy()) and unit.source, units)
+    return [unit for unit in units if not (unit.istranslated() or unit.isfuzzy()) and unit.source]
 
 
 class summarizer:
@@ -197,10 +197,10 @@
         self.complete_count = 0
 
         if (self.style == style_csv):
-            print "Filename, Translated Messages, Translated Source Words, Translated \
+            print("Filename, Translated Messages, Translated Source Words, Translated \
 Target Words, Fuzzy Messages, Fuzzy Source Words, Untranslated Messages, \
 Untranslated Source Words, Total Message, Total Source Words, \
-Review Messages, Review Source Words"
+Review Messages, Review Source Words")
         if (self.style == style_short_strings or self.style == style_short_words):
             for filename in filenames:  # find longest filename
                 if (len(filename) > self.longestfilename):
@@ -217,15 +217,15 @@
             if self.incomplete_only:
                 summarize("TOTAL (incomplete only):", self.totals,
                 incomplete_only=True)
-                print "File count (incomplete):   %5d" % (self.filecount - self.complete_count)
+                print("File count (incomplete):   %5d" % (self.filecount - self.complete_count))
             else:
                 summarize("TOTAL:", self.totals, incomplete_only=False)
-            print "File count:   %5d" % (self.filecount)
-            print
+            print("File count:   %5d" % (self.filecount))
+            print()
 
     def updatetotals(self, stats):
         """Update self.totals with the statistics in stats."""
-        for key in stats.keys():
+        for key in list(stats.keys()):
             if key == "extended":
                 #FIXME: calculate extended totals
                 continue
--- .\tools\podebug.py	(original)
+++ .\tools\podebug.py	(refactored)
@@ -73,24 +73,24 @@
         if not isinstance(string, StringElem):
             string = StringElem(string)
         string.sub.insert(0, prepend)
-        if unicode(string).endswith(u'\n'):
+        if str(string).endswith('\n'):
             # Try and remove the last character from the tree
             try:
                 lastnode = string.flatten()[-1]
-                if isinstance(lastnode.sub[-1], unicode):
-                    lastnode.sub[-1] = lastnode.sub[-1].rstrip(u'\n')
+                if isinstance(lastnode.sub[-1], str):
+                    lastnode.sub[-1] = lastnode.sub[-1].rstrip('\n')
             except IndexError:
                 pass
-            string.sub.append(append + u'\n')
+            string.sub.append(append + '\n')
         else:
             string.sub.append(append)
         return string
 
     def rewrite_xxx(self, string):
-        return self._rewrite_prepend_append(string, u"xxx")
+        return self._rewrite_prepend_append(string, "xxx")
 
     def rewrite_bracket(self, string):
-        return self._rewrite_prepend_append(string, u"[", u"]")
+        return self._rewrite_prepend_append(string, "[", "]")
 
     def rewrite_en(self, string):
         if not isinstance(string, StringElem):
@@ -98,7 +98,7 @@
         return string
 
     def rewrite_blank(self, string):
-        return StringElem(u"")
+        return StringElem("")
 
     def rewrite_chef(self, string):
         """Rewrite using Mock Swedish as made famous by Monty Python"""
@@ -137,7 +137,7 @@
             self.apply_to_translatables(string, lambda s: re.sub(a, b, s))
         return string
 
--- .\tools\pogrep.py	(original)
+++ .\tools\pogrep.py	(refactored)
@@ -138,7 +138,7 @@
             invertmatch=False, keeptranslations=False, accelchar=None, encoding='utf-8',
             max_matches=0):
         """builds a checkfilter using the given checker"""
-        if isinstance(searchstring, unicode):
+        if isinstance(searchstring, str):
             self.searchstring = searchstring
         else:
             self.searchstring = searchstring.decode(encoding)
@@ -213,7 +213,7 @@
             if self.matches(unit.getnotes()):
                 return True
         if self.search_locations:
-            if self.matches(u" ".join(unit.getlocations())):
+            if self.matches(" ".join(unit.getlocations())):
                 return True
         return False
 
@@ -241,7 +241,7 @@
             flags |= re.IGNORECASE
         if not self.useregexp:
             searchstring = re.escape(searchstring)
-        self.re_search = re.compile(u'(%s)' % (searchstring), flags)
+        self.re_search = re.compile('(%s)' % (searchstring), flags)
 
         matches = []
         indexes = []
--- .\tools\porestructure.py	(original)
+++ .\tools\porestructure.py	(refactored)
@@ -77,7 +77,7 @@
             fullinputpath = self.getfullinputpath(options, inputpath)
             try:
                 success = self.processfile(options, fullinputpath)
-            except Exception, error:
+            except Exception as error:
                 if isinstance(error, KeyboardInterrupt):
                     raise self.warning("Error processing: input %s" % (fullinputpath), options, sys.exc_info())
                 success = False
--- .\tools\poterminology.py	(original)
+++ .\tools\poterminology.py	(refactored)
@@ -38,9 +38,9 @@
     termunit = po.pounit(term)
     if unit is not None:
         termunit.merge(unit, overwrite=False, comments=False)
-    if len(targets.keys()) > 1:
+    if len(list(targets.keys())) > 1:
         txt = '; '.join(["%s {%s}" % (target, ', '.join(files))
-                         for target, files in targets.iteritems()])
+                         for target, files in targets.items()])
         if termunit.target.find('};') < 0:
             termunit.target = txt
             termunit.markfuzzy()
@@ -53,7 +53,7 @@
         termunit.addnote(sourcenote, "developer")
     for transnote in transnotes:
         termunit.addnote(transnote, "translator")
-    for filename, count in filecounts.iteritems():
+    for filename, count in filecounts.items():
         termunit.addnote("(poterminology) %s (%d)\n" % (filename, count), 'translator')
     return termunit
 
@@ -125,7 +125,7 @@
                     self.stoprelist.append(re.compile(stopline[1:-1] + '$'))
                 else:
                     self.stopwords[stopline[1:-1]] = actions[stoptype]
-        except KeyError, character:
+        except KeyError as character:
             logger.warning("%s:%d - bad stopword entry starts with '%s'",
                            self.stopfile, line, str(character))
             logger.warning("%s:%d all lines after error ignored",
@@ -240,7 +240,7 @@
         terms = {}
         locre = re.compile(r":[0-9]+$")
         logger.info("%d terms from %d units", len(self.glossary), self.units)
-        for term, translations in self.glossary.iteritems():
+        for term, translations in self.glossary.items():
             if len(translations) <= 1:
                 continue
             filecounts = {}
@@ -299,7 +299,7 @@
     def filter_terms(self, terms, nonstopmin=1, sortorders=sortorders_default):
         """reduce subphrases from extracted terms"""
         # reduce subphrase
-        termlist = terms.keys()
+        termlist = list(terms.keys())
         logger.info("%d terms after thresholding", len(termlist))
         termlist.sort(lambda x, y: cmp(len(x), len(y)))
         for term in termlist:
@@ -319,8 +319,8 @@
                 words.pop(0)
                 if terms[term][0] == terms.get(' '.join(words), [0])[0]:
                     del terms[' '.join(words)]
-        logger.info("%d terms after subphrase reduction", len(terms.keys()))
-        termitems = terms.values()
+        logger.info("%d terms after subphrase reduction", len(list(terms.keys())))
+        termitems = list(terms.values())
         if sortorders is None:
             sortorders = self.sortorders_default
         while len(sortorders) > 0:
@@ -427,7 +427,7 @@
             success = True
             try:
                 self.processfile(None, options, fullinputpath)
-            except Exception, error:
+            except Exception as error:
                 if isinstance(error, KeyboardInterrupt):
                     raise
                 self.warning("Error processing: input %s" % (fullinputpath), options, sys.exc_info())
--- .\tools\pydiff.py	(original)
+++ .\tools\pydiff.py	(refactored)
@@ -113,7 +113,7 @@
         """writes the actual diff to the given file"""
         fromfiles = os.listdir(self.fromdir)
         tofiles = os.listdir(self.todir)
-        difffiles = dict.fromkeys(fromfiles + tofiles).keys()
+        difffiles = list(dict.fromkeys(fromfiles + tofiles).keys())
         difffiles.sort()
         for difffile in difffiles:
             if self.isexcluded(difffile):
--- .\tools\test_pocount.py	(original)
+++ .\tools\test_pocount.py	(refactored)
@@ -1,7 +1,7 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 
-import StringIO
+import io
 from translate.tools import pocount
 
 from pytest import mark
@@ -18,10 +18,10 @@
         if target is not None:
             poelement.target = target
         wordssource, wordstarget = statsdb.wordsinunit(poelement)
-        print 'Source (expected=%d; actual=%d): "%s"' % (expectedsource, wordssource, source)
+        print('Source (expected=%d; actual=%d): "%s"' % (expectedsource, wordssource, source))
         assert wordssource == expectedsource
         if target is not None:
-            print 'Target (expected=%d; actual=%d): "%s"' % (expectedtarget, wordstarget, target)
+            print('Target (expected=%d; actual=%d): "%s"' % (expectedtarget, wordstarget, target))
             assert wordstarget == expectedtarget
 
     def test_simple_count_zero(self):
@@ -119,41 +119,41 @@
 '''
 
     def test_translated(self):
-        pofile = StringIO.StringIO(self.inputdata)
+        pofile = io.StringIO(self.inputdata)
         stats = pocount.calcstats_old(pofile)
         assert stats['translated'] == 1
 
     def test_fuzzy(self):
-        pofile = StringIO.StringIO(self.inputdata)
+        pofile = io.StringIO(self.inputdata)
         stats = pocount.calcstats_old(pofile)
         assert stats['fuzzy'] == 1
 
     def test_untranslated(self):
-        pofile = StringIO.StringIO(self.inputdata)
+        pofile = io.StringIO(self.inputdata)
         stats = pocount.calcstats_old(pofile)
         assert stats['untranslated'] == 1
 
     def test_total(self):
-        pofile = StringIO.StringIO(self.inputdata)
+        pofile = io.StringIO(self.inputdata)
         stats = pocount.calcstats_old(pofile)
         assert stats['total'] == 3
 
     def test_translatedsourcewords(self):
-        pofile = StringIO.StringIO(self.inputdata)
+        pofile = io.StringIO(self.inputdata)
         stats = pocount.calcstats_old(pofile)
         assert stats['translatedsourcewords'] == 2
 
     def test_fuzzysourcewords(self):
-        pofile = StringIO.StringIO(self.inputdata)
+        pofile = io.StringIO(self.inputdata)
         stats = pocount.calcstats_old(pofile)
         assert stats['fuzzysourcewords'] == 2
 
     def test_untranslatedsourcewords(self):
-        pofile = StringIO.StringIO(self.inputdata)
+        pofile = io.StringIO(self.inputdata)
         stats = pocount.calcstats_old(pofile)
         assert stats['untranslatedsourcewords'] == 2
 
     def test_totalsourcewords(self):
-        pofile = StringIO.StringIO(self.inputdata)
+        pofile = io.StringIO(self.inputdata)
         stats = pocount.calcstats_old(pofile)
         assert stats['totalsourcewords'] == 6
--- .\tools\test_podebug.py	(original)
+++ .\tools\test_podebug.py	(refactored)
@@ -35,48 +35,48 @@
 
     def test_keep_target(self):
         """Test that we use the target for rewriting if it exists."""
-        unit = base.TranslationUnit(u"blie")
+        unit = base.TranslationUnit("blie")
 
-        unit.target = u"bla"
+        unit.target = "bla"
         debugger = podebug.podebug(rewritestyle="xxx")
         unit = debugger.convertunit(unit, "")
-        assert unit.target == u"xxxblaxxx"
+        assert unit.target == "xxxblaxxx"
 
-        unit.target = u"d%d"
+        unit.target = "d%d"
         debugger = podebug.podebug(rewritestyle="flipped")
         unit = debugger.convertunit(unit, "")
-        assert unit.target == u"\u202ep%d"
+        assert unit.target == "\u202ep%d"
 
     def test_rewrite_blank(self):
         """Test the blank rewrite function"""
-        assert str(self.debug.rewrite_blank(u"Test")) == u""
+        assert str(self.debug.rewrite_blank("Test")) == ""
 
     def test_rewrite_en(self):
         """Test the en rewrite function"""
-        assert str(self.debug.rewrite_en(u"Test")) == u"Test"
+        assert str(self.debug.rewrite_en("Test")) == "Test"
 
     def test_rewrite_xxx(self):
         """Test the xxx rewrite function"""
-        assert str(self.debug.rewrite_xxx(u"Test")) == u"xxxTestxxx"
-        assert str(self.debug.rewrite_xxx(u"Newline\n")) == u"xxxNewlinexxx\n"
+        assert str(self.debug.rewrite_xxx("Test")) == "xxxTestxxx"
+        assert str(self.debug.rewrite_xxx("Newline\n")) == "xxxNewlinexxx\n"
 
     def test_rewrite_bracket(self):
         """Test the bracket rewrite function"""
-        assert str(self.debug.rewrite_bracket(u"Test")) == u"[Test]"
-        assert str(self.debug.rewrite_bracket(u"Newline\n")) == u"[Newline]\n"
+        assert str(self.debug.rewrite_bracket("Test")) == "[Test]"
+        assert str(self.debug.rewrite_bracket("Newline\n")) == "[Newline]\n"
 
     def test_rewrite_unicode(self):
         """Test the unicode rewrite function"""
--- .\tools\test_pogrep.py	(original)
+++ .\tools\test_pogrep.py	(refactored)
@@ -23,7 +23,7 @@
         options, args = pogrep.cmdlineparser().parse_args(["xxx.po"] + cmdlineoptions)
         grepfilter = pogrep.GrepFilter(searchstring, options.searchparts, options.ignorecase, options.useregexp, options.invertmatch, options.keeptranslations, options.accelchar)
         tofile = grepfilter.filterfile(self.poparse(posource))
-        print str(tofile)
+        print(str(tofile))
         return str(tofile)
 
     def test_simplegrep_msgid(self):
@@ -78,7 +78,7 @@
                                          (poascii, queryunicode, ''),
                                          (pounicode, queryascii, ''),
                                          (pounicode, queryunicode, pounicode)]:
-            print "Source:\n%s\nSearch: %s\n" % (source, search)
+            print("Source:\n%s\nSearch: %s\n" % (source, search))
             poresult = self.pogrep(source, search)
             assert poresult.index(expected) >= 0
 
@@ -92,7 +92,7 @@
                                          (poascii, queryunicode, ''),
                                          (pounicode, queryascii, ''),
                                          (pounicode, queryunicode, pounicode)]:
-            print "Source:\n%s\nSearch: %s\n" % (source, search)
+            print("Source:\n%s\nSearch: %s\n" % (source, search))
             poresult = self.pogrep(source, search, ["--regexp"])
             assert poresult.index(expected) >= 0
 
@@ -106,18 +106,18 @@
 
     def test_unicode_normalise(self):
         """check that we normlise unicode strings before comparing"""
-        source_template = u'# comment\n#: test.c\nmsgid "test"\nmsgstr "t%sst"\n'
+        source_template = '# comment\n#: test.c\nmsgid "test"\nmsgstr "t%sst"\n'
         # , e + '
--- .\tools\test_pomerge.py	(original)
+++ .\tools\test_pomerge.py	(refactored)
@@ -62,8 +62,8 @@
                                   mergefuzzy=mergefuzzy,
                                   mergecomments=mergecomments)
         outputxliffstring = outputfile.getvalue()
-        print "Generated XML:"
-        print outputxliffstring
+        print("Generated XML:")
+        print(outputxliffstring)
         outputxlifffile = xliff.xlifffile(outputxliffstring)
         return outputxlifffile
 
@@ -154,7 +154,7 @@
         inputpo = '''#: location.c:1\n#: location.c:2\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n'''
         expectedpo = '''#: location.c:1%slocation.c:2\nmsgid "Simple String"\nmsgstr "Dimpled Ring"\n''' % po.lsep
         pofile = self.mergestore(templatepo, inputpo)
-        print pofile
+        print(pofile)
         assert str(pofile) == expectedpo
 
     def test_unit_missing_in_template_with_locations(self):
@@ -174,7 +174,7 @@
 msgstr "Dimpled Ring"
 '''
         pofile = self.mergestore(templatepo, inputpo)
-        print pofile
+        print(pofile)
         assert str(pofile) == expectedpo
 
     def test_unit_missing_in_template_no_locations(self):
@@ -190,7 +190,7 @@
 msgstr "Dimpled Ring"
 '''
         pofile = self.mergestore(templatepo, inputpo)
-        print pofile
+        print(pofile)
         assert str(pofile) == expectedpo
 
     def test_reflowed_source_comments(self):
@@ -201,7 +201,7 @@
         expectedpo = '''#: newMenu.label%snewMenu.accesskey\nmsgid "&New"\nmsgstr "&Nuwe"\n''' % po.lsep
         pofile = self.mergestore(templatepo, newpo)
         pounit = self.singleunit(pofile)
-        print pofile
+        print(pofile)
         assert str(pofile) == expectedpo
 
     def test_comments_with_blank_lines(self):
@@ -217,7 +217,7 @@
         expectedpo = templatepo
         pofile = self.mergestore(templatepo, newpo)
         pounit = self.singleunit(pofile)
-        print pofile
+        print(pofile)
         assert str(pofile) == expectedpo
 
     def test_merge_dont_delete_unassociated_comments(self):
@@ -228,7 +228,7 @@
         expectedpo = '''# Lonely comment\n# Translation comment\nmsgid "Bob"\nmsgstr "Builder"\n'''
         pofile = self.mergestore(templatepo, mergepo)
 #        pounit = self.singleunit(pofile)
-        print pofile
+        print(pofile)
         assert str(pofile) == expectedpo
 
     def test_preserve_format_trailing_newlines(self):
@@ -237,7 +237,7 @@
         mergepo = '''msgid "Simple string\\n"\nmsgstr "Dimpled ring\\n"\n'''
         expectedpo = '''msgid "Simple string\\n"\nmsgstr "Dimpled ring\\n"\n'''
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
         templatepo = '''msgid ""\n"Simple string\\n"\nmsgstr ""\n'''
@@ -245,7 +245,7 @@
         expectedpo = '''msgid ""\n"Simple string\\n"\nmsgstr "Dimpled ring\\n"\n'''
         expectedpo2 = '''msgid "Simple string\\n"\nmsgstr "Dimpled ring\\n"\n'''
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo or str(pofile) == expectedpo2
 
     def test_preserve_format_minor_start_and_end_of_sentence_changes(self):
@@ -255,21 +255,21 @@
         mergepo = '''msgid "Target type:"\nmsgstr "Doelsoort:"\n'''
         expectedpo = mergepo
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
         templatepo = '''msgid "&Select"\nmsgstr "Kies"\n\n'''
         mergepo = '''msgid "&Select"\nmsgstr "&Kies"\n'''
         expectedpo = mergepo
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
         templatepo = '''msgid "en-us, en"\nmsgstr "en-us, en"\n'''
         mergepo = '''msgid "en-us, en"\nmsgstr "af-za, af, en-za, en-gb, en-us, en"\n'''
         expectedpo = mergepo
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
     def test_preserve_format_last_entry_in_a_file(self):
@@ -279,14 +279,14 @@
         mergepo = '''msgid "First"\nmsgstr "Eerste"\n\nmsgid "Second"\nmsgstr "Tweede"\n'''
         expectedpo = '''msgid "First"\nmsgstr "Eerste"\n\nmsgid "Second"\nmsgstr "Tweede"\n'''
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
         templatepo = '''msgid "First"\nmsgstr ""\n\nmsgid "Second"\nmsgstr ""\n\n'''
         mergepo = '''msgid "First"\nmsgstr "Eerste"\n\nmsgid "Second"\nmsgstr "Tweede"\n'''
         expectedpo = '''msgid "First"\nmsgstr "Eerste"\n\nmsgid "Second"\nmsgstr "Tweede"\n'''
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
     @mark.xfail(reason="Not Implemented")
@@ -301,7 +301,7 @@
 msgstr "Eerste\tTweede"
 '''
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
     def test_preserve_comments_layout(self):
@@ -311,7 +311,7 @@
         mergepo = '''# (pofilter) unchanged: please translate\n#: filename\nmsgid "Desktop Background.bmp"\nmsgstr "Desktop Background.bmp"\n'''
         expectedpo = mergepo
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
     def test_merge_dos2unix(self):
@@ -382,12 +382,12 @@
         mergepo = '''msgid "_: KDE comment\\n"\n"File"\nmsgstr "_: KDE comment\\n"\n"Ifayile"\n\n'''
         expectedpo = '''msgid ""\n"_: KDE comment\\n"\n"File"\nmsgstr "Ifayile"\n'''
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
         # Translated kde comment.
         mergepo = '''msgid "_: KDE comment\\n"\n"File"\nmsgstr "_: KDE kommentaar\\n"\n"Ifayile"\n\n'''
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
         # multiline KDE comment
@@ -395,7 +395,7 @@
         mergepo = '''msgid "_: KDE "\n"comment\\n"\n"File"\nmsgstr "_: KDE "\n"comment\\n"\n"Ifayile"\n\n'''
         expectedpo = '''msgid ""\n"_: KDE comment\\n"\n"File"\nmsgstr "Ifayile"\n'''
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n\nMerged:\n%s" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
     def test_merging_untranslated_with_kde_disambiguation(self):
@@ -427,7 +427,7 @@
 ''' % (po.lsep, po.lsep)
         expectedpo = mergepo
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n---\nMerged:\n%s\n---" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n---\nMerged:\n%s\n---" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
     def test_merging_header_entries(self):
@@ -490,7 +490,7 @@
 msgstr "Dimpled Ring"
 '''
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n---\nMerged:\n%s\n---" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n---\nMerged:\n%s\n---" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo
 
     def test_merging_different_locations(self):
@@ -547,5 +547,5 @@
 
         expectedpo = mergepo
         pofile = self.mergestore(templatepo, mergepo)
-        print "Expected:\n%s\n---\nMerged:\n%s\n---" % (expectedpo, str(pofile))
+        print("Expected:\n%s\n---\nMerged:\n%s\n---" % (expectedpo, str(pofile)))
         assert str(pofile) == expectedpo or str(pofile) == expectedpo2
--- .\tools\test_pretranslate.py	(original)
+++ .\tools\test_pretranslate.py	(refactored)
@@ -58,10 +58,10 @@
         """checks that the pofile contains a single non-header unit, and
         returns it"""
         if len(pofile.units) == 2 and  pofile.units[0].isheader():
-            print pofile.units[1]
+            print(pofile.units[1])
             return pofile.units[1]
         else:
-            print pofile.units[0]
+            print(pofile.units[0])
             return pofile.units[0]
 
     def test_pretranslatepo_blank(self):
@@ -122,7 +122,7 @@
         template_source = '''#: simple.label\n#: simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n'''
         poexpected = '''#: simple.label\n#: simple.accesskey\n#, fuzzy\nmsgid "Its &hard coding a newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n'''
         newpo = self.pretranslatepo(input_source, template_source)
-        print newpo
+        print(newpo)
         assert str(newpo) == poexpected
 
     def test_merging_location_change(self):
@@ -132,7 +132,7 @@
         template_source = '''#: simple.label%ssimple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
         poexpected = '''#: new_simple.label%snew_simple.accesskey\nmsgid "A &hard coded newline.\\n"\nmsgstr "&Hart gekoeerde nuwe lyne\\n"\n''' % po.lsep
         newpo = self.pretranslatepo(input_source, template_source)
-        print newpo
+        print(newpo)
         assert str(newpo) == poexpected
 
     def test_merging_location_and_whitespace_change(self):
@@ -142,7 +142,7 @@
         template_source = '''#: doublespace.label%sdoublespace.accesskey\nmsgid "&We  have  spaces"\nmsgstr "&One  het  spasies"\n''' % po.lsep
         poexpected = '''#: singlespace.label%ssinglespace.accesskey\n#, fuzzy\nmsgid "&We have spaces"\nmsgstr "&One  het  spasies"\n''' % po.lsep
         newpo = self.pretranslatepo(input_source, template_source)
-        print newpo
+        print(newpo)
         assert str(newpo) == poexpected
 
     @mark.xfail(reason="Not Implemented")
@@ -153,7 +153,7 @@
         template_source = '''#: someline.c\nmsgid "&About"\nmsgstr "&Info"\n'''
         poexpected = '''#: someline.c\nmsgid "A&bout"\nmsgstr "&Info"\n'''
         newpo = self.pretranslatepo(input_source, template_source)
-        print newpo
+        print(newpo)
         assert str(newpo) == poexpected
 
     @mark.xfail(reason="Not Implemented")
@@ -209,10 +209,10 @@
         poexpected = template_source
         newpo = self.pretranslatepo(input_source, template_source)
         newpounit = self.singleunit(newpo)
-        print "expected"
-        print poexpected
-        print "got:"
-        print str(newpounit)
+        print("expected")
+        print(poexpected)
+        print("got:")
+        print(str(newpounit))
         assert str(newpounit) == poexpected
 
     def test_merging_msgidcomments(self):
@@ -238,7 +238,7 @@
         input_source = '''msgid "One"\nmsgid_plural "Two"\nmsgstr[0] ""\nmsgstr[1] ""\n'''
         template_source = '''msgid "One"\nmsgid_plural "Two"\nmsgstr[0] "Een"\nmsgstr[1] "Twee"\nmsgstr[2] "Drie"\n'''
         newpo = self.pretranslatepo(input_source, template_source)
-        print newpo
+        print(newpo)
         newpounit = self.singleunit(newpo)
         assert str(newpounit) == template_source
 
@@ -249,7 +249,7 @@
         template_source = '''#~ msgid "&About"\n#~ msgstr "&Omtrent"\n'''
         expected = '''#: resurect.c\nmsgid "&About"\nmsgstr "&Omtrent"\n'''
         newpo = self.pretranslatepo(input_source, template_source)
-        print newpo
+        print(newpo)
         assert str(newpo) == expected
 
     def test_merging_comments(self):
@@ -258,7 +258,7 @@
         template_source = '''#. Don't do it!\n#: file.py:2\nmsgid "One"\nmsgstr "Een"\n'''
         poexpected = '''#. Don't do it!\n#: file.py:1\nmsgid "One"\nmsgstr "Een"\n'''
         newpo = self.pretranslatepo(input_source, template_source)
-        print newpo
+        print(newpo)
         newpounit = self.singleunit(newpo)
         assert str(newpounit) == poexpected
 
@@ -269,7 +269,7 @@
         poexpected = '''#: file.c:1\n#, c-format\nmsgid "%d pipes"\nmsgstr "%d pype"\n'''
         newpo = self.pretranslatepo(input_source, template_source)
         newpounit = self.singleunit(newpo)
-        print newpounit
+        print(newpounit)
         assert str(newpounit) == poexpected
 
         input_source = '''#: file.c:1\n#, c-format\nmsgid "%d computers"\nmsgstr ""\n'''
@@ -295,9 +295,9 @@
         template = xliff.xlifffile.parsestring(xlf_template)
         old = xliff.xlifffile.parsestring(xlf_old)
         new = self.pretranslatexliff(template, old)
-        print str(old)
-        print '---'
-        print str(new)
+        print(str(old))
+        print('---')
+        print(str(new))
         assert new.units[0].isapproved()
         # Layout might have changed, so we won't compare the serialised
         # versions
